{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Develop &amp; Engineer","text":"<p>Disclaimer: This docs add-on my opinion from a data engineer experiences and experiments since 2019.</p> <p>Important</p> <p>I do not have much proper English grammar because I am in the middle level (I try to practice on writing and reading more and more). Please understand this problem and open your mind before continue to read this documents </p> <p>This project will deliver all Practice and Knowledge in Data Developer and Engineer.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p> First, Data Engineering is a critical part of the Data Lifecycle that enables organizations to manage and process large volumes of data efficiently and reliably<sup>3</sup>.</p> <p>By these concepts, Data Engineer should design and implement Data Pipeline and Data Management Strategy that meet the requirements and KPI of their organizations and ensure that your data was managed Consistently and Reliably.</p> <p>What is DE do?</p> <p>Data Engineer is who able to Develop, Implement, Operate, and Maintain any tools on the current Data Infrastructure that your organization use, either On-premises or Cloud providers, comprising databases, storages, compute engines, and pipelines.<sup>1</sup></p> Life Cycle of Data Engineering <p>Fundamentals of Data Engineering</p> <p>Data Engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning.</p> <p>Data engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>A Data Engineer manages the Data Engineering Lifecycle, beginning with getting data from source systems and ending with serving data for use cases, such as analysis or machine learning.</p> <p>\u2014 Joe Reis and Matt Housley in Fundamentals of Data Engineering</p> <p>You will see that stages of the cycle include Data Ingestion, Data Transformation, Data Serving, and Data Storage components.</p> Best practice Importance Proactive data monitoring Regularly checks datasets for anomalies to maintain data integrity. This includes identifying missing, duplicate, or inconsistent data entries. Schema drift management Detects and addresses changes in data structure, ensuring compatibility and reducing data pipeline breaks. Continuous documentation Manages descriptive information about data, aiding in discoverability and comprehension. Data security measures Controls and monitors access to data sources, enhancing security and compliance. Version control and backups Tracks change to datasets over time, aiding in reproducibility and audit trails. <p> Since I started on this role, I got the idea about the future of my responsibilities. I know the Data Engineering tools shifts so fast because the last three year I started with the Map-Reduce processing on the Hadoop HDFS but nowadays, it changes to In-Memory processing like Impala or Spark. The knowledge I gained from Map-Reduce will be wasted .</p> <p></p> <p>The right picture, the 2023 MAD (ML/AI/Data) Landscape <sup>2</sup>, that show about how many possibility tools that able to use on your project. It has many area that you should to choose which one that match with the current architect or fit with your cost planing model.</p> <p> Finally, the below diagram shows how the focus areas of Data Engineering Shift as the analytics organization evolves. That mean Data Engineer does not create a part of data ingestion or serving only. When data engineering tools change very quickly, The focus of data engineers has changed as well.</p> Data Engineering Shift <p>Based upon this illustration, we can observe three distinct focus areas for the role:</p> <ul> <li> <p>Data Infrastructure: One example of a problem being solved in this instance might   be setting up a spark cluster for users to issue HQL queries against data on S3.</p> </li> <li> <p>Data Integration: An example task would be creating a dataset via SQL query,   joining tens of other datasets, and then scheduling the query to run daily using   the orchestration framework.</p> </li> <li> <p>Data Accessibility: An example could be enabling end-users to analyze significant   metrics movements in a self-serve manner.</p> </li> </ul> <p> The trend of Modern Data Stack will make a data engineering process so easy to implement and maintenance that making you have the time to focus on business problem instead technical problem.</p> <p>In the another side, Business users able to use less of technical knowledge to interact the serving data in their data contract platform. It decreases SLA to require the data engineer for need support a lot! </p> <p>You can follow the modern data stack on the below topics:</p> <ul> <li>Services</li> <li>Tools</li> </ul>"},{"location":"#roles","title":"Roles","text":"<p> In the future, if I do not in love with communication or management skill that make me be  Lead Data Engineer, I will go to any specialize roles such as,</p> <ul> <li> <p> Data Platform Engineer</p> <p>Data Platform Engineer</p> <p>Read More about Data Architect</p> </li> <li> <p> DataOps Engineer</p> <p>DataOps Engineer</p> <p>Read More about DataOps</p> </li> <li> <p> MLOps Engineer</p> <p>MLOps Engineers Build and Maintain a platform to enable the development and deployment of machine learning models. They typically do that through standardization, automation, and monitoring.</p> <p>MLOps Engineers reiterate the platform and processes to make the machine learning model development and deployment quicker, more reliable, reproducible, and efficient.</p> <p>Read More about MLOps</p> </li> <li> <p> Analytic Engineer</p> <p>Analytic Engineer is who make sure that companies can understand their data and use it to Solve Problems, Answer Questions, or Make Decisions.</p> <p>Read More about Analytic Engineer</p> </li> </ul> <p>The role from above, I reference from Types of Data Professionals<sup>4</sup>.</p>"},{"location":"#communities","title":"Communities","text":"<p> This below is the list of Communities that you must join for keep update knowledge for Developer and Data Engineer trends.</p> <ul> <li> <p> Data Engineering</p> <p>The Medium Tag for Data Engineering knowledge and solutions </p> </li> <li> <p> Data Engineer Cafe</p> <p>An Area of Discussing Blog for Data Engineer like talk to your close friend at the Cafe </p> </li> <li> <p> ODDS Team</p> <p>The Medium Group that believes software development should be joyful and advocates deliberate practice </p> </li> <li> <p> TPA Roadmap</p> <p>Community Driven Roadmaps, Articles and Resources for developers in Thailand</p> </li> <li> <p> TestDriven</p> <p>Learn to build high-quality web apps with best practices</p> </li> <li> <p> Second Brain</p> <p>My inspiration Data Engineering document website.</p> </li> </ul> <ol> <li> <p>Information of this quote reference from  What is Data Engineering? \u21a9</p> </li> <li> <p> The 2023 MAD (ML/AI/DATA) Landscape \u21a9</p> </li> <li> <p>Unlocking the Power of Data: A Beginner\u2019s Guide to Data Engineering \u21a9</p> </li> <li> <p>Types of Data Professionals, credit to  Kevin Rosamont Prombo for creating the Infographic \u21a9</p> </li> </ol>"},{"location":"daily-work/","title":"Daily Work &amp; Routine","text":"<p>Quote</p> <p>Let's journey through a typical day, highlighting the myriad tasks, challenges, and collaborations that define this pivotal role.<sup>1</sup></p>"},{"location":"daily-work/#getting-started","title":"Getting Started","text":"<p> The role of a Data Engineer is fundamental in today's data-driven decision-making landscape, embodying a unique amalgamation of technical acumen, strategic foresight, and collaborative dynamism.</p>"},{"location":"daily-work/#morning","title":"Morning","text":"<p>Note</p> <p>Laying the Groundwork</p>"},{"location":"daily-work/#reviewing-system-health-and-performance","title":"Reviewing System Health and Performance","text":"<p>The day begins with a critical review of the data systems' health. Monitoring dashboards for alerts on data ingestion or processing issues ensures that any potential bottlenecks are identified and addressed early, safeguarding the data pipeline's integrity.</p>"},{"location":"daily-work/#stand-up-meetings","title":"Stand-up Meetings","text":"<p>Quick stand-up meetings with the data team, and occasionally with cross-functional teams, set the tone for the day. These discussions are vital for aligning on priorities, sharing updates on ongoing projects, and strategizing solutions for any emerging challenges.</p>"},{"location":"daily-work/#midday","title":"Midday","text":"<p>Note</p> <p>Deep Dive into Core Responsibilities</p>"},{"location":"daily-work/#tackling-data-and-quality-issues","title":"Tackling Data and Quality Issues","text":"<p>Data engineers meticulously address data discrepancies, ensuring accuracy and consistency. This includes cleansing data, resolving duplicate records, and rectifying format discrepancies, thereby upholding the data's integrity for reliable analysis.</p>"},{"location":"daily-work/#aligning-with-business-requirements","title":"Aligning with Business Requirements","text":"<p>A significant part of the day involves translating business questions and requirements into technical specifications. This close collaboration with stakeholders ensures that the data infrastructure and processes align with business objectives, facilitating data-driven decision-making.</p>"},{"location":"daily-work/#building-and-optimizing-etl-processes","title":"Building and Optimizing ETL Processes","text":"<p>Designing and refining ETL processes to automate the flow of data through the pipeline is a core task. This involves coding, testing, and deploying processes that extract data from diverse sources, transform it into a structured format, and load it into a data warehouse or lake for analysis.</p>"},{"location":"daily-work/#afternoon","title":"Afternoon","text":"<p>Note</p> <p>Expansion and Collaboration</p>"},{"location":"daily-work/#creating-dashboards-and-reporting-tools","title":"Creating Dashboards and Reporting Tools","text":"<p>Collaborating with data analysts and business users, data engineers contribute to developing dashboards and visualizations. These tools are designed to make data insights accessible, supporting strategic business decisions.</p>"},{"location":"daily-work/#collaborating-with-domain-experts","title":"Collaborating with Domain Experts","text":"<p>Working with domain experts is crucial for refining data models and ensuring they accurately reflect real-world entities. This collaboration enhances the relevance and quality of data, facilitating more meaningful analysis.</p>"},{"location":"daily-work/#embracing-software-engineering-practices","title":"Embracing Software Engineering Practices","text":"<p>Developing custom tools and applications to automate routine tasks and improve efficiency is an integral part of the day. This might include creating scripts for data quality checks or developing integrations to streamline data flows.</p>"},{"location":"daily-work/#late-afternoon-to-evening","title":"Late Afternoon to Evening","text":"<p>Note</p> <p>Reflection and Continuous Learning</p>"},{"location":"daily-work/#addressing-technical-debt-and-bugs","title":"Addressing Technical Debt and Bugs","text":"<p>Late afternoons often involve tackling technical debt and debugging issues, ensuring the data infrastructure is robust and efficient.</p>"},{"location":"daily-work/#documentation","title":"Documentation","text":"<p>Comprehensive documentation of systems, data flows, and processes ensures knowledge sharing and consistency. This is crucial for maintaining a transparent and efficient data operation.</p>"},{"location":"daily-work/#learning-new-technologies","title":"Learning New Technologies","text":"<p>Staying abreast of emerging technologies and tools is essential. Dedicating time to learning enhances a data engineer's ability to implement innovative solutions, driving the organization's data capabilities forward.</p>"},{"location":"daily-work/#special-tasks","title":"Special Tasks","text":""},{"location":"daily-work/#prioritizing-deep-work-data-issue-resolution-and-development-blocks","title":"Prioritizing Deep Work - Data Issue Resolution and Development Blocks","text":"<p> In the dynamic field of data engineering, amidst the array of daily tasks and responsibilities, it's crucial to carve out substantial, uninterrupted periods dedicated solely to either resolving data issues or focusing on development projects. This practice of setting aside larger blocks of time is not merely a scheduling preference but a strategic necessity to ensure the depth of focus required for these complex and time-consuming activities.</p>"},{"location":"daily-work/#data-issue-resolution-blocks","title":"Data Issue Resolution Blocks","text":"<p> Data issues, encompassing discrepancies, inconsistencies, or integrity concerns, demand meticulous attention to detail and a thorough investigative process. Given their potential to impact the reliability of data insights and the overall efficiency of data operations, data engineers allocate significant, focused periods to dive deep into these problems. This dedicated time allows for a comprehensive review of data pipelines, identification of root causes, and the implementation of robust solutions without the distractions of routine tasks.</p> <p>Example</p> <p>On identifying a recurring discrepancy in sales data, a data engineer might block off a morning to systematically trace data flow, review transformation logic, and test potential fixes. This focused effort ensures that not only is the immediate issue resolved but also that systemic improvements are made to prevent future occurrences.</p>"},{"location":"daily-work/#development-time-blocks","title":"Development Time Blocks","text":"<p> Similarly, development activities, whether designing new data pipelines, optimizing existing processes, or developing custom tools, require uninterrupted attention. These blocks of time are earmarked for deep work, where data engineers can engage in complex coding tasks, architectural design, and testing. By isolating these development periods from the day's operational demands, engineers can innovate and build solutions that enhance the data infrastructure's scalability, efficiency, and robustness.</p> <p>Example</p> <p>Planning a session to develop a new ETL process for integrating a recently acquired data source involves uninterrupted hours where the engineer can design, code, and iteratively test the pipeline, ensuring it meets the organization's data standards and performance criteria.</p>"},{"location":"daily-work/#strategic-scheduling-for-maximum-impact","title":"Strategic Scheduling for Maximum Impact","text":"<p> Incorporating these focused blocks into the daily schedule requires strategic planning. Data engineers, in collaboration with their teams and management, identify priorities and deadlines to determine the best times for these deep work sessions. This might involve scheduling data issue resolution early in the week when data loads are typically lower, or blocking development time post stand-up meetings when the day's objectives are clear.</p>"},{"location":"daily-work/#continuous-improvement-and-collaboration","title":"Continuous Improvement and Collaboration","text":""},{"location":"daily-work/#security-and-compliance","title":"Security and Compliance","text":"<p>Ensuring data security and compliance with regulations is an ongoing concern. Landing zones provide a controlled environment to enforce data governance standards, crucial for managing sensitive information.</p>"},{"location":"daily-work/#standardized-data-communication","title":"Standardized Data Communication","text":"<p>Establishing a standardized protocol for data exchange between the Enterprise Data Platform and various sources ensures a cohesive data strategy, vital for the integrity and accessibility of organizational data.</p>"},{"location":"daily-work/#10-hard-truths-about-data-engineering","title":"10 Hard Truths About Data Engineering","text":"<ol> <li> <p>Your Data Is Always Messier Than You Expect</p> <p>No matter how advanced your ETL pipeline is, you will spend 80% of your time cleaning, transforming and validating data. Typos, missing values, incorrect formats and duplicate records will haunt you daily.</p> <p>\ud83d\udc49 Reality check: The best data pipeline won't fix bad source data. Garbage in, garbage out.</p> </li> <li> <p>Stakeholders Expect Magic, Not Engineering</p> <p>Non-technical stakeholders believe Data Engineers are wizards. They think you can snap your fingers and fix broken reports, accelerate queries or build real-time dashboards overnight.</p> <p>\ud83d\udc49 Reality check: Managing expectations is part of the job. If you don't push back, you'll be trapped in an endless cycle of firefighting.</p> </li> <li> <p>Scaling Is 10x Harder Than Building</p> <p>It's one thing to write an elegant data pipeline for 10,000 records per day. It's a different beast when that scales to millions or billions.</p> <p>Latency spikes, unexpected bottlenecks and exploding storage costs will become your daily battle.</p> <p>\ud83d\udc49 Reality check: You don't just need efficient pipelines, you need resilient ones.</p> </li> <li> <p>Data Governance Is Boring But Critical</p> <p>Everyone wants real-time analytics and AI-driven insights, until a bad data update wipes out mission-critical dashboards or regulatory compliance comes knocking.</p> <p>\ud83d\udc49 Reality check: Metadata management, lineage tracking and access control aren't sexy, but they will save your job.</p> </li> <li> <p>Batch vs. Streaming? The Answer Is \"It Depends\"</p> <p>Tech blogs love hyping streaming pipelines, but 90% of business use cases can be solved with batch processing. Yet, everyone wants to jump on the \"real-time\" bandwagon without considering costs and complexity.</p> <p>\ud83d\udc49 Reality check: Batch is cheaper, simpler and more reliable for most use cases. Only build streaming when it's truly needed.</p> </li> <li> <p>Your Job Will Be Repetitive (And That's Okay)</p> <p>You're not always working on cutting-edge projects. Sometimes, you're just maintaining the same pipelines, fixing failed jobs or rewriting slow queries for the tenth time.</p> <p>\ud83d\udc49 Reality check: The best Data Engineers embrace boring work because reliability is more valuable than novelty.</p> </li> <li> <p>Everyone Blames Data Engineers for Bad Data</p> <p>If a dashboard is wrong, Data Engineers get the blame first. No one questions whether the source data was bad, a transformation was misconfigured or an analyst misinterpreted the results.</p> <p>\ud83d\udc49 Reality check: Be proactive. Build monitoring, set up alerts and document your work.</p> </li> <li> <p>No Tool Will Solve All Your Problems</p> <p>From Databricks to Snowflake, Airflow to dbt, tools promise automation and simplicity. But at the end of the day, tools are only as good as the people using them.</p> <p>\ud83d\udc49 Reality check: Master fundamentals (SQL, distributed systems, data modeling) instead of chasing every new tool.</p> </li> <li> <p>Good Data Engineering Is Invisible</p> <p>When everything runs smoothly, no one notices. You only get attention when things break. A successful Data Engineer is one whose work is so reliable, people forget it exists.</p> <p>\ud83d\udc49 Reality check: If you want constant praise, Data Engineering isn't for you.</p> </li> <li> <p>AI Won't Replace You, But Lazy Engineers Will Be Replaced</p> <p>With AI automating ETL tasks and generating SQL queries, some believe Data Engineering jobs will vanish.</p> <p>The truth?</p> <p>Bad engineers will be replaced. Great engineers will thrive.</p> <p>\ud83d\udc49 Reality check: AI is a tool, not a threat. Learn how to use it to your advantage.</p> </li> </ol>"},{"location":"daily-work/#conclusion","title":"Conclusion","text":"<p> The daily routine of a data engineer is characterized by a balance of technical challenges, strategic planning, and proactive collaboration. From ensuring the smooth operation of data pipelines to addressing data quality, aligning with business needs, and embracing continuous learning, each aspect of their day contributes to the overarching goal of leveraging data for strategic insights.</p> <p>As the data landscape continues to evolve, the role of the data engineer will remain integral to unlocking the potential of data to drive decision-making and innovation in the digital age.</p>"},{"location":"daily-work/#references","title":"References","text":"<ul> <li>10 Hard Truths About Data Engineering No One Tells You!</li> </ul> <ol> <li> <p>Daily Work Routine as a Data Engineer \u21a9</p> </li> </ol>"},{"location":"data-storytelling/","title":"Data Storytelling","text":"<p>Quote</p> <p>In a world of information and patterns, Data Storytelling is a beautiful art form that uses ones and zeros.<sup>1</sup></p> <p>Data Storytelling entails communicating facts to a specific audience. It showcases the company\u2019s effectiveness and influence on its customers. Through data insights and beautiful representation, it conveys the entire story of the essential performance metrics, organizational strategies, and processes.</p>"},{"location":"data-storytelling/#what-are-the-components-of-data-storytelling","title":"What Are The Components Of Data Storytelling?","text":"<p>Data storytelling has four main parts: data, visual design, narration, and communication. You need to effectively communicate and share it with your audience.</p> <p>1) Data: The foundation of data storytelling is the data and its credibility. First and foremost, data analytics are very important. This helps a person find insights in the data and draw a conclusion. These key insights build the narration while proving its credibility and usage. When you have a thorough understanding of the data, it provides a stronger basis for supporting the numbers.</p> <p>2) Visual Design: Visually designing your data can enhance the process of data storytelling; examples include charts, graphs, KPIs, tables, etc. These visuals can turn complex concepts into easy narratives. By combining narrative and graphic representation, one can smoothly deliver the information and improve its recall for future use.</p> <p>3) Narrative: Narration is important in data storytelling since it requires expressing data verbally or in written form. It acts as an outlet for communicating to the target audience the story created through analyzing data. While showing the data, you can give recommendations or suggestions based on the data, pointing out vulnerable plots.</p> <p>4) Communication: To succeed in data storytelling, you need to know your audience, set clear goals, and have feedback mechanisms in place. To build interesting data narratives, one must understand the audience and their distinct viewpoints, inclinations, and expertise.</p> <p>Let\u2019s come to the main question \u2014 how is data storytelling beneficial for businesses? Well, businesses often become overwhelmed by a sea of data when navigating the maze. A directing light \u2014 data storytelling paves the way to clarity!</p>"},{"location":"data-storytelling/#why-is-data-storytelling-important-for-your-business","title":"Why Is Data Storytelling Important For Your Business?","text":"<p>Quote</p> <p>Numbers have an important story to tell. They rely on you to give them a clear and convincing voice. \u2014 Stephen Few.</p> <p>The extraction of value from the data that a company has gathered is an important step in strategizing its business plans.</p> <p>Easing complexity by sharing insights: It\u2019s often a complex procedure if the data comes in a huge bunch. Data storytelling plays a major role in simplifying complexity. It transforms data into easily understandable insights using visuals, narratives, and various charts like bar charts, columns, and pie charts. It builds a bridge between the tangled data and meaningful information.</p> <p>Its main job is to simplify complex concepts so that normal users can grasp the importance of those data points. Thus, apply them to their business growth.</p> <p>Make strategic decisions: Knowledge is power. By knowing the context or story behind every piece of information, companies can make strategic decisions. Businesses can successfully coordinate their growth strategies and set goals by utilizing the effectiveness of data storytelling. With this strategy, businesses can use the context and narrative capabilities of data to guide their decision-making and coordinate efforts to meet their objectives.</p> <p>Presenting data driven storytelling: Businesses can successfully communicate the narrative the data offers by using impact storytelling. By relating insights to real people and events, this strategy increases their applicability and makes them more accessible in the workplace. Companies may include stakeholders, decision-makers, and workers and ensure that the insights drawn from the data align with their objectives and experiences by narratively presenting the data. This strategy helps the business make better decisions by increasing the effect of data-driven insights.</p> <p>Also, it grabs people\u2019s attention and sparks emotions, which motivates them to take action. Thus, the organization can aim to create a data-driven culture. This involves identifying areas for improvement and understanding complex processes. Examples of these processes include customer service and supply chain management.</p> <p>Making a connection with data storytelling: One of the best practices of data storytelling is making it interactive. It lies in the message you convey through your data. The clearer the narrative of your data is, the better they can connect with or relate to it.</p> <p>For instance, questions like \u201cWill the presentation offer actionable insights to stakeholders? or Do these trends hold any significance?\u201d can go deeper into the root cause, and thus getting a solution will be easy.</p> <p>The trick to successful data storytelling is \u2014 when it blends together narratives, real-life events, and fascinating experiences. Communicating data and connecting with your clients with an element of authenticity makes your message more relatable.</p>"},{"location":"data-storytelling/#steps-to-a-persuasive-data-storytelling-presentation","title":"Steps to a Persuasive Data Storytelling Presentation","text":"<p>The power of data storytelling lies in the simplification of complicated information. It enhances effectiveness by utilizing visual representation, while traditional data storytelling relies on conventional elements like character, conflict, and resolution.</p> <p>Data storytelling is like an artist choosing colors and brushstrokes to create feelings. It uses visuals, narratives, and settings to engage and convince listeners. By carefully selecting and presenting data, you can transform complex information into a captivating narrative that connects and motivates action. Creating a compelling data story helps make informed decisions, facilitates effective interaction, and drives positive change in various industries.</p> <p>We all know that datasets are just numbers without the story that lies behind them, right? If the data is easy to understand, imagine the time and effort it can save. You can utilize these efforts to generate ideas for making better business decisions. We will examine the topic in more detail.</p> <p></p> <p>The characters that represent your ideal customer or group of clients are basically user personas. This helps in understanding your customers better by identifying things like their interests, personalities, or ages. These personas help in connecting with or communicating with your audience about the product marketing approach on a deeper level. This aids in understanding their choices and preferences.</p> <p>For instance \u2014 a dashboard with personalized hubs that help your audience use your product without any complications can make things easy. It will captivate a diverse audience from different departments. But before that \u2014 there are some crucial questions to focus on:</p> <ul> <li>What kind of data analysis and storytelling are they interested in? Use data to construct a picture that clarifies their course.</li> <li>A broad overview or a close examination of the data \u2014 what do they prefer? Adapt your story to their understanding and grab their attention.</li> <li>Are they beginners or professionals? Acknowledge their expertise and interact with them accordingly.</li> </ul> <p>The ability to effectively build an engaging narrative that keeps your audience interested in your story is a must. Ultimately, your ability to captivate them determines the impact of your data narrative presentation and the resulting outcome.</p>"},{"location":"data-storytelling/#colors-that-speak-of-the-facts-conveyed","title":"Colors That Speak of the Facts Conveyed:","text":"<p>Using color in data storytelling presentations is a bonus to a compelling narrative \u2014 it helps your audience understand the information more effectively. Data shown in green may indicate a favorable outcome, whereas data displayed in red may demonstrate a poor outcome.</p> <p>A personalized dashboard that helps you easily understand important information can greatly affect your clients.</p>"},{"location":"data-storytelling/#develop-engagement-among-users","title":"Develop Engagement Among Users","text":"<p>Prioritize the interests of your targeted audience. You could improve their engagement and make it more interactive and engaging by responding to their queries and comprehending their expectations.</p> <p>Responding to their questions and understanding their expectations is a successful strategy. This encourages a feeling of participation and connection while also enabling you to customize your material or products to suit their requirements better. A more pleasant and meaningful encounter may arise from taking the time to listen and understand their preferences, which will eventually boost audience participation and trust.</p>"},{"location":"data-storytelling/#visualizing-data-insights","title":"Visualizing Data Insights:","text":"<p>Data visualization storytelling plays the ultimate role. You can impress your audience with your data by using charts, filters, buttons, and visuals instead of dull tables and sheets. To learn more about the principles of effective data visualization, click here.</p> <p>You may compare data, evaluate performance, and draw attention to variations between scenarios by using data visualization and storytelling. This gives you the opportunity to provide useful feedback and pointers to your audience. This will help them progress toward their objectives.</p>"},{"location":"data-storytelling/#data-storytelling-in-action","title":"Data Storytelling in Action","text":"<p>Developing compelling and intuitive dashboard designs is essential for thriving in today\u2019s highly competitive business world. In response to this need, mokkup.ai created a design platform that gives businesses the ability to produce interactive dashboards.</p> <p>Key performance indicators (KPIs): KPI metrics are valuable in determining how well a business is doing in reaching specific targets. You can compare past and present information when you customize dashboards with Mokkup, which is one of its best features. As a result, users will find it easy to understand and will visualize the information being presented.</p> <p></p> <p>Themes: Is spring green the brand color for your organization, and are you seeking to align the dashboard\u2019s color scheme accordingly? Enhance the dashboard easily by using different styles and themes that match your preferences and clients\u2019 needs. You can effortlessly achieve the appearance you want owing to this flexibility.</p> <p></p> <p>Charts and Maps: Charts give you an almost limitless number of alternatives. You can enhance the quality of your dashboard design. Combo charts, pie charts, donut-shaped charts, funnel charts, and more are merely a few examples of the alternatives available. There are more alternatives available, including area and line charts. So use your imagination to create an incredible data storytelling screen for your dashboard.</p> <p></p> <p>Including maps can further enhance the visual appeal and context of data that is geographically focused. In such circumstances, you have the opportunity to explore the geomap, treemap, and heatmap. Isn\u2019t it better to use a map to display information about a specific location rather than bland tables?</p> <p>However, balancing the use of tables with other visual elements can avoid overwhelming the dashboard with excessive data. Instead of relying completely on bland tables, maps such as geomaps, treemaps, and heatmaps offer a more engaging and intuitive way to convey information.</p>"},{"location":"data-storytelling/#final-thoughts","title":"Final Thoughts","text":"<p>Presenting data storytelling in an accessible manner can have a greater impact on your audience. It transforms complex numbers into visually appealing content, making it more engaging and easy to understand. It will be simple for the average user to understand the story the numbers are trying to tell. There is no other format that can compare to the impact that words and images have on the audience\u2019s minds.</p> <p>Finally, knowing your audience and their needs and keeping your presentation concise will help you make effective data storytelling presentations.</p> <ol> <li> <p>Can Data Make You a Better Storyteller? \u21a9</p> </li> </ol>"},{"location":"lead-data-engineer/","title":"Lead Data Engineer","text":"<p>Quote</p> <p>Lead Data Engineers need to possess strong people skills and critical thinking abilities.<sup>1</sup></p> <ol> <li> <p> Lead Data Engineer Career Guide \u21a9</p> </li> </ol>"},{"location":"methodology/","title":"Methodology of Data Engineer","text":""},{"location":"methodology/#getting-started","title":"Getting Started","text":"<p>The data engineering process covers a sequence of tasks that turn a large amount of raw data into a practical product meeting the needs of analysts, data scientists, machine learning engineers, and others.</p>"},{"location":"methodology/#discovery-phase","title":"Discovery Phase","text":"<p>At this early stage, flexibility is crucial because requirements in large enterprises often evolve.</p> <p>Note</p> <p>Identify the primary and secondary stakeholders.</p> <p>Meet them, and get their avatar into an orientation diagram or slide deck. These are ultimately your customers; engagement with them will make or break your project.</p> <p>Identify the product owner. This person, or people, will own the delivery and operations for this data product. They will be your tiebreakers and demo audience, and they are the ones who will sign off on your project's delivery.</p>"},{"location":"methodology/#requirements-gathering","title":"Requirements Gathering","text":"<p> The process of gathering requirements from various stakeholders can be technically and inter-personally challenging. Not everyone will have the same understanding of the problem or its solutions. Techniques such as interviews, surveys, and workshops will help expose these differences so that we can work towards getting everyone on the same page. Mock-ups and wireframes can also help flush out requirements at this phase.</p>"},{"location":"methodology/#design-and-planning","title":"Design and Planning","text":"<p>Data engineers should plan the architecture, data pipelines, and infrastructure based on the gathered requirements. This phase should stress the importance of designing for scalability and maintainability.</p> <p>Deciding to buy or build happens here. Will there be legacy code to deal with, existing system integrations, or greenfield? Consider which systems will consume the data when selecting a data store.</p> <p>Example</p> <p>Web apps will need low-latency data stores and will benefit from caching (think MemCacheD). Dashboards usually work best with relational database systems such as Postgres or MySQL, and caching may not be ideal for real-time or near-real-time data, but read replicas may help fan out the read load.</p> <p>Most data projects will benefit from an ETL (Extract, Transform, Load) pattern, where each step is a discrete, independent step that can be run and tested in isolation and end to end. The artifacts from this design process should include diagrams, documentation, and, hopefully, the beginnings of test cases.</p> <p>It would also be a good idea to start a source-to-target mapping that defines the data points in the consuming application (dashboard, web app, etc.) and shows their lineage all the way back to their original source.</p>"},{"location":"methodology/#story-grooming-and-backlog-building","title":"Story grooming and Backlog building","text":"<p>Once we have a design and some test cases, we identify the Minimum Viable Product (MVP). This could be a thin slice of functionality where a subset of data is exposed end to end or a single data source end to end, the idea is to break up the work into phases or chunks that are more manageable.</p> <p>Features are written to encapsulate the required work for each part of the MVP, those features are filled in with the user stories that make that feature work. At this point, you will want a detailed design for the MVP, including data flows, expected inputs and outputs, API contracts, etc. The components not in scope for MVP can have epics and even features at this point, but since you will learn as you build out MVP, it might make sense to leave the detail for non-MVP components until closer to the delivery of the MVP.</p>"},{"location":"methodology/#implementation-and-testing","title":"Implementation and Testing","text":"<p>Once we have user stories, engineers can fill out the tasks that will satisfy the user story requirements. We can take these requirements and start writing tests that our production code will then satisfy. Some organizations will open a pull request with just the test cases to facilitate the conversation with the technical team and stakeholders. Initial development artifacts should include production code and unit tests. As complete functionality is deployed, integration tests can be built out where real data moves through the pipeline and is measured at critical points during its journey for accuracy and completeness.</p> <p>Once the entire pipeline is in place, we develop our end-to-end tests that cover the complete pipeline with checks to ensure we push the right data to the correct place. Using this test-driven development (TDD) approach, we can ensure the person writing the software feature is writing just the code needed to satisfy requirements and nothing more. This goes a long way to cut down the number of defects and missed requirements.</p>"},{"location":"methodology/#validation-and-quality-assurance","title":"Validation and Quality Assurance","text":"<p>Tracking row counts at every point in the pipeline and automated tests for known business rules will eliminate many validation issues</p> <p>Having Subject Matter Experts (SMEs) or Business analysts with domain knowledge get eyeballs on the data will identify problems before production. This means providing a stable, consistent, accessible place for validations. A good artifact from this process is a data quality report that shows the health or quality of data at each step in the pipeline. Things like anomaly detection on the values can automate the detection of bugs or drift.</p>"},{"location":"methodology/#feedback-loops","title":"Feedback Loops","text":"<p>Throughout each phase of this methodology, stakeholders should establish feedback loops to ensure alignment and address evolving requirements. Initially, initial diagrams, documentation, and story grooming can help establish feedback loops. Later, regular and frequent demos and design reviews will help shape conversations around expectations and functionality.</p>"},{"location":"methodology/#hand-off","title":"Hand-off","text":"<p>Once the functionality is delivered, an operations team must be trained to support and maintain the application. Clear diagrams and documentation will aid this process. Run-books and troubleshooting guides are also invaluable at this point. Ideally, most of the invasive maintenance has been automated, and the operations team is left to handle support requests, such as requests for access, data issues, etc. Metrics such as the number of incidents and their root causes will help illuminate where future development work might need to happen.</p>"},{"location":"methodology/#conclusion","title":"Conclusion","text":"<ul> <li>Know your customers: Strong stakeholder and product owner relationships are   often at the crux of a successful data project.</li> <li>Drive consensus using diagrams, documentation, and test cases.   The availability of such intellectual artifacts will likely shape the project   delivery.</li> <li>Focus on repeatability and testing: time spent here early on will pay dividends   later in the development lifecycle.</li> </ul>"},{"location":"methodology/#read-mores","title":"Read Mores","text":"<ul> <li>A Comprehensive Guide on Planning a Data Engineering Project</li> </ul>"},{"location":"requirement-gathering/","title":"Requirement Gathering","text":"<p>One of the mistakes you\u2019ll make as a Data Engineer is not truly understanding the Business Requirements.</p> <p>Example</p> <p>Quote</p> <p>The business will come to you and ask for a real-time dashboard.<sup>1</sup></p> <p>But they mean they want the data updated 3-4x a day, or maybe they only look at the report once a week; at that moment, the data should be as up-to-date as possible.</p>"},{"location":"requirement-gathering/#getting-started","title":"Getting Started","text":""},{"location":"requirement-gathering/#identify-the-end-users","title":"Identify the End-Users","text":"<p> Begin by identifying the end-users, crucial stakeholders who utilize the project's output (Understanding the capabilities and preferences of the end-user is crucial for designing an appropriate solution).</p> <p>End-users (&amp; their preferences) for data projects are usually one of;</p> <ul> <li> <p>Data Analysts/Data Scientists</p> <p>SQL, No-SQL, CSV files</p> </li> <li> <p>Business Users</p> <p>Dashboards, Reports, Excel files</p> </li> <li> <p>Software Engineers</p> <p>SQL, APIs, CRMs, JSON files</p> </li> <li> <p>External Clients</p> <p>Cloud storage, SFTP/FTP, APIs, SQL</p> </li> </ul> <p>Warning</p> <p>If you do not know who is the end users, you can ask the Solution Architect in that data project.</p>"},{"location":"requirement-gathering/#help-end-users-define-requirements","title":"Help End-Users Define Requirements","text":"<p>Note</p> <p>Understand The Business - Not Just The Technical Requirements</p> <p> Assist end-users in defining requirements by engaging in conversations about their objectives and challenges. Understand their current operations to gain valuable insights.</p> <p>Use the following questions to refine requirements:</p> <ul> <li> <p>Business Impact</p> <p>Evaluate how the data impacts the business and quantify the improvements.</p> <ul> <li>How does having this data impact the business?</li> <li>What is the measurable improvement in the bottom line, business OKR, etc?   Knowing the business impact helps in determining if this project is worth   doing.</li> </ul> </li> <li> <p>Semantic Understanding</p> <ul> <li>What does the data represent? What business process generates this data?   Knowing this will help you model the data and understand its relation to other   tables in your warehouse.</li> </ul> <p>Grasp the data's representation and its relation to other warehouse tables.</p> </li> <li> <p>Data Source</p> <p>Where does the data originate? (an application database, external vendor via SFTP/Cloud store dumps, API data pull, manual upload, etc).</p> </li> <li> <p>Frequency of Data Pipeline</p> <ul> <li>How fresh does the data need to be? (n minutes, hourly, daily, weekly, etc).</li> <li>Is there a business case for not allowing a higher frequency?</li> <li>What is the highest frequency of data load acceptable by end-users?</li> </ul> </li> <li> <p>Data Output Requirements</p> <ul> <li>What is the data output schema? (table name, column names, API field names,   Cloud storage file name/size, etc)</li> </ul> </li> <li> <p>Historical Data</p> <ul> <li>Does historical data need to be stored? When loading data into a warehouse,   the answer is usually yes.</li> </ul> </li> <li> <p>Data Caveats</p> <ul> <li>Does the data have any caveats? (e.g. seasonality affecting size, data skew,   inability to join, or data unavailability).</li> <li>Are there any known issues with upstream data sources, such as late arriving   data, or missing data?</li> </ul> </li> <li> <p>Access Pattern</p> <ul> <li>How will the end user access the data? Is access via SQL, dashboard tool,   APIs, or cloud storage? In the case of SQL or dashboard access, What are the   commonly used filter columns (e.g. date, some business entity)?</li> <li>What is the expected access latency?</li> </ul> </li> <li> <p>Business Rules Check (QA)</p> <ul> <li>What data quality metrics do the end-users care about?</li> <li>What are business logic-based data quality checks? Which numeric fields   should be checked for divergence (e.g. can\u2019t differ by more than x%) across   data pulls?</li> </ul> </li> </ul> <p> Show appreciation to end-users for their time, keep them updated on progress, incorporate their feedback, suggest solutions for common issues, and acknowledge their expertise when presenting the project to a wider audience.</p> <p>Clearly define the requirements, record them (e.g. JIRA, etc), and get sign-off from the stakeholders.</p>"},{"location":"requirement-gathering/#end-user-validation","title":"End-User Validation","text":"<p> Provide end-users with sample data for analysis, allowing them to validate its accuracy and usability. Record any new requirements or changes, getting sign-off from stakeholders before proceeding.</p> <p>Record any new requirements or changes (e.g. JIRA, etc), and get sign-off from the stakeholders. Do not start work on the transformation logic until you get a sign-off from the stakeholders.</p>"},{"location":"requirement-gathering/#deliver-iteratively","title":"Deliver Iteratively","text":"<p> Break down large projects into smaller, manageable parts and work with stakeholders to set timelines and priorities. This approach facilitates a short feedback cycle, making it easier to adapt to changing requirements. Track progress with clear acceptance criteria.</p> <p>Example</p> <p>If you are building an ELT pipeline (REST API =&gt; dashboard), you can split it into modeling the data, pulling data from a REST API, loading it into a raw warehouse table, &amp; building a dashboard for the modeled data.</p> <p>Delivering in small chunks enables a short feedback cycle from the end-user making changing requirements easy to handle. Track your work (tickets, etc) with clear acceptance criteria.</p>"},{"location":"requirement-gathering/#handling-changing-requirementsnew-features","title":"Handling Changing Requirements/New Features","text":"<p> Establish a process for handling change or feature requests, ensuring end-users can request modifications. Prioritize requests with stakeholder input, communicate delivery timelines, and educate end-users on the request process to prevent scope creep and maintain timely delivery.</p> <p>Warning</p> <p>Do not accept Adhoc change/feature requests! (unless it\u2019s an emergency).</p> <p>Create a process to ...</p> <ul> <li>Allow end-users to request changes/features</li> <li>Prioritize the change/feature requests with help from stakeholders</li> <li>Decide and communicate delivery timelines to end-users</li> </ul> <p>Educate the end-user on the process of requesting a new feature/change. Following a process will prevent scope creep and allow you to deliver on time.</p>"},{"location":"requirement-gathering/#examples","title":"Examples","text":""},{"location":"requirement-gathering/#project-starter","title":"Project Starter","text":"Question Answer What responsibility of a data engineer on this project? Load all data sources from RDBMS to the warehouse for making dashboards by DA."},{"location":"requirement-gathering/#data-component","title":"Data Component","text":"Question Answer What is the data source system type? RDBMS (Postgres, MySQL, SQL Server, etc.),NO-SQL (MongoDB, Elastic),API What is data compute service to extract from the source system? Docker Application (FastAPI), Batch job via Cloud services (Azure Batch, AWS Batch, etc.) ETL Services (Azure Databricks, AWS Glue etc.) How to authenticate to the data source system? Use user and password. Use service account on that cloud provider."},{"location":"requirement-gathering/#data-source","title":"Data Source","text":"<p>These questions use per data source.</p> Question Answer What is data source name? What is the schema of this source (column and data type docs)? What is the primary of this data source? How to ingest data to this data source? How to incremental load of this data source? The filter fields for filter period of changed data like create_date, updated_date etc. What is loading type of this data source? Incremental via Merge or Append.Full-dump load. When to load this data source that does not effect to the source system? Every 1:30 AM to 3:00AM is good or not? What is the rule check for this data?"},{"location":"requirement-gathering/#conclusion","title":"Conclusion","text":"<p>Effectively managing ever-changing requirements is a challenging aspect of a data engineer's role. By following the steps outlined in this article, you can navigate these challenges, ensuring timely project delivery, making a significant impact, enjoying your work on data projects, and fostering supportive end-users.</p> <p>The next time you start a data project, follow the steps shown above to</p> <ul> <li>Deliver on Time</li> <li>Make a Huge Impact</li> <li>Make working on the data projects a Joy</li> <li>Build supportive end-users</li> </ul> <ol> <li> <p>Becoming a Better Data Engineer Tips \u21a9</p> </li> <li> <p>How to gather requirements for your data project \u21a9</p> </li> </ol>"},{"location":"transform-spec/","title":"Transform Spec","text":"<p>When the requirement of data ingestion and transformation was completed by user, the final product that use to communicate is the transformation spec data.</p> <p>The flow before implementation data modeling.</p> <pre><code>---\ntitle: End-to-End Flow\n---\nstateDiagram-v2\n    direction LR\n    R: Requirement Gathering\n    D: Datasource Exploring\n    T: Transform Spec\n\n    [*] --&gt; R\n\n    R --&gt; D\n    D --&gt; R: recheck logic\n    D --&gt; T\n    T --&gt; [*]</code></pre>"},{"location":"transform-spec/#metadata-of-transform-spec","title":"Metadata of Transform Spec","text":"Component Column Name Description Data Type PDM Information (Semantic Layer) Database Database or schema name STRING Table Name / File Name Table or file name STRING Column Name Column name STRING Data Type Data type STRING Key Flag for primary key marking BOOLEAN Field Definition Definition of this field STRING Source Information Source System / Database STRING Table Name / File Name STRING Alias STRING Column STRING Transform Business Rule Type LITERAL[\"Not Mapped\", \"Constant\", \"Not Mapped\", \"Move\", \"Filter\", \"Join\"] Business Rule / Join / Condition STRING Remark STRING Updating Information Updated Date DATETIME Updated By STRING Remark STRING <p>Example</p> <p>If the transformation spec fron the user that provide to you be like;</p> <pre><code>WITH addr AS (\n    SELECT DISTINCT\n        cust_address\n        , sub_district_code\n        , district_code\n        , province_code\n        , post_code\n    FROM `data-prod-mobile.b2c_customer.transaction`\n)\nSELECT\n      md5(concat(\n        coalesce(cust_address, '')\n        , coalesce(sub_district_code, '')\n        , coalesce(district_code, '')\n        , coalesce(province_code, '')\n        , coalesce(post_code, '')\n      ))                                                    AS HOUSE_ID\n    , 1                                                     AS HOUSE_LVL\n    , cust_address                                          AS ADDR\n    , null                                                  AS HOUSE_NO\n    , null                                                  AS STREET\n    , sub_district_code                                     AS SUB_DIST\n    , sub_district_name                                     AS SUB_DIST_DESC\n    , district_code                                         AS DIST\n    , district_name                                         AS DIST_DESC\n    , province_code                                         AS PROV\n    , province_name                                         AS PROV_NM\n    , post_code                                             AS POST_CD\nFROM\n    addr                                                    AS addr\nLEFT JOIN `data-prod-mobile.external.outbound_subdistrict`  AS sub\n    ON addr.subdistrict_code    = sub.subdistrict_id\nLEFT JOIN `data-prod-mobile.external.outbound_district`     AS dis\n    ON addr.district_code       = dis.district_id\n</code></pre> <p>Assume that this transform spec query use for ingest data to the target table on your warehouse name like <code>DWHMODEL.MODEL_HOUSE</code>.</p> <p>The data on the transform spec table will be like;</p> Database Table Name /File Name Column Name Data Type Key FieldDefinition Source System /Database Table Name /File Name Alias Column Business Rule Type Business Rule / Join / Condition Remark Updated Date Updated By Remark DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.b2c_customer.transaction addr Filter SELECT DISTINCTcust_address, sub_district_code, district_code, province_code, post_codeFROM <code>data-prod-mobile.b2c_customer.transaction</code> DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.external.outbound_subdistrict sub Join LEFT JOIN <code>data-prod-mobile.external.outbound_subdistrict</code> AS subON addr.subdistrict_code = sub.subdistrict_id DWHMODEL MODEL_HOUSE - - - MOBILE data-prod-mobile.external.outbound_district dis Join LEFT JOIN <code>data-prod-mobile.external.outbound_district</code> AS disON addr.district_code = dis.district_id DWHMODEL MODEL_HOUSE HOUSE_ID STRING Y House ident MOBILE data-prod-mobile.b2c_customer.transaction addr cust_addresssub_district_codedistrict_code province_codepost_code Business Rule md5(concat(  coalesce(cust_address, ''), coalesce(sub_district_code, ''), coalesce(district_code, ''), coalesce(province_code, ''), coalesce(post_code, ''))) Use <code>md5</code> algorithm DWHMODEL MODEL_HOUSE HOUSE_LVL INTEGER House level - N/A N/A N/A Constant 1 DWHMODEL MODEL_HOUSE ADDR STRING House's address MOBILE data-prod-mobile.b2c_customer.transaction addr cust_address Move DWHMODEL MODEL_HOUSE HOUSE_NO STRING House's Number - - - - Not Mapped DWHMODEL MODEL_HOUSE STREET STRING House's street - - - - Not Mapped DWHMODEL MODEL_HOUSE SUB_DIST STRING House's sub-district code MOBILE data-prod-mobile.b2c_customer.transaction addr sub_district_code Move DWHMODEL MODEL_HOUSE SUB_DIST_DESC STRING House's sub-district name MOBILE data-prod-mobile.external.outbound_subdistrict sub sub_district_name Move DWHMODEL MODEL_HOUSE DIST STRING House's district code MOBILE data-prod-mobile.b2c_customer.transaction addr district_code Move DWHMODEL MODEL_HOUSE DIST_DESC STRING House's district name MOBILE data-prod-mobile.external.outbound_district dis district_name Move DWHMODEL MODEL_HOUSE PROV STRING House's province code MOBILE data-prod-mobile.b2c_customer.transaction addr province_code Move DWHMODEL MODEL_HOUSE PROV_NM STRING House's province name MOBILE data-prod-mobile.b2c_customer.transaction addr province_name Move DWHMODEL MODEL_HOUSE POST_CD STRING House's post code MOBILE data-prod-mobile.b2c_customer.transaction addr post_code Move"},{"location":"abstract/data_architecture/","title":"Data Architecture","text":"<ul> <li>Data Mesh Implementation Strategies</li> <li>Event-Driven Data Architectures</li> <li>Polyglot Persistence in Microservices</li> <li>Serverless Data Architectures</li> <li>Cloud-Native Data Architectures</li> <li>Data Lake and Lakehouse Architectures</li> </ul>"},{"location":"abstract/data_architecture/#why","title":"Why?","text":"<ul> <li>\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2d\u0e2d\u0e01\u0e41\u0e1a\u0e1a Data Platform Architecture \u0e43\u0e2b\u0e49\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 data \u0e43\u0e19\u0e1a\u0e23\u0e34\u0e29\u0e31\u0e17\u0e44\u0e14\u0e49</li> <li>\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e41\u0e25\u0e30\u0e14\u0e39\u0e41\u0e25 Data Platform \u0e43\u0e2b\u0e49 scalability \u0e41\u0e25\u0e30 reliability \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 resource/cost \u0e43\u0e2b\u0e49\u0e04\u0e38\u0e49\u0e21\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14</li> </ul> <ul> <li>Building a Data Platform in 2024</li> <li>https://life.wongnai.com/get-to-know-data-platform-engineer-role-at-lmwn-4b43443eaca8</li> <li>Medium: Cloud Agnostic Data Platform</li> <li>Modern Architecture for Emerging Data Infrastructures</li> <li>Modern Architecture for Emerging Data Infrastructures</li> </ul>"},{"location":"abstract/data_architecture/#getting-started","title":"Getting Started","text":"<p>Data Architecture is the blueprint of a data system that serves the business requirements of a product and describes how data is collected, stored, transformed and distributed. It consists of data models, governance policies, rules and standards that need to be implemented and followed to build a robust and secure data system.</p> <p></p>"},{"location":"abstract/data_architecture/#roles","title":"Roles","text":""},{"location":"abstract/data_architecture/#data-platform-architect","title":"Data Platform Architect","text":"<ul> <li>Data Platforms : Good Architect \u2014 Bad Architect</li> </ul>"},{"location":"abstract/data_architecture/#knowledge","title":"Knowledge","text":"<ul> <li>My key takeaways after building a data engineering platform</li> <li>Microservices vs. Monolithic Approaches in Data</li> </ul>"},{"location":"abstract/data_architecture/#use-cases","title":"Use Cases","text":""},{"location":"abstract/data_architecture/data-architecture-secure-data-platform/","title":"Secure Data Platform","text":""},{"location":"abstract/data_architecture/data-architecture-secure-data-platform/#read-mores","title":"Read Mores","text":"<ul> <li>Building Secure Data Platforms: A Guide for Team\u2019s Structure &amp; Access Strategies</li> </ul>"},{"location":"abstract/data_architecture/event-driven-data-architecture/","title":"Event-Driven Data Architecture","text":"<p>Quote</p> <p>I believe that the way the industry is often building asynchronous event-driven applications is misguided and can be greatly improved.<sup>1</sup></p> <p>Online businesses (online food ordering and delivery, ride-hailing, financial applications, e-commerces, etc.) often have business processes implemented with services (Orders, Notifications, Tracking, Payments, Users, etc.) communicating through an event broker (e.g. Kafka, NATS, Solace, RabbitMQ or Pulsar):</p> Business process running through services exchanging events <p>Using events has many advantages:</p> <ul> <li>Unlike API-driven services, event-driven services can function independently   of each other, passing messages that are stored durably in the event brokers.   This makes business processes very reliable by design: even if a service is down,   it will be able to resume when it\u2019s back up again.</li> <li>You do not need to oversize your infrastructure: adding more process instances   will increase latency in their execution but will not break your services   (as long as your event broker can handle new messages).</li> <li>It\u2019s flexible enough that you can easily add an action by \u201chooking\u201d a service to an event. To do this, you don\u2019t need to update the service that issued the event. From an organizational point of view, it\u2019s an appreciable flexibility.</li> </ul> <p>Warning</p> <p>However, this event-driven pattern does not simplify your life.</p>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#getting-started","title":"Getting Started","text":"<p> Asynchronous communication is much more complex than it appears. You must not only operate an event broker, but also write a lot of boilerplate code, as you have to define your queues, define your messages (types and schemas), learn the event broker APIs and write specific consumers and producers, implement error handling, and be extra careful not to break anything during updates.</p>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#the-choreography-pattern","title":"The Choreography Pattern","text":"<p>Vendors like Confluent try to convince you that a central event broker cleans up the mess created by direct inter-service HTTP communications. But in practice, each event-driven service reacts to other service events and produces its own events (this is why it\u2019s called the choreography pattern). So instead of API-driven services calling other ones, we have event-driven services pulling messages from other ones:</p> Choreography Pattern <p>Despite being ubiquitous, this pattern has important issues:</p> <ul> <li>There is no easy way to understand how a specific business process is actually   implemented. The whole process is defined by how each service reacts to others.   There is no central repository, and the implementation details of business   processes are scattered everywhere.</li> <li>For the same reason as above, there\u2019s no easy way to understand what business   processes are running. Your customer-support and customer-success teams will   soon be forcing you to set up some kind of tracing system to be able to do this.</li> <li>A service must know the structure of the events it must react to.   A modification to an event can break the services that use it downstream.   It\u2019s an important issue, especially if multiple teams are evolved.</li> <li>\u201cFan-in\u201d situations where a service wants to react to multiple events combined   are complex to implement as they require storing states.</li> <li>Implementing transactions (when multiple service executions must be successful   together or none) is challenging as well.</li> <li>Updating the code of a business process without breaking ongoing ones is really   difficult.</li> </ul>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#the-orchestrator-pattern","title":"The Orchestrator Pattern","text":"<p>As we can see above, event-driven applications based on the choreography pattern are efficient when running, but complex to implement and very complex to maintain.</p> <p>Note</p> <p>For these reasons, I think an \u201corchestration\u201d model is preferable. In this pattern, a service called \u201corchestrator\u201d or \u201cworkflow\u201d is responsible for managing service executions based on the business process definition.</p> <p>Historically, these orchestrators were rather cumbersome, low-performance software, like BPM engines. But today an orchestrator can be lightweight and event-based to keep all the benefits described above:</p> Orchestrator Pattern <p>Conceptually, this pattern it simpler:</p> <ul> <li>Each service has a command and an event topic. The service reacts to commands (\u201ccreate invoice\u201d) by producing events (\u201cinvoice created\u201d).</li> <li>Each workflow service pilots business processes by producing commands to services (potentially to other workflows) and consuming the corresponding events. A workflow service has its own database to store the current state of processes, that are now visible.</li> <li>Workflow services are horizontally scalable. But for consistency we should no have race condition when updating a state \u2014 either by locking the corresponding row in the database \u2014 or by ensuring that a specific process instance will always be handled by the same workflow service instance. I prefer the latter, which is more scalable and allows also to cache the state in-memory.</li> </ul> <p>The benefits of this pattern are:</p> <ul> <li>The services are better decoupled, as they no longer need to know what events the other services are producing. (The workflow services need to know about other services, but it is precisely their role to ensure the smooth flow of data between services).</li> <li>It\u2019s easy to understand how a process is implemented, as it\u2019s described within the workflow service.</li> <li>The workflow services can easily implement sophisticated workflows, including compensations if needed.</li> <li>The workflow services can be versioned to ensure the smooth updates of business processes.</li> </ul>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#infinitic-framework","title":"Infinitic Framework","text":"<p>In practice, nevertheless, this pattern is more complex to implement, and I believe this is why it\u2019s more rarely implemented, despite its many benefits. That\u2019s why we created Infinitic, a framework to make event-driven applications easy using the orchestrator pattern.</p> <p>This is what an event-driven application using Infinitic looks like:</p> Infinitic Framework <p>Where everything in green is provided by Infinitic:</p> <ul> <li>The topics topology with their schemas are managed for you</li> <li>A Client to start and interact with workflows.</li> <li>The Service workers wrapping your Java services (removing the need for you to implement consumers and producers, and to manage data serialization and deserialization).</li> <li>The Workflow workers providing an easy way to implement your business logic   in full Java. (This will be the subject of another article.)</li> </ul> <p>The scheduler has disappeared from the diagram because Infinitic provides its own, simplifying the architecture even further.</p> <p>As you do not need to know the internals of Infinitic to run it, we can simplify the above illustration to:</p> Infinitic Framework Internals <p>By providing all the boilerplate code, Infinitic makes it easier to build an event-based application using an orchestration pattern with the following benefits:</p> <ul> <li>Excellent scalability and resilience</li> <li>Horizontally scalable event-driven services that are entirely decoupled</li> <li>Horizontally scalable workflow services</li> <li>Built-in error-management (retries, dead letter queues, error propagation, etc.)</li> <li>Ability to code the more complex business processes (e.g. including timers or signals)</li> <li>Centralized and git-versioned definitions for workflows.</li> <li>Ability to update workflows without breaking running ones.</li> <li>Built-in observability of workflows states.</li> </ul> <p>Note that you do not need to switch your entire infrastructure to Infinitic to start using it. You can easily start with an isolated process:</p> Use of Infinitic for an isolated sub-process <p>Infinitic is currently based on Apache Pulsar, which has the right features for an ideal implementation (message queues, key-shared subscriptions, delayed messages). Other platforms like Kafka or RabbitMQ could be added later as they add those features.</p> <p>You do not need to know Pulsar at all to use Infinitic. You just need a running instance. If you are not familiar with it, you will find managed instances at companies such as StreamNative, Datastax, or CleverCloud.</p>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#conclusion","title":"Conclusion","text":"<p>While the choreography pattern for event-driven applications is widely used, the orchestrator pattern offers significant advantages by centralizing process flow management within dedicated workflow services. This approach promotes better decoupling of services, easier comprehension of business process implementations, and simpler handling of complex workflows, transactions, and process versioning. However, implementing the orchestrator pattern can be challenging due to increased complexity. Thankfully, frameworks like Infinitic greatly simplify adoption by providing the necessary boilerplate code allowing developers to focus on implementing business logic while enjoying the benefits of an orchestrator pattern with minimal overhead.</p>"},{"location":"abstract/data_architecture/event-driven-data-architecture/#read-mores","title":"Read Mores","text":"<ul> <li> An Easy Path From API-Based Microservices to An Event-Driven Architecture For Improved Reliability and Consistency</li> </ul> <ol> <li> <p>The Way We Are Building Event-Driven Applications is Misguided.  This is Why We Created Infinitic \u21a9</p> </li> </ol>"},{"location":"abstract/data_architecture/lambda-and-kappa-architecture/","title":"Lambda and Kappa Architecture","text":"<p>Data Engineers and Architects are constantly seeking ways to build scalable, fault-tolerant, and efficient data processing systems. Two prominent architectures that have emerged to address these challenges are Lambda and Kappa Architecture. In this article, we will explore these architectures, their key components, and how they relate to the traditional Data Warehouse.</p> <p></p>"},{"location":"abstract/data_architecture/lambda-and-kappa-architecture/#reference","title":"Reference","text":"<ul> <li>Lambda and Kappa Architecture</li> </ul>"},{"location":"abstract/data_architecture/modern-data-stack/","title":"Modern Data Stack","text":"<p> Modern Data Stack (MDS) is a collection of Software Tools and Cloud Services that used to Collect, Process, Store, and Analyze data.</p> DBT Ecosystem Overview <p>You can see the below diagram show a Modern Data Stack Ecosystem that include the Cloud Services more than the Open-Source Stacks.</p> Modern Data Stack Ecosystem <p> As such, the definition of a Modern Data Stack cannot be clearly stated since every business tries to adapt modern technologies to their requirements.</p> <p>However, there are definite features of the modern data stack that identify it:</p> <ul> <li>It\u2019s Cloud-Based, requires very little maintenance, is easy to install,   and can scale quickly with little effort.</li> <li>It can be used by Small and Medium-Sized Data Teams, as it has a lot of   out-of-the-box functionality and doesn't rely on the number of data professionals.</li> <li>It offers a Lot of Integration Opportunities for creating a comprehensive   data ecosystem.</li> </ul>"},{"location":"abstract/data_architecture/modern-data-stack/#getting-started","title":"Getting Started","text":"<p> Modern Data Stack advocates a lot of changes, but if not done right they can be painful, expensive, and risky. Most often when I talk to data stack owners in the enterprise, the question is not so much how one tool compares to another but whether they can live with the status quo or Have a strong enough desire to change.</p> <p>The Key Concepts:</p> <ul> <li>Easy to Try &amp; Deploy</li> <li>Massive Scalability; Data, Users, and Use-Cases</li> <li>Composable Data Stack</li> <li>Flexible Pricing</li> </ul> The Modern Open-Source Data Stack <p> Modern Data Stack allows you to build on top of the software of giants instead of adopting the \"not invented here syndrome\". Consider whether the business value from your analytics solution is coming from how well you manage your data pipelines and data infrastructure, versus how well you build analytics and AI products on top of your infrastructure.</p> <p>Overall, the Modern Data Stack centerpiece is about democratizing data usage: Making data more accessible, covering different dimensions of business, improving analytics capabilities, and simplifying the infrastructure.</p> Modern Data Stack Architecture <p> If the advantages of Modern Data Stack do not seem convincing enough, let's have a look at how it differs from the Traditional Data Stack.</p> Traditional Data Stack Modern Data Stack Has coupled structure Has modular structure Complex setup requiring large IT teams Less time on technical configuration Requires serious technical background Suitable for users without extensive technical background Contains traditional RDBMS Works with RDBMS as well as big data, unstructured data <p>The Modern Data Stack provides businesses with a bias for action. Creating a Modern Data Stack enables organizations to devote more time to analyzing their data and less time engineering their data processing pipelines.</p>"},{"location":"abstract/data_architecture/modern-data-stack/#read-mores","title":"Read Mores","text":"<p>-</p>"},{"location":"abstract/data_architecture/data_lakehouse/","title":"Data Lakehouse","text":""},{"location":"abstract/data_architecture/data_lakehouse/#getting-started","title":"Getting Started","text":""},{"location":"abstract/data_architecture/data_lakehouse/#examples","title":"Examples","text":"<ul> <li> 5 Brilliant Lakehouse Architectures from Tencent, WeChat, and More</li> </ul>"},{"location":"abstract/data_architecture/data_lakehouse/#read-mores","title":"Read Mores","text":"<ul> <li>https://behindthescenes.nocnoc.com/%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AB%E0%B8%A5%E0%B8%B1%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-data-lakehouse-%E0%B8%9A%E0%B8%99-aws-%E0%B8%97%E0%B8%B5%E0%B9%88-nocnoc-ceb2c7334443</li> <li> <p>https://medium.com/@midogax272/spark-submit-with-pyspark-and-aws-emr-serverless-6-9-0-aa451c3961e5</p> </li> <li> <p>https://medium.com/@karim.faiz/open-data-lakehouse-revolutionizing-data-management-2cecf94f95e3</p> </li> <li>Data Lake 101: Architecture</li> </ul>"},{"location":"abstract/data_architecture/data_lakehouse/data-lakehouse-policy-based-access-control/","title":"Data Lakehouse: Policy-Based Access Control (PBAC)","text":"<p>https://python.plainenglish.io/policy-based-access-control-pbac-what-it-is-and-why-you-need-it-in-your-modern-data-lakehouse-672e869c2082</p>"},{"location":"abstract/data_governance/","title":"Data Governance","text":"<p>Data Governance is a comprehensive approach that comprises the principles, practices and tools to manage an organization\u2019s data assets throughout their lifecycle. By aligning data-related requirements with business strategy, data governance provides superior data management, quality, visibility, security and compliance capabilities across the organization. Implementing an effective data governance strategy allows companies to make data easily available for data-driven decision-making while safeguarding their data from unauthorized access, and ensuring compliance with regulatory requirements.</p>"},{"location":"abstract/data_governance/#key-elements-of-data-governance","title":"Key Elements of Data Governance","text":"<ul> <li>Data cataloging</li> </ul> <p>Effective data governance requires knowledge of the data that exists within an   organization. This is where a data catalog comes in, as it provides a centralized   metadata repository for an organization\u2019s data assets. A data catalog allows   stakeholders to quickly discover, understand and access the data they need,   improving data-related activities such as discovery, governance and analytics.   It acts as a searchable index of all the data available, including information   about its format, structure, location and usage, providing semantic value to an   otherwise unidentifiable sea of information. Incorporating a data catalog into   a governance program can help organizations improve their data management, enhance   collaboration, reduce redundancy and ensure proper access controls and audit   information retrieval.</p>"},{"location":"abstract/data_governance/#why-is-data-governance-so-important","title":"Why is Data Governance so important?","text":"<ul> <li> <p>It can be a colossal waste of time when not done correctly\u2014 Think about organizing   a library with no books.</p> </li> <li> <p>It is foundational to data.</p> </li> <li> <p>It can save significant time and money when done correctly.</p> </li> <li> <p>It can help with the rapid adoption of new technologies or strategies.</p> </li> <li> <p>Mismanagement of data is a liability for an organization \u2014 Data that gets into   the wrong hands represents real business risk.</p> </li> </ul>"},{"location":"abstract/data_governance/#references","title":"References","text":"<ul> <li>Databricks: Data Governance</li> <li>Digging into Data Governance and Data Modeling</li> </ul>"},{"location":"abstract/data_governance/dgn-data-quality-framework-for-scale/","title":"Data Quality Framework for Scale","text":""},{"location":"abstract/data_governance/dgn-data-quality-framework-for-scale/#references","title":"References","text":"<ul> <li>GetDBT: Data Quality Framework</li> </ul>"},{"location":"abstract/data_governance/dgn-implement-process/","title":"How to implement a data governance process effectively","text":""},{"location":"abstract/data_governance/dgn-implement-process/#references","title":"References","text":"<ul> <li>https://www.thoughtspot.com/data-trends/data-governance/how-to-implement-data-governance</li> </ul>"},{"location":"abstract/data_governance/dgn-with-modern-data-team/","title":"Data Governance: Rethinking with Modern Data Teams","text":"<p>https://prukalpa.medium.com/special-edition-rethinking-data-governance-with-modern-data-team-43f2f05fe5f9</p>"},{"location":"abstract/data_management/","title":"Data Management","text":"<p>Quote</p> <p>Data Management is an umbrella term that covers everything put in place by an organization to manage information and utilize it to make decisions.<sup>1</sup></p> <p> A Data Management refers to the Practices, Architectural Techniques, Strategies, and Tools that manage, store, and analyze data throughout its lifecycle. Effective data management ensures that data is accurate, available, and accessible when needed while maintaining security and compliance with relevant regulations.</p> <p>For the trend of a Data Management that we should follow the sharing knowledge and research from the  Gartner Hype Cycle for Data Management. You should to know that a data management strategy can change and shift everytime because it depend on a data service or technology that was implemented by your organize.</p> Gartner Hype Cycle for Data Management 2023 <p>Note</p> <p>A Modern Data Management involves a set of practices, techniques, and technologies used to handle data as a valuable resource. Its aim is to ensure the availability, integrity, security, and usability of data within an organization.<sup>2</sup></p>"},{"location":"abstract/data_management/#getting-started","title":"Getting Started","text":"<p> The buzz word that has taken over pretty much every industry and large organization in the last decade is \"Data-Driven\". You can find millions of articles, studies or blogs out there on why you should adopt a data-driven culture in your organization. You can identify problems and opportunities faster than you could without data. You can analyze past and sometimes even real-time data to make informed decisions about the future. Ultimately when you're making an important decision, you want as much information and context as you can possibly get, so you can make the decision confidently.</p> <p>Being able to make data-driven decisions gives you an incredibly strong competitive advantage in the marketplace, because very few organizations are. Building a data-driven business is difficult but if you pull it off, it\u2019s way easier to become a leader in your market because you can leverage information your competitors just don\u2019t have.</p> <p>And on the flip side of that, if you lack any type of data management in your company - good luck staying competitive, it\u2019s probably not going to happen.</p> <p>Note</p> <p>In this New Vantage Partners report from 2022 on The State of Corporate Data Initiatives, it states that only 26.5% of organizations have achieved their data-driven goals.</p> <p> So if you're looking to get ahead and stay ahead, becoming data-driven should 100% be part of your plan. But building an effective data-driven culture only works when it\u2019s built on strong data management foundations.</p> <p>3 Pillars of a Strong Data Management Strategy:</p> <ol> <li>Data Governance</li> <li>Data Architecture</li> <li>Data Quality</li> </ol>"},{"location":"abstract/data_management/#strategies","title":"Strategies","text":""},{"location":"abstract/data_management/#data-governance","title":"Data Governance","text":"<p>Data Governance refers to all the organizational rules that are put in place around data, including roles and responsibilities of team members, access control, policies, procedures and standards.</p> <p>Everything that will enforce a high quality of data securely flowing through the organization. Governance is typically documented but looks different for every company depending on company size, what your systems are, and what types of data you're dealing with.</p> <p>Includes policies, procedures, and standards that ensure the appropriate use, management, and protection of data throughout its lifecycle. It also involves establishing roles and responsibilities for data management, as well as ensuring compliance with legal and regulatory requirements.</p> <ul> <li>Policies and Standards: Establishing clear policies for data usage, privacy, and security.</li> <li>Data Stewardship: Assigning responsibilities to ensure data quality and compliance.</li> </ul> <p>Read More about Data Governance</p>"},{"location":"abstract/data_management/#data-quality-and-consistency","title":"Data Quality and Consistency","text":"<p>Having accurate and up-to-date data is an essential part of being data-driven. All systems need to be monitored and data needs to be cleaned very carefully. The data you receive in reports needs to reflect what reality is, which is the whole point of being data driven, so the quality of the data is super important.</p> <p>Ensuring that data is accurate, complete, and consistent. It includes defining data quality metrics, establishing data quality rules, and implementing data profiling and cleansing tools and techniques.</p> <ul> <li>Data Cleansing: Regularly cleaning data to remove inaccuracies and inconsistencies.</li> <li>Data Validation: Implementing processes to ensure data accuracy and reliability.</li> </ul>"},{"location":"abstract/data_management/#data-integration-transformation","title":"Data Integration &amp; Transformation","text":"<p>Consolidating data from multiple sources into a single, unified view of the data. It includes selecting appropriate data integration tools, defining data mapping and transformation rules, and establishing data synchronization and replication protocols.</p> <ul> <li>ETL/ELT Processes</li> <li>Batch and Stream Processing</li> <li>Real-Time Data Integration</li> <li>Data Transformation Techniques</li> </ul> <p>Read More about Data Integration &amp; Data Transformation</p>"},{"location":"abstract/data_management/#data-lifecycle-management","title":"Data Lifecycle Management","text":"<ul> <li>Archiving and Retention: Implementing policies for data archiving and retention   based on data usage and legal requirements.</li> <li>Disposal: Securely disposing of data that is no longer needed.</li> </ul>"},{"location":"abstract/data_management/#data-security-and-privacy","title":"Data Security and Privacy","text":"<p>This involves ensuring the confidentiality, integrity, and availability of data. It includes establishing data security policies and procedures, implementing access controls and encryption, and complying with legal and regulatory requirements.</p> <ul> <li>Advanced Encryption: Using cutting-edge encryption techniques to protect data.</li> <li>Regulatory Compliance: Ensuring adherence to global data protection regulations   like GDPR, CCPA, and others.</li> <li> <p>Zero Trust Architecture: Implementing security models that verify every access   request as if it originated from an open network.</p> </li> <li> <p>Data Encryption and Masking</p> </li> <li>Access Control and Authentication</li> <li>Compliance and Regulatory Requirements</li> <li>Privacy-Preserving Data Processing</li> </ul>"},{"location":"abstract/data_management/#data-architecture","title":"Data Architecture","text":"<p>The next important pillar is Data Architecture. This is the actual design of the systems involved in how data flows through your company and gets where it needs to be, when it needs to be there. Broadly speaking this includes your infrastructure which could be in the cloud or big server rooms at your company and all the applications running on those servers which are responsible for making the data flow from place to place. It\u2019s the hardware and software.</p> <p>Read More about Data Architecture</p>"},{"location":"abstract/data_management/#data-democratization","title":"Data Democratization","text":"<ul> <li>Self-service Analytics: Providing tools and platforms that enable non-technical   users to access and analyze data.</li> <li>Data Literacy Programs: Promoting data literacy across the organization to empower   employees to make data-driven decisions.</li> </ul>"},{"location":"abstract/data_management/#data-storage","title":"Data Storage","text":"<p>Data Storage on this part we will focus on below list:</p> <ul> <li>Data Warehouse</li> <li>Data Mart</li> <li>Operation Data Store</li> <li>Data Lake</li> </ul> EDW ODS DM Purpose Serves the entire organization Supports operational reporting Serves a specific business unit/department Data Integration Integrates data from multiple sources Integrates real-time data from transactional systems Integrates data from a specific subject area Data Model Top-down approach to design Bottom-up approach to design Designed based on specific business requirements Complexity More complex and time-consuming to design and implement Less complex and quicker to implement Less complex and quicker to implement Query and Analysis Supports complex queries and analytics Supports operational reporting and simple analysis Optimized for querying and reporting on a specific subject area Data Volume Large volume of historical data Real-time or near-real-time data Smaller volume of data Users Business analysts, executives, data scientists Operational staff, business analysts Business analysts, departmental staff Cost Higher cost due to complexity and scale Lower cost due to simpler design and implementation Lower cost due to simpler design and implementation Criteria EDW ODS DM Scope Enterprise-wide Operational Departmental or functional Data sources Multiple internal and external sources Multiple operational sources EDW, ODS, or other sources Data integration High degree of integration and standardization Moderate degree of integration and standardization Low degree of integration and standardization Data granularity Mixed levels of granularity Low level of granularity (detailed) High level of granularity (aggregated or summarized) Data currency Historical and current data Near real-time or real-time data Historical and current data Data quality High quality (cleansed and validated) Moderate quality (some cleansing and validation) High quality (cleansed and validated) Data structure Relational or dimensional models Relational models Dimensional models Data volume Very large (terabytes or petabytes) Large (gigabytes or terabytes) Small or medium (megabytes or gigabytes) Query performance Moderate to high (depends on indexing and partitioning) Low to moderate (depends on updates and concurrency) High (optimized for analysis) Query complexity High (supports complex and ad-hoc queries) Low to moderate (supports simple and predefined queries) Moderate to high (supports complex and ad-hoc queries) Query frequency Low to moderate (periodic or on-demand) High (continuous or near-continuous) Moderate to high (periodic or on-demand) User types Analysts, managers, executives, data scientists, etc. Operational staff, managers, etc. Analysts, managers, etc."},{"location":"abstract/data_management/#conclusion","title":"Conclusion","text":"<p>Overall, a Data Management strategy is a comprehensive approach to managing data that includes a range of components designed to ensure the effective use, management, and protection of data throughout its lifecycle.</p> <ol> <li> <p>CNDI: What is Data Management \u21a9</p> </li> <li> <p>Modern Data Management: 8 Things You Can Gain From It \u21a9</p> </li> </ol>"},{"location":"abstract/data_management/data-denormalization-form/","title":"De-Normalization","text":"<p>De-normalization is an optimization technique to make our database respond faster to queries by reducing the number of joins needed to satisfy user needs.</p> <ul> <li> <p>In de-normalization, we mainly aim to reduce the number of tables that are needed   by re-joining these tables together and add redundant data.</p> </li> <li> <p>De-normalization is commonly used with read-intensive, low number of updates and   high number of read queries, systems such as Data Warehouse (DWH).</p> </li> </ul> <p>Quote</p> <p>De-normalization is a strategy used on a previously-normalized database to increase performance. In computing, denormalization is the process of trying to improve the read performance of a database, at the expense of losing some write performance, by adding redundant copies of data or by grouping data.</p> <p>It is often motivated by performance or scalability in relational database software needing to carry out very large numbers of read operations. Denormalization should not be confused with Unnormalized form. Databases/tables must first be normalized to efficiently denormalize them.</p> <p>By Wiki Denormalization</p>"},{"location":"abstract/data_management/data-denormalization-form/#getting-started","title":"Getting Started","text":"<p>De-normalization doesn't mean that we won't normalize our tables, It's like we said before, an optimization technique that used after normalizing our table to make it faster in some cases.</p>"},{"location":"abstract/data_management/data-denormalization-form/#de-normalization-data-warehouses","title":"De-Normalization &amp; Data Warehouses","text":"<p>De-normalization is stable and commonly used with data warehouses, Data warehouse is a Specially created data repository for decision-making, It involves a large historical data repository related to the organization, The typical data warehouse is a subject-oriented corporate database that involves multiple data models implemented on multiple platforms and architectures.</p> <p>There are some aspects to consider when building data warehouses:</p> <ul> <li> <p>Extraction of data from several sources that may be homogeneous or heterogeneous.</p> </li> <li> <p>Initialize data for compatibility in the data warehouse.</p> </li> <li> <p>Cleaning the data with validity, and is done through the database from which the data were taken.</p> </li> <li> <p>Monitor and control the data warehouse while it is uploading data.</p> </li> <li> <p>Update data every period of time.</p> </li> </ul> <p>De-normalization can increase the speed of retrieval and optimize query performance for data warehouses, with some drawbacks of update anomalies, but it won't be a big problem because data warehouse is not typically used for update, It's commonly used for reading operations, which are used for analysis and decision-making as we said, hence, a data warehouse is a great source for applying de-normalization, because it attributes rarely updated.</p> Use Case: Star Schema <p>Star schema is the simplest and easiest dimensional model for data warehouses, because of that it's the most suitable schema for query processing, and it's highly de-normalized, but its drawback is its need for a large space to store data.</p> <p>Star schema consists of a fact table with a single table for each dimension.</p> <ul> <li>A fact table in a pure star schema consists of multiple foreign keys, each paired   with a primary key in a dimension, together with the facts containing the measurements.</li> <li>Typically, normalized.</li> <li>Dimension tables not joined for each other.</li> <li>It joined using a fact table that does have a foreign key for each dimension.</li> <li>Typically, heavily de-normalized.</li> </ul> <p></p>"},{"location":"abstract/data_management/data-denormalization-form/#drawbacks","title":"Drawbacks","text":"<ul> <li>De-normalization can slow updates, with many update overheads.</li> <li>De-normalization can increase your table and database size.</li> <li>De-normalization in some cases can make querying more complex instead of making   it easier.</li> <li>More storage will be needed for duplicated data.</li> </ul>"},{"location":"abstract/data_management/data-denormalization-form/#techniques","title":"Techniques","text":"<p>To de-normalize our normalized table, We will follow some methods that will be discussed below.</p> <p>Before doing de-normalization you have to make sure of two things:</p> <ul> <li>The performance of the normalized system doesn't satisfy the user.</li> <li>De-normalization is the right solution for this performance issue.</li> </ul> <p>So briefly, De-normalization is used for:</p> <ul> <li>Reduce the number and need for joins.</li> <li>Reduce the number of needed tables.</li> <li>Reduce foreign keys of your database.</li> </ul>"},{"location":"abstract/data_management/data-denormalization-form/#adding-redundant-columns","title":"Adding Redundant Columns","text":"<p>We can apply it by adding commonly used columns in joins to the joined table for reducing or eliminating join operations.</p> <p>Example</p> <p>If we have a customers table and orders table, the orders table does have customer_id only (as a foreign key) that referenced to customers table, but It doesn't have customer_name. When we need to retrieve a list of all orders with the customer name, we will have to join these tables together.</p> <pre><code>SELECT\n    C.CUSTOMER_NAME,\n    O.ORDER_NAME\nFROM CUSTOMERS  AS C\nJOIN ORDERS     AS O\n    ON C.CUSTOMER_ID = O.CUSTOMER_ID\n;\n</code></pre> <p>To de-normalize this table, we will add a redundant customer_name column to Orders, Which will increase performance, and we won't need to join this table again.</p> <pre><code>SELECT\n    O.CUSTOMER_NAME,\n    O.ORDER_NAME\nFROM ORDERS O\n;\n</code></pre> <p>Drawbacks of this method:</p> <ul> <li>Maintenance will be costly, and increase update overhead, because updates will   be made for two tables: customers(name) and orders(customer_name).</li> <li>More storage will be needed because customer_name is duplicated now.</li> <li>Increase in Table Size.</li> <li>In case of an update in one value of a customer, you have to update all the   records of that customer in the fact table which will be a very costly operation   for big Fact tables.</li> </ul>"},{"location":"abstract/data_management/data-denormalization-form/#coming-tables","title":"Coming Tables","text":"<p>We can apply it by combine tables that are joined together frequently into one table, which will eliminate join, and increase performance significantly.</p> <p>Example</p> <p>If we frequently need to generate a report that shows all Employees their full addresses, we will have to join these tables together.</p> <pre><code>SELECT E.* , A.CITY+\", \"+A.STATE+\", \"+A.DISTRICT+\", \"+A.ZIP AS \"FULL ADDRESS\"\nFROM EMPLOYEES AS E\nJOIN ADDRESSES AS A\n  ON A.LOCATION_ID = e.LOCATION_ID;\n</code></pre> <p>So to de-normalize this situation, We will combine employees' table and addresses table into one table, And combine address table attributes into one attribute (Full address) to make querying easier, This solution will increase performance significantly, and eliminate costly joins.</p> <pre><code>SELECT * FROM EMPLOYEES;\n</code></pre>"},{"location":"abstract/data_management/data-denormalization-form/#adding-derived-column","title":"Adding Derived Column","text":"<p>Adding derived columns to our table can help us to eliminate joins, and improve the performance of aggregating our data.</p> <p>A derived column is attribute whose value is derived from another attribute. For example: Using date_of_birth attribute to generate age attribute.</p> <p>Example</p> <p>If we need to generate a report containing Students and their grades, We will join students table and grades table, and start comparing marks with grades to know the grade of each student.</p> <pre><code>SELECT S.NAME, S.MARKS, G.GRADE\nFROM STUDENTS S\nJOIN GRADES G\nON S.MARKS BETWEEN G.MIN_MARK AND G.MAX_MARK;\n</code></pre> <p>So to de-normalize this situation, We will use marks in students' table and compare them to predefined values to generate grades with no need of joining grades with students.</p> <pre><code>SELECT NAME, MARKS,\nCASE\n  WHEN MARKS &gt;= 85 AND MARKS &lt;= 100 THEN \"A\"\n  WHEN MARKS &gt;= 75 AND MARKS  &lt; 85  THEN \"B\"\n  WHEN MARKS &gt;= 65 AND MARKS  &lt; 75  THEN \"C\"\n  WHEN MARKS &gt;= 50 AND MARKS  &lt; 65  THEN \"D\"\n  ELSE \"F\"\nEND AS \"GRADE\"\nFROM STUDENTS;\n</code></pre>"},{"location":"abstract/data_management/data-denormalization-form/#partitioning-relation","title":"Partitioning Relation","text":"<p>In this approach, We won't combine tables together, We will go for decomposing them into multiple smaller manageable tables, It will decrease the size that we have to read, which will impact the performance of operations in some meaningful way.</p> HorizontalVertical <p>Horizontal partitioning or Row Splitting. Split our main table rows into smaller partitions (tables) that will have the same columns.</p> <p>This approach aims to make where clause more efficient by making it search in a smaller amount of data, Filter specify only a subset of the table that related to the query, not the whole table, and reduced I/O overhead.</p> <p>Vertical partitioning or Column Splitting. In Vertical partitioning, We distribute table attributes across multiple partitions with primary key duplicated for each partition to make reconstructing of original table easier. We partition our table based on frequently used attributes and rarely used attributes.</p> <p>We need to use this approach When some columns are frequently accessed more than other columns, To reduce table header size, And retrieve only the required attributes.</p> <p>Warning</p> <p>In case you have multiple requirements that needs the data combined you will have to join the tables again which will cause performance problems.</p>"},{"location":"abstract/data_management/data-denormalization-form/#materialized-views","title":"Materialized Views","text":"<p>Materialized Views can improve performance and decrease time-consuming significantly, by using it to precomputing and store the result of costly queries like join and aggregation as view in your storage disk for future usage.</p> <p>Note</p> <p>Materialized View is all about run one time, and read many times.</p> <p>When you need to use a query frequently, you can store it as materialized view, So in the future, you can retrieve the result of your query directly from the view stored in your disk, So you don't need to recompute query again.</p> <p>But you should to know that,</p> <ul> <li>Data will be updated once, And to refresh you have to re-run the query again.</li> <li>The unavailable source will block maintenance of view.</li> <li>Data are replicated, And it needs more storage.</li> </ul>"},{"location":"abstract/data_management/data-denormalization-form/#conclusion","title":"Conclusion","text":"<p>De-normalization aims to add redundancy to your database for better performance, It also a great optimization technique that will help you to decrease query processing time. with drawbacks of reducing the integrity of your system, slowing data manipulation operations, and need more space for storing redundant data.</p>"},{"location":"abstract/data_management/data-denormalization-form/#read-mores","title":"Read Mores","text":"<ul> <li>DataValley: De-normalization When, Why, and How</li> </ul>"},{"location":"abstract/data_management/data-integration/","title":"Data Integration","text":"<p>Quote</p> <p>How should developers and data practitioners start to incorporate environmental factors when developing end-to-end data solutions?<sup>1</sup></p>"},{"location":"abstract/data_management/data-integration/#getting-started","title":"Getting Started","text":""},{"location":"abstract/data_management/data-integration/#sustainable-data-ingestion","title":"Sustainable Data Ingestion","text":""},{"location":"abstract/data_management/data-integration/#data-minimization","title":"Data minimization","text":"<p>Ensuring that only required data is needed for a data ingestion process mitigates excess energy consumption. This can be on a full dataset or on a table field level. Before any data source connection, the data needed should be explicitly listed and reviewed. In some cases, developers may choose to ingest all the data so as not to run into the process of further adjustments or features to the pipelines, which may result in short term effort reduction yet increasing compute and data transfer long term. Data minimization can apply both to the data ingestion and serving level.</p> <ul> <li>Ensure that only required data is ingested.</li> </ul>"},{"location":"abstract/data_management/data-integration/#scheduling","title":"Scheduling","text":"<p>Data requirements collected will contain freshness criteria. That is consumers are expecting to have this data up to date within a needed time interval. Ingestion of data in real-time can incur large costs and increase energy consumption. It\u2019s important that requirements are well reviewed beforehand, to ensure that such costs are mitigated if SLAs (service level agreements) do not meet real-time data requirements.</p> <ul> <li>Review data freshness requirements and adjust frequency of ingestion accordingly.</li> </ul>"},{"location":"abstract/data_management/data-integration/#incremental-data-loading","title":"Incremental Data Loading","text":"<p>Loading data from source systems can present hurdles depending on the data characteristics and the system itself. Full batch data loading involves loading the same data repeatedly to capture newly added or updated records. This approach results in redundancy and can require significant compute resources to complete. Strategies to employ here should incorporate incremental loading strategies to ensure only new and updated records are captured and ingested.</p> <ul> <li>Load and update data incrementally in consistent small batches.</li> <li>Ensure data integrity with consistency checks on each incremental run.</li> </ul>"},{"location":"abstract/data_management/data-integration/#idempotency","title":"Idempotency","text":"<p>Data pipelines are suspectable to failures due to many factors, those can range from data source changes to sudden data size increase, downtimes etc ... Re-triggering failures without pipeline idempotent properties will compromise data integrity and can result in duplicate and inconsistent data. Such properties can include crucial checks on previous pipeline runs, which records were already processed on timestamps and data batch ids.</p> <ul> <li>Regularly checkpoint data pipeline ingestion progress</li> <li>Ensure consistent logging of all pipelines for efficient debugging</li> </ul>"},{"location":"abstract/data_management/data-integration/#sustainable-data-transformation","title":"Sustainable Data Transformation","text":""},{"location":"abstract/data_management/data-integration/#data-recency","title":"Data Recency","text":"<p>Operational systems can sustain failure and thus temporarily house stale data. Initial data pipeline ingestion stages can succeed in detecting such states and avoid redundant data imports. This does not necessarily represent a failed run. Transformation stages are often triggered after successful ingestion stage, which makes them susceptible to processing stale and already processed raw data.</p> <p>Implement data recency checks throughout the data transformation stages.</p>"},{"location":"abstract/data_management/data-integration/#divide-conquer","title":"Divide &amp; Conquer","text":"<p>Distributed big data processing ensures efficient data transformations and high compute utilization. Consistent monitoring of compute clusters utilization is crucial to prevent waste of compute power and thus energy.</p> <p>Common best practices to ensure that data fulfills utilization requirements of distributed compute:</p> <ul> <li>Ensure uniform data distribution and avoid data skew across partitions for   full compute utilization.</li> <li>Ensure adequate data volume for the distributed compute resources assigned.</li> <li>Take into consideration data locality to reduce cross-node communication.</li> </ul>"},{"location":"abstract/data_management/data-integration/#compute-specs","title":"Compute Specs","text":"<p>Data volume and transformation play a huge role in deciding compute specifications. Assigning a medium/large compute specs can typically handle all data use cases for most companies. While it is easy to choose the default specs without closely monitoring usage and utilization, such practice could lead to idle compute and thus energy consumption for apparent benefit.</p> <ul> <li>Monitor compute usage and utilization and adjust specs accordingly.</li> <li>Provision auto-scaling compute resources where possible.</li> </ul>"},{"location":"abstract/data_management/data-integration/#code-optimizations","title":"Code Optimizations","text":"<p>Working with efficient and optimized code helps in mitigating technology and resource limitations to further reduce energy consumptions. Modern tools including open-source projects offer support to optimize and plan data queries to run optimally (e.g. Apache Spark..).</p> <ul> <li>Optimize code for parallel processing to maximize efficiency and speed.</li> <li>Review and structure code and queries utilizing frameworks and tools to improve   planning and performance.</li> </ul>"},{"location":"abstract/data_management/data-integration/#sustainable-data-storage","title":"Sustainable Data Storage","text":""},{"location":"abstract/data_management/data-integration/#data-retention","title":"Data Retention","text":"<p>It is important when collecting data requirements that data is classified with the right retention tag. Those are based on a standard data management retention strategy ensuring that only necessary data is stored for a specified period. This helps in managing and optimizing data storage resources thus moving towards reducing further CO2 emissions.</p> <ul> <li>Ensure data retention strategies are incorporated in the overall data management   policies and applied.</li> <li>Ensure that data assets classifications are streamlined across the data lifecycle.</li> </ul>"},{"location":"abstract/data_management/data-integration/#data-compression","title":"Data compression","text":"<p>Efficient data file formats support powerful compression capabilities and for large data can immensely reduce storage requirements. Especially for long term data storage, having it in more sustainable formats that also support efficient querying like Parquet, ORC, Delta, and Avro etc</p>"},{"location":"abstract/data_management/data-integration/#sustainable-data-serving","title":"Sustainable Data Serving","text":""},{"location":"abstract/data_management/data-integration/#optimal-compute","title":"Optimal Compute","text":"<p>Technologies nowadays offer capabilities to manage idle compute resources efficiently, such as automatic shutdown functionalities or scheduled power activation. The helps in ensuring that compute are not always consuming resources when not needed, especially when self-service consumers are active during specific hours of the day.</p> <ul> <li>Ensure compute resources are automatically powered off during idle times.   (if it is feasible to do so)</li> <li>Cap the total runtime limit for queries to prevent excessive resource consumption.</li> </ul>"},{"location":"abstract/data_management/data-integration/#caching","title":"Caching","text":"<p>Introducing a caching layer reduces the need to repeatedly retrieve data from data storage therefore ensuring faster response time and reduced energy consumption. Applications should incorporate caching capabilities where possible for providing scalable and sustainable request handling.</p>"},{"location":"abstract/data_management/data-integration/#flexible-data-querying","title":"Flexible Data Querying","text":"<p>It\u2019s crucial that consumers select and filter by their needed data fields. Many applications limit selection and filtering capabilities for their interfaces (e.g. APIs) thereby preventing consumers from only consuming data they actually need.</p> <ul> <li>Provide flexible interfaces for consumers to easily select and filter data.</li> </ul>"},{"location":"abstract/data_management/data-integration/#boost-expertise","title":"Boost Expertise","text":"<p>Data consumers are often unaware of the background compute resources required to serve their queries. Complex queries may require a few seconds to complete yet require heavy compute resources. It is important to bring awareness to consumers on the impact their querying activities have.</p> <ul> <li>Regular training (if possible) for consumers on query optimizations and resource   allocation insights.</li> </ul>"},{"location":"abstract/data_management/data-integration/#sustainable-technology","title":"Sustainable Technology","text":""},{"location":"abstract/data_management/data-integration/#research-providers","title":"Research Providers","text":"<p>Technology providers often limit comprehensive reporting on their CO2 footprint. This trend nonetheless is changing, and more companies are indeed delivering CO2 emissions statistics and aiming for increased sustainability, which makes the selection strategy with environmental criteria simpler to manage. In addition to the emergence of tools that one can use to calculate carbon footprint of certain applications.</p> <ul> <li>Ensure that environmental considerations are incorporated into technology   selection criteria.</li> <li>Estimate CO2 emissions of your applications and set goals to reduce them.</li> </ul>"},{"location":"abstract/data_management/data-integration/#the-role-of-open-source","title":"The role of open-source","text":"<p>Open-source projects provide complete transparency which helps users have control and visibility over their resource consumption. On a code level, it further provides opportunity to profile and further optimize existing code to one\u2019s use cases.</p> <ul> <li>Be open to using open-source projects that can potentially fulfill project   requirements.</li> <li>Review open-source projects and try to optimize code to your requirements.</li> </ul>"},{"location":"abstract/data_management/data-integration/#read-mores","title":"Read Mores","text":"<ul> <li>Data Ingestion: Architectural Patterns</li> </ul> <ol> <li> <p>Data Engineering: Towards Sustainable Integration Patterns \u21a9</p> </li> </ol>"},{"location":"abstract/data_management/data-lifecycle-management/","title":"Data Lifecycle Management","text":""},{"location":"abstract/data_management/data-lifecycle-management/#read-mores","title":"Read Mores","text":"<ul> <li>8 stages of Data lifecycle management</li> </ul>"},{"location":"abstract/data_management/data-mart/","title":"Data Mart","text":"<p>Data Mart is a subset of the data warehouse. It specially designed for a particular line of business, such as sales, finance, sales or finance. In an independent data mart, data can collect directly from sources.</p> <p>Note</p> <p>Data Mart is also a storage component used to store data of a specific function or part related to a company by an individual authority, so data marts are flexible and small.</p>"},{"location":"abstract/data_management/data-mart/#getting-started","title":"Getting Started","text":"<p>A Data Mart is a subset of an EDW that is designed to serve a specific business unit or department. It is optimized for querying and reporting on a specific subject area, such as sales or marketing, and it is typically easier and faster to implement than an EDW.</p> <p>Quote</p> <p>In truth, the Kimball model was for data marts, not a data warehouse. A data mart and a data warehouse are fundamentally different things. \u2014 Bill Inmon</p> <p>Quote</p> <p>A data mart is a curated subset of data often generated for analytics and business intelligence users. Data marts are often created as a repository of pertinent information for a subgroup of workers or a particular use case. \u2014 Snowflake</p> <p> A data mart (as noted above) is a focused version of a data warehouse that contains a smaller subset of data important to and needed by a single team or a select group of users within an organization. A data mart is built from an existing data warehouse (or other data sources) through a complex procedure that involves multiple technologies and tools to design and construct a physical database, populate it with data, and set up intricate access and management protocols.</p>"},{"location":"abstract/data_management/data-mart/#who-uses-a-data-mart-and-how","title":"Who uses a data mart (and how)?","text":"<p> Data marts guide important business decisions at a departmental level. For example, a marketing team may use data marts to analyze consumer behaviors, while sales staff could use data marts to compile quarterly sales reports. As these tasks happen within their respective departments, the teams don't need access to all enterprise data.</p> <p>Typically, a data mart is created and managed by the specific business department that intends to use it. The process for designing a data mart usually comprises the following steps:</p> <ol> <li> <p>Document essential requirements to understand the business and technical needs     of the data mart.</p> </li> <li> <p>Identify the data sources your data mart will rely on for information.</p> </li> <li> <p>Determine the data subset, whether it is all information on a topic or specific     fields at a more granular level.</p> </li> <li> <p>Design the logical layout for the data mart by picking a schema that correlates     with the larger data warehouse.</p> </li> </ol> <p> With the groundwork done, you can get the most value from a data mart by using specialist business intelligence tools, such as Qlik or SiSense. These solutions include a dashboard and visualizations that make it easy to discern insights from the data, which ultimately leads to smarter decisions that benefit the company.</p>"},{"location":"abstract/data_management/data-mart/#types","title":"Types","text":"<p> There are three types of data marts that differ based on their relationship to the data warehouse and the respective data sources of each system.</p> <ul> <li> <p>Dependent data marts are partitioned segments within an enterprise data     warehouse.     This top-down approach begins with the storage of all business data in one     central location. The newly created data marts extract a defined subset of     the primary data whenever required for analysis.</p> </li> <li> <p>Independent data marts act as a standalone system that doesn't rely on     a data warehouse.     Analysts can extract data on a particular subject or business process from     internal or external data sources, process it, and then store it in a data     mart repository until the team needs it.</p> </li> <li> <p>Hybrid data marts combine data from existing data warehouses and other     operational sources.     This unified approach leverages the speed and user-friendly interface of a     top-down approach and also offers the enterprise-level integration of the     independent method.</p> </li> </ul>"},{"location":"abstract/data_management/data-mart/#structure","title":"Structure","text":"<p> A data mart is a subject-oriented relational database that stores transactional data in rows and columns, which makes it easy to access, organize, and understand. As it contains historical data, this structure makes it easier for an analyst to determine data trends. Typical data fields include numerical order, time value, and references to one or more objects.</p> <p>Companies organize data marts in a multidimensional schema as a blueprint to address the needs of the people using the databases for analytical tasks. The three main types of schema are star, snowflake, and vault.</p>"},{"location":"abstract/data_management/data-mart/#star","title":"Star","text":"<p>Star schema is a logical formation of tables in a multidimensional database that resembles a star shape. In this blueprint, one fact table\u2014a metric set that relates to a specific business event or process\u2014resides at the center of the star, surrounded by several associated dimension tables.</p> <p>There is no dependency between dimension tables, so a star schema requires fewer joins when writing queries. This structure makes querying easier, so star schemas are highly efficient for analysts who want to access and navigate large data sets.</p>"},{"location":"abstract/data_management/data-mart/#snowflake","title":"Snowflake","text":"<p>A snowflake schema is a logical extension of a star schema, building out the blueprint with additional dimension tables. The dimension tables are normalized to protect data integrity and minimize data redundancy.</p> <p>While this method requires less space to store dimension tables, it is a complex structure that can be difficult to maintain. The main benefit of using snowflake schema is the low demand for disk space, but the caveat is a negative impact on performance due to the additional tables.</p>"},{"location":"abstract/data_management/data-mart/#vault","title":"Vault","text":"<p>Data vault is a modern database modeling technique that enables IT professionals to design agile enterprise data warehouses. This approach enforces a layered structure and has been developed specifically to combat issues with agility, flexibility, and scalability that arise when using the other schema models.</p> <p>Data vault eliminates star schema's need for cleansing and streamlines the addition of new data sources without any disruption to existing schema.</p>"},{"location":"abstract/data_management/data-mart/#read-mores","title":"Read Mores","text":"<ul> <li>IBM: Data Mart</li> </ul>"},{"location":"abstract/data_management/data-model/","title":"Data Model","text":"<p>Data Model is essentially a blueprint for a building designed by an architect. It is the technique of documenting complex software system designs in the form of an easily understandable graphic. To describe how the data will flow, the diagram will be made using text and symbols.</p> Overall Data Modeling <p>Note</p> <p>Data Modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures.<sup>1</sup></p>"},{"location":"abstract/data_management/data-model/#types-of-data-models","title":"Types of Data Models","text":"Types of Data Models"},{"location":"abstract/data_management/data-model/#conceptual-data-models","title":"Conceptual Data Models","text":"<p> They are also referred to as domain models and offer a big-picture view of what the system will contain, how it will be organized, and which business rules are involved. Conceptual models are usually created as part of the process of gathering initial project requirements.</p> <p>Typically, they include entity classes (defining the types of things that are important for the business to represent in the data model), their characteristics and constraints, the relationships between them and relevant security and data integrity requirements. Any notation is typically simple.</p> <p></p>"},{"location":"abstract/data_management/data-model/#logical-data-models","title":"Logical Data Models","text":"<p> They are less abstract and provide greater detail about the concepts and relationships in the domain under consideration. One of several formal data modeling notation systems is followed. These indicate data attributes, such as data types and their corresponding lengths, and show the relationships among entities. Logical data models don't specify any technical system requirements.</p> <p>This stage is frequently omitted in agile or DevOps practices. Logical data models can be useful in highly procedural implementation environments, or for projects that are data-oriented by nature, such as data warehouse design or reporting system development.</p> <p></p>"},{"location":"abstract/data_management/data-model/#physical-data-models","title":"Physical Data Models","text":"<p> They provide a schema for how the data will be physically stored within a database. As such, they're the least abstract of all. They offer a finalized design that can be implemented as a relational database, including associative tables that illustrate the relationships among entities as well as the primary keys and foreign keys that will be used to maintain those relationships.</p> <p>Physical data models can include database management system (DBMS)-specific properties, including performance tuning.</p> <p></p>"},{"location":"abstract/data_management/data-model/#how-can-define-a-good-data-model","title":"How Can Define a Good Data Model?","text":"<p>In other words, how can we compare various Data Modeling options? What Factors should be taken into account?</p> <ol> <li> <p>Performance    This is a vast topic, and we are not discussing database vendors, data indexing,    or technical modifications to boost read and write speeds. I believe we can    ascribe performance advantages solely based on how we model the data.</p> </li> <li> <p>Productivity    On the developer side, we want a model that is simple to work with and reason about,    so we can \"create a lot of good code\" without wasting time (the concept of productivity).</p> </li> <li> <p>Clearness    The Data Model\u2019s ability to be comprehended by those who look at it. As you may    have heard, most developers read code rather than write it, therefore we must    clearly grasp what we are doing with our data.</p> </li> <li> <p>Flexibility    The Model\u2019s capacity to evolve without having a significant influence on our code.    Because the startup you work for is evolving, the systems and Data Models that    power it will need to evolve as well.</p> </li> <li> <p>Traceability    Finally, we want to have data that is useful to the system as well as data that    is valuable to our users. Knowing what happened in the past, what values the    entities had at some point in time, being able to travel back and forth in time,    and so on.</p> </li> </ol> <p>Example</p> <p>Standard Data Modeling (a.k.a. Domain Models)</p> <p>It\u2019s as simple as this: The domain of the problem you\u2019re solving dictates how you define your entities and their properties. As a result, each entity will be comparable to other entities in the same domain while being significantly different from entities in other domains. As an example, we may have a distinct sort of box for each type of item that we want to store.</p> <p>This type of modeling is obvious since it is specified in the same way that we think about the problem.</p> <p>So, let\u2019s run the first test on our five dimensions of analysis (points 1\u201310):</p> Performance: <code>6 points</code> <p>This type of model does not perform well, as we will demonstrate later.</p> Productivity: <code>3 points</code> <p>Each collection (or table) will require its own function to update the values in each field, correct? This is not helpful for developer productivity unless you create a middleware to communicate with the database in a \"parametric fashion,\" which is equally unnatural. We shall offer a better method of accomplishing this.</p> Clearness: <code>10 points</code> <p>Yes, the model is crystal clear, precisely as humans think.</p> Flexibility: <code>3 points</code> <p>However, things aren\u2019t going so well here. Each new field that is added necessitates a change to the model.</p> Traceability: <code>2 points</code> <p>This type of modeling updates fields in place, so if your address changes, you\u2019ll lose the previous one, right? The remedy is to have a distinct table (a log table) that records all changes, but it will be independent of the rest of the model and thus \"noisy.\"</p>"},{"location":"abstract/data_management/data-model/#read-mores","title":"Read Mores","text":"<ul> <li>Types of Data Model</li> </ul> <ol> <li> <p>:simple-ibm: What is data modeling? \u21a9</p> </li> </ol>"},{"location":"abstract/data_management/data-normalization-form/","title":"Normalization","text":"<p>Normalization is a database design technique that reduces data redundancy and eliminates undesirable characteristics like Insertion, Update and Deletion Anomalies. Normalization rules divides larger tables into smaller tables and links them using relationships. The purpose of Normalization in SQL is to eliminate redundant (repetitive) data and ensure data is stored logically.</p> <p>However, in most practical applications, normalization achieves its best in 3rd Normal Form (3NF).</p> <p>Quote</p> <p>Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model.</p> <p>Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).</p> <p>By Wiki Database Normalization</p>"},{"location":"abstract/data_management/data-normalization-form/#getting-started","title":"Getting Started","text":"<p>Abstract</p> primary key <p>A single column that uniquely identifies the records of data in that table. It\u2019s a unique identifier such as an employee ID, student ID, voter\u2019s identification number (VIN), and so on.</p> foreign key <p>A field that relates to the primary key in another table. Unlike the primary key, they do not have to be unique. most often they are not. foreign keys can be null even though primary keys can not.</p> composite key <p>A primary key composed of multiple columns used to identify a record uniquely.</p> candidate key <p>A field that able to be primary key.</p> <p>Prime &amp; Non-Prime</p> prime attribute <p>Attributes can be used to uniquely identify a tuple in the table because they have unique values. they also known as Key Attributes.</p> non-prime attribute <p>Attributes of the relation which does not exist in any of the possible candidate keys of the relation. They also known as Non-Key Attributes.</p> Example <pre><code>R = (H, I, J, K, L, M, N, O)\n\nF = {\n  L -&gt; MNO\n  HI -&gt; JKLMNO\n  J -&gt; KL\n  K -&gt; H\n}\n</code></pre> <p>This schema has three (candidate) keys:</p> <pre><code>HI\nIJ\nIK\n</code></pre> <p>While it is immediate to discover tha HI is a candidate key since it determines all the other attributes, you can see that this is true also for IJ and IK by calculating the closure of those attributes:</p> <pre><code>IJ+ = IJ\nIJ+ = IJKL     (by adding the right part of J \u2192 KL)\nIJ+ = IJKLH    (by adding the rigth part of K \u2192 H)\nIJ+ = IJKLHMNO (by adding the right part of L \u2192 MNO)\n</code></pre> <p>Note that it use the notation IJ+. This is called the closure, which means all the possible attributes that can be inferred from what we know in LHS.</p> <p>analogously for IK:</p> <pre><code>IK+ = IK\nIK+ = IKH      (by adding the rigth part of K \u2192 H)\nIK+ = IKHJLMNO (by adding the right part of HI \u2192 JKLMNO)\n</code></pre> <p>For this reason, the prime attributes of the relation are:</p> <pre><code>HIJK\n</code></pre> <p>while the non prime attributes are:</p> <pre><code>LMNO\n</code></pre> <p>Note that there are no other candidate keys since M, N and O appear only on the right side of functional dependencies, so that they cannot \u201ccontribute\u201d to any key. L appears both on a left part and on a right part of functional dependencies, but it is determined by HI, IJ and IK, and does not determine any of these attributes, so it cannot be part of a key. Finally, you cannot remove any attribute from HI, IJ and IK without losing the key property.</p> <p>Read more this example</p>"},{"location":"abstract/data_management/data-normalization-form/#what-is-anomalies","title":"What is Anomalies?","text":"<p>Anomalies are the problems that occur in the update, delete and insert operations in poorly designed or un-normalized data when all data stored in one table (Flat file).</p> <p>In simple words we can say anomaly is when you have one table which has multiple related information, and you cannot do any kind of operation on single information.</p> <p>Example of Anomalies:</p> <pre><code>R = (courseNo,  tutor,  lab, labSize){\n    (300,       Ahmed,  C3,  150),\n    (301,       John,   A1,  210),\n    (302,       Kamal,  C3,  150)\n}\n</code></pre> InsertDeleteUpdate <p>What if we built a new lab (e.g. <code>lab: A4</code>), but it\u2019s not assigned to any courses or tutors yet, so we won\u2019t be able to insert it to our table because (<code>lab</code>) comes with (<code>course</code>) and (<code>tutor</code>) because it does not have separated table.</p> <p>What if we need to delete (<code>courseNo: 300</code>) that means we will delete the details of (<code>lab: C3</code>) also and that\u2019s not what we want.</p> <p>What if we improved (<code>lab: C3</code>) and now it (<code>labSize: 250</code>), to update it we will have to update all other columns where (<code>lab: C3</code>).</p>"},{"location":"abstract/data_management/data-normalization-form/#why-do-we-need-to-normalize-our-tables","title":"Why do we need to normalize our tables?","text":"<ul> <li>When (ACID compliant) is required</li> </ul> <p>It improves integrity and consistency of your data.   (ACID = Atomicity Consistency Isolation Durability)</p> <ul> <li>Fewer storage needed</li> </ul> <p>Since we eliminated repeated groups, and divided our tables, we reduced the   size of our tables and database.</p> <ul> <li>less logical I/O cost</li> </ul> <p>When you need to retrieve data, you will retrieve smaller amount of data, and   when you need to add or insert in tables, it will be easier, and more organized.</p> <ul> <li>Queries become easier</li> </ul> <p>If we have un-normalized table that has (location) attribute {City, Zip} as   composite attribute, and we need to count the unique zip codes in our table,   so we will access first location, then we will try to get zip, after normalize   this table we will be able to access zip directly because location will be   divided to two attributes (city) and (zip).</p> <ul> <li>Write-intensive databases</li> </ul> <p>Normalization increases the performance of write-intensive databases Significantly,   because it reduces data modification anomalies, which make it easier to manipulate   your database.</p>"},{"location":"abstract/data_management/data-normalization-form/#dependency-rules","title":"Dependency Rules","text":"<p>Rule for call the attribute relationships</p>"},{"location":"abstract/data_management/data-normalization-form/#functional-dependency","title":"Functional Dependency","text":"<p>This is the primary key (single or composite) relationship for identified other attributes. When we say A is identified by B (B is a primary key), then A functionally dependent on B, can be represented graphically as (B -&gt; A)</p> Complete/Fully FunctionalPartialTransitiveMultivalued <p>Example</p> <pre><code>R = (customerID, name, salary){\n    (1,          Tom,  18,000),\n    (2,          June, 22,000),\n    (3,          Rose, 15,000)\n}\n</code></pre> <p><code>customerID -&gt; {name, salary}</code> is Fully Functional Dependency (FFD).</p> <ul> <li><code>name</code> and <code>salary</code> were identified by <code>customerID</code> and functionally   dependent on it.</li> <li><code>customerID</code> determines <code>name</code> and <code>salary</code>.</li> </ul> <p>When only one of the prime attributes determines another attribute with no exist of other prime attributes in this relation. OR when not all non-prime attributes depend on all prime attributes.</p> <p>Example</p> <p>{A, B} are our prime attributes, C is non-prime attribute and A -&gt; Z not {A, B} -&gt; C, so it's partial dependency because C is functionally dependent on only one prime attribute not all prime attributes.</p> <pre><code>R = (order, product, productName, quantity){\n    (1,     A,       table,       1),\n    (1,     B,       chair,       4),\n    (2,     A,       table,       2)\n}\n</code></pre> <p><code>{order, product} -&gt; {productName, quantity}</code> is Fully Functional Dependency (FFD) because of using for all key.</p> <p><code>product -&gt; productName</code> is being Partial Dependency because use one key from prime attributes.</p> <p>There is non-prime attribute functionally dependent on another non-prime attribute OR It means that changing a value in one column leads to a change in another column-columns other than prime attributes.</p> <p>A transitive functional dependency is when changing a non-key column, might cause any of the other non-key columns to change</p> <p>Note</p> <p>In transitive dependency non-prime attribute determines another non-prime attribute, In partial dependency when only one of the prime attributes determines another attribute with no exist of other prime attributes (because of that we called it partial dependency).</p> <p>Example</p> <p>We say that <code>A -&gt; C</code> is transitive dependency if it generated from <code>A -&gt; B</code> &amp; <code>B -&gt; C</code> not <code>A -&gt; C</code> directly.</p> <p>It usually a relationship consisting of 3 attributes (A, B, C). single value from (A) gives more than one value in (B), single value of (A) gives more than one value in (C), and (B) , (C) are independent of each other.</p> <ul> <li>There are at least 3 attributes A, B, C in a relation and</li> <li>For each value of A there is a well-defined set of values   for B, and a well-defined set of values for C,</li> <li>But the set of values for B is independent on the set of   values for C</li> </ul> <p>Example</p> <p>(emp_no , proj_no, dependents)</p> <p>(employee) do have many (projects), (employee) do have many ( dependents ) like his children, and it\u2019s obviously projects and his dependents are independent of each other, which means if we need to remove one of his projects we don't have to delete one of his dependent. so we have here multivalued dependency which violates 4NF.</p>"},{"location":"abstract/data_management/data-normalization-form/#join-dependency","title":"Join Dependency","text":"<p>Whenever we can recreate a table by simply joining various tables where each of these tables consists of a subset of the table\u2019s attribute, then this table is known as a Join Dependency.</p>"},{"location":"abstract/data_management/data-normalization-form/#orders","title":"Orders","text":"<p>The normalization process takes our relational schema throw a series or pipeline of tests to make sure that\u2019s it satisfy a certain normal form, this process proceeds in a top-down manner by evaluating our relational schema against the criteria of normal forms.</p>"},{"location":"abstract/data_management/data-normalization-form/#1nf-first-normal-form","title":"1NF: First Normal Form","text":"<p>First Normal Form (1NF) is the first step towards a full normalization of your data, to apply 1NF it should have the following criteria:</p> <ul> <li>Each column or single cell contains atomic values</li> <li>Each entity has a primary key</li> <li>No duplicated rows or columns</li> </ul> <p>In other words, each row in the table should have a unique identifier, and each value in the table should be indivisible.</p> <p>Example</p> <pre><code>R = (customerID, Address){\n    (1,          Main street, First Town, BKK 10100),\n    (2,          12 street, A Town, BKK 10120)\n}\n</code></pre> <pre><code>R = (customerID, street,      town,       city, zipCode){\n    (1,          Main street, First Town, BKK,  10100),\n    (2,          12 street,   A Town,     BKK,  10120)\n}\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#2nf-second-normal-form","title":"2NF: Second Normal Form","text":"<p>Second Normal Form (2NF) do more than the 1NF because 1NF only eliminates repeating groups, not redundancy.</p> <ul> <li>It should be in 1NF.</li> <li>It should not have partial dependencies. Each non-prime attribute is full   functionally dependent on the whole primary key (all prime attributes).</li> </ul> <p>Example</p> <pre><code>R = (studentID, studentName, subjectID, grade, prize){\n    (1,         Tom,         S01,       A,     Voucher),\n    (2,         Sara,        S01,       B+,    Nothing),\n    (3,         John,        S02,       A,     Voucher)\n}\n</code></pre> <pre><code>R = (studentID, subjectID, grade, prize){\n    (1,         S01,       A,     Voucher),\n    (2,         S01,       B+,    Nothing),\n    (3,         S02,       A,     Voucher)\n}\n\nRS = (studentID, studentName){\n     (1,         Tom),\n     (2,         Sara),\n     (3,         John)\n}\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#3nf-third-normal-form","title":"3NF: Third Normal Form","text":"<p>Third Normal Form (3NF)</p> <ul> <li> <p>It should be in 2NF.</p> </li> <li> <p>It should not have transitive dependencies.</p> </li> </ul> <p>Example</p> <pre><code>R = (studentID, subjectID, grade, prize){\n    (1,         S01,       A,     Voucher),\n    (2,         S01,       B+,    Nothing),\n    (3,         S02,       A,     Voucher)\n}\n</code></pre> <pre><code>R = (studentID, subjectID, grade){\n    (1,         S01,       A),\n    (2,         S01,       B+),\n    (3,         S02,       A)\n}\n\nRP = (grade, prize){\n     (A,     Voucher),\n     (B+,    Nothing),\n}\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#bcnf-boyce-codd-normal-form","title":"BCNF: Boyce-Codd Normal Form","text":"<p>Boyce-Codd Normal Form (BCNF) will remove all candidate key.</p> <ul> <li>It should be in 3NF.</li> <li>Any attribute in table depends only on super-key.   A -&gt; Z means (A) is super-key of (Z) (even (Z) is a prime attribute)</li> </ul> <p>Note</p> <ul> <li>If our table contains only one prime key, 3NF and BCNF are equivalent.</li> <li>BCNF is a special case of 3NF, and it also called 3.5NF.</li> </ul> <p>Example</p> <pre><code>R = (student, subject, professor){\n    (A,       Math,    P.Tom),\n    (A,       Science, P.Sara),\n    (B,       Math,    P.Kan),\n}\n</code></pre> <ul> <li><code>{student, subject} -&gt; professor</code>, so <code>student</code> and <code>subject</code> are super-key.</li> <li><code>professor -&gt; subject</code>, but subject is prime attribute and professor is not   super-key.</li> </ul> <pre><code>RS = (student, professorID){\n     (A,       1),\n     (A,       2),\n     (B,       3),\n}\n\nRP = (professorID, professor, subject){\n     (1,           P.Tom,     Math),\n     (2,           P.Sara,    Science),\n     (3,           P.Kan,     Math),\n}\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#4nf-fourth-normal-form","title":"4NF: Fourth Normal Form","text":"<p>Fourth Normal Form (4NF)</p> <ul> <li> <p>It should be in BCNF.</p> </li> <li> <p>It should not have multivalued dependencies.</p> </li> </ul> <p>If no database table instance contains two or more, independent and multivalued data describing the relevant entity, then it is in 4th Normal Form.</p> <p>Example</p> <pre><code>R = (projectID, businessUnit, Location){\n    (A01,       BU01,         North),\n    (A01,       BU01,         South),\n    (A01,       BU02,         North),\n    (A01,       BU02,         South),\n}\n</code></pre> <pre><code>R1 = (projectID, businessUnit){\n     (A01,       BU01),\n     (A01,       BU02)\n}\n\nR2 = (projectID, Location){\n     (A01,       North),\n     (A01,       South)\n}\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#5nf-fifth-normal-form","title":"5NF: Fifth Normal Form","text":"<p>Fifth Normal Form (5NF) is also known as Project-Join Normal Form (PJNF). It is used to handle complex many-to-many relationships in a database.</p> <ul> <li>It should not have join dependency, it cannot be decomposed into any number of   smaller tables without loss of data.</li> </ul> <p>In a many-to-many relationship, where each table has a composite primary key, it is possible for a non-trivial functional dependency to exist between the primary key and a non-key attribute. 5NF deals with these situations by decomposing the tables into smaller tables that preserve the relationships between the attributes.</p> <p>Example</p> <pre><code>R = (year,   subjectID, buildingID){\n    (1/2560, EE400,     A012),\n    (1/2560, EE402,     B012),\n    (2/2560, EE400,     B012),\n    (1/2560, EE400,     B012)\n}\n</code></pre> <pre><code>R1 = (year,   subjectID){\n     (1/2560, EE400),\n     (1/2560, EE402),\n     (2/2560, EE400)\n}\n\nR2 = (subjectID, buildingID){\n     (EE400,     A012),\n     (EE400,     B012),\n     (EE402,     B012),\n}\n\nR3 = (year,   buildingID){\n     (1/2560, A012),\n     (1/2560, B012),\n     (2/2560, B012)\n}\n</code></pre> <pre><code>R != R1 join R2 join R3\n</code></pre>"},{"location":"abstract/data_management/data-normalization-form/#6nf-sixth-normal-form","title":"6NF: Sixth Normal Form","text":"<p>Sixth Normal Form (6NF) (Proposed)</p> <ul> <li>The row contains the primary key, and at most one other attribute</li> </ul> <p>Warning</p> <p>The obvious drawback of 6NF is the proliferation of tables required to represent the information on a single entity. If a table in 5NF has one primary key column and N attributes, representing the same information in 6NF will require N tables; multi-field updates to a single conceptual record will require updates to multiple tables; and inserts and deletes will similarly require operations across multiple tables.</p>"},{"location":"abstract/data_management/data-normalization-form/#read-mores","title":"Read Mores","text":"<ul> <li> Third normal form</li> <li> Database normalization</li> <li>database normalization</li> <li>table normalization</li> <li>https://datavalley.technology/normalization-in-depth/</li> <li>https://medium.com/swlh/a-complete-database-normalization-tutorial-732df3748d0e</li> </ul>"},{"location":"abstract/data_management/data-ods/","title":"Operational Data Store","text":"<p> Operational Data Store (ODS) are nothing but data store required when neither Data warehouse nor OLTP systems support organizations reporting needs. In ODS, Data warehouse is refreshed in real-time. Hence, it is widely preferred for routine activities like storing records of the Employees.</p> <p>Note</p> <p>Unlike traditional data warehouses typically used for long-term storage and historical data analysis, an ODS focuses on providing a current, integrated, and consistent view of operational data from multiple sources.</p> <p>It acts as an intermediary layer between the operational systems (such as transactional databases, CRM systems, or ERP systems) and the data warehouse or data mart.</p> <p>This is a type of data warehouse that stores operational data from various sources and provides near real-time reporting and analysis. It is designed to handle frequent updates and queries from operational systems. It also serves as a source of data for the EDW or data marts.</p> <p> An ODS is a type of data warehouse that stores real-time or near-real-time data from transactional systems. It is designed to support operational reporting and analysis, and it typically uses a bottom-up approach to design, which means that the data model is based on specific business requirements.</p>"},{"location":"abstract/data_management/data-ods/#getting-started","title":"Getting Started","text":"<p>An operational data store (ODS) is a central database that aggregates data from multiple systems, providing a single destination for housing a variety of data. With information constantly updated, an ODS enables a current snapshot of relevant business metrics, allowing decision-makers to take advantage of time-sensitive opportunities and make data-informed decisions while business operations are occurring.</p> <p>Operational data stores integrate data from their source systems in their original format. Once loaded, data in the ODS can be scrubbed, resolved for redundancy, and checked for compliance with relevant business rules. An ODS is especially useful for light-duty analytical processing such as operational reporting. And since data contained in the ODS is always the most recent data available, this system is ideal for real-time data analysis on business processes as they are happening.</p> <p>Unlike extract, transform, load (ETL) systems, an operational data store ingests raw data from production systems in its original format, storing it as is. There\u2019s no need to transform the data before it can be analyzed or used for making operational decisions about the business.</p>"},{"location":"abstract/data_management/data-ods/#read-mores","title":"Read Mores","text":"<ul> <li>A Modern Approach To The Operational Data Store</li> </ul>"},{"location":"abstract/data_management/data-quality/","title":"Data Quality","text":"<p>Data Quality measures how well a dataset complies with the criteria of accuracy, completeness, validity, consistency, uniqueness, timeliness, and fitness for purpose, and is fundamental for all data governance initiatives within an organization. Data quality standards ensure that companies make decisions based on data to achieve their business objectives.</p> <p>https://medium.com/@romina.elena.mendez/data-quality-f720fc56912a</p> <ul> <li>Layers of Data Quality</li> <li>Why Data Quality Is Harder than Code Quality</li> </ul>"},{"location":"abstract/data_management/data-quality/#getting-started","title":"Getting Started","text":""},{"location":"abstract/data_management/data-quality/#read-mores","title":"Read Mores","text":"<ul> <li>https://snowplow.io/blog/the-road-to-better-data-quality-your-questions-answered/</li> <li>https://www.talend.com/resources/data-quality-best-practices/</li> </ul>"},{"location":"abstract/data_management/data-scd/","title":"Slowly Changing Dimension","text":""},{"location":"abstract/data_management/data-scd/#getting-started","title":"Getting Started","text":"<p>A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse.</p>"},{"location":"abstract/data_management/data-scd/#types","title":"Types","text":""},{"location":"abstract/data_management/data-scd/#type-1-overwriting","title":"Type 1 (Overwriting)","text":"<p>If a record in a dimension table changes, the existing record is updated or overwritten. Otherwise, the new record is inserted into the dimension table. This means records in the dimension table always reflect the current state and no historical data is maintained.</p>"},{"location":"abstract/data_management/data-scd/#type-2-history-row-based","title":"Type 2 (History Row-Based)","text":"<p>While having a table that reflects only the current state may be useful, there are times when it\u2019s convenient, and even essential, to track historical changes to a dimension. With SCD type 2, historical data is maintained by adding a new row when a dimension changes and properly denoting this new row as current while denoting the newly historical record accordingly.</p> <p>However, a couple of metadata fields are required in order to track changes:</p> Column Description Alias <code>is_current</code> Whether the record has the currently valid information for the specific product code. CurrentFlag <code>valid_from</code> The timestamp from which the record was/is current. EffectiveData, StartDate <code>valid_to</code> The timestamp until which the record was/is current. EndDate <code>is_deleted</code> Whether the product code no longer exists in the source data. DeleteFlag"},{"location":"abstract/data_management/data-scd/#type-3-history-column-based","title":"Type 3 (History Column-Based)","text":"Column Description Alias <code>current_field</code> The current value of this field. <code>previous_field</code> The previous value of this field. OriginalField"},{"location":"abstract/data_management/data-scd/#type-4-history-table","title":"Type 4 (History Table)","text":"<p>This method resembles how database audit tables and change data capture techniques function.</p>"},{"location":"abstract/data_management/data-scd/#type-5-4-1","title":"Type 5 (4 + 1)","text":"<p>The type 5 technique builds on the type 4 mini-dimension by embedding a \"current profile\" mini-dimension key in the base dimension that's overwritten as a type 1 attribute.</p>"},{"location":"abstract/data_management/data-scd/#type-6-1-2-3","title":"Type 6 (1 + 2 + 3)","text":""},{"location":"abstract/data_management/data-scd/#type-7-hybrid","title":"Type 7 (Hybrid)","text":"<p>An alternative implementation is to place both the surrogate key and the natural key into the fact table.</p>"},{"location":"abstract/data_management/data-scd/#read-mores","title":"Read Mores","text":"<ul> <li> Unlocking the Secrets of Slowly Changing Dimension (SCD): A Comprehensive View of 8 Types</li> <li> Navigating Slowly Changing Dimensions (SCD) and Data Restatement: A Comprehensive Guide</li> <li> Wikipedia: Slowly changing dimension</li> </ul>"},{"location":"abstract/data_management/data-transformation/","title":"Data Transformation","text":"<p>Data transformation is the process of taking raw data and making meaning out of it; it forms the foundation of all analytics work and represents how data practitioners create tangible value from their companies.</p> <p>ETL: Data is extracted from different sources, transformed to meet analysis needs, and then loaded into a data warehouse. This can be time-consuming and less adaptable to changes.</p> <p>ELT: Data is extracted and directly loaded into the data warehouse in its raw form. Transformation occurs afterward, leveraging the power of the modern cloud-based data warehouse\u2019s processing capabilities. This approach is more agile and scalable, accommodating the growing volume and variety of data.</p>"},{"location":"abstract/data_management/data-transformation/#read-mores","title":"Read Mores","text":"<ul> <li>https://medium.com/towards-data-engineering/why-etl-becomes-elt-or-even-let-1ea7b21e2f28</li> <li>Medium: Part2 - Tool Selection Strategy</li> <li>Unveiling Essential Framework Components</li> </ul>"},{"location":"abstract/data_management/data-warehouse/","title":"Data Warehouse","text":"<p>Data Warehouse (DW or DWH) is a centralized repository or digital storage system that integrates and stores data from various sources within an organization. It is designed to support Business Intelligence (BI) activities such as reporting, data analysis, and decision-making.</p> Data Warehouse Elements"},{"location":"abstract/data_management/data-warehouse/#getting-started","title":"Getting Started","text":"<p> A Data Warehouse, or Enterprise Data Warehouse (EDW) is a centralized warehouse that provides decision support service across the enterprise. It offers a unified approach for organizing and representing data. It also provides the ability to classify data according to the subject and give access according to those divisions.</p> <p>EDWs are usually a collection of databases that offer a unified approach for organizing data and classifying data according to subject. These data sources could be the Databases of various Enterprise Resource Planning (ERP) systems, Customer Relationship Management (CRM) systems, and other forms of Online Transactional Processing (OLTP) systems.</p> <p>Note</p> <p>This type of data warehouse is the most common and serves as a centralized repository for all of an organization's data. An EDW typically integrates data from various sources across the entire enterprise and is designed to support the reporting and analytics needs of multiple departments.</p>"},{"location":"abstract/data_management/data-warehouse/#understanding-olap-and-oltp","title":"Understanding OLAP and OLTP","text":"<p>OLAP (online analytical processing) is software for performing multidimensional analysis at high speeds on large volumes of data from unified, centralized data store, such as a data warehouse. OLTP (online transactional processing), enables the real-time execution of large numbers of database transactions by large numbers of people, typically over the internet. The main difference between OLAP and OLTP is in the name: OLAP is analytical in nature, and OLTP is transactional.</p> <p>OLAP tools are designed for multidimensional analysis of data in a data warehouse, which contains both historical and transactional data. Common uses of OLAP include data mining and other business intelligence apps, complex analytical calculations, and predictive scenarios, as well as business reporting functions like financial analysis, budgeting, and forecast planning.</p> <p>OLTP is designed to support transaction-oriented applications by processing recent transactions as quickly and accurately as possible. Common uses of OLTP include ATMs, e-commerce software, credit card payment data processing, online bookings, reservation systems, and record-keeping tools.</p>"},{"location":"abstract/data_management/data-warehouse/#architecture","title":"Architecture","text":"<p>In addition to the below three-tier architecture, some data warehouse architectures also include a metadata layer, which provides information about the data in the data warehouse, such as its origin, format, and meaning. The metadata layer can be used to help users understand and navigate the data.</p> <p></p>"},{"location":"abstract/data_management/data-warehouse/#bottom-tier","title":"Bottom tier","text":"<p>The Bottom Tier (Data Layer) consists of the Data Repository, usually a relational database system, which collects, cleanses, and transforms data from various data sources through a process known as;</p> <ul> <li>Extract Transform Load (ETL)</li> <li>Extract Load Transform (ELT)</li> </ul> <p>As a preliminary process, before the data is loaded into the repository, all the data relevant and required are identified from several sources of the system. These data are then cleaned up, to avoid repeating or junk data from its current storage units. The next step is to transform all these data into a single format of storage. The final step of ETL is to Load the data on the repository.</p> <p>ETL Layer, This layer is responsible for storing the data in the data warehouse. The data is typically stored in a relational database management system (RDBMS), which is optimized for querying and reporting on large datasets. In some cases, data may also be stored in columnar or in-memory databases for improved performance.</p>"},{"location":"abstract/data_management/data-warehouse/#middle-tier","title":"Middle tier","text":"<p>The Middle Tier (Semantics Layer) consists of an <code>OLAP</code> (Online Analytical Processing) servers which enables fast query speeds. The Data Warehouse can have more than one OLAP server, and it can have more than one type of OLAP server model as well, which depends on the volume of the data to be processed and the type of data held in the bottom tier.</p> <p>This layer is responsible for extracting, transforming, and loading the data from various sources into the data warehouse. This is typically done using ETL (Extract, Transform, Load) tools, which automate the process of moving and converting data.</p> <p>Three types of OLAP models can be used in this tier, which are known as</p> ROLAPMOLAPHOLAP <p>Relational online analytical processing is a model of online analytical processing which carries out an active multidimensional breakdown of data stored in a relational database, instead of redesigning a relational database into a multidimensional database.</p> <p>This is applied when the repository consists of only the relational database system in it.</p> <p>Multidimensional online analytical processing is another model of online analytical processing that catalogs and comprises directories directly on its multidimensional database system.</p> <p>This is applied when the repository consists of only the multidimensional database system in it.</p> <p>Hybrid online analytical processing is a hybrid of both relational and multidimensional online analytical processing models.</p> <p>When the repository contains both the relational database management system and the multidimensional database management system, HOLAP is the best solution for a smooth functional flow between the database systems. HOLAP allows storing data in both the relational and the multidimensional formats.</p> <p>Note</p> <p>The type of OLAP model used is dependent on the type of database system that exists.</p>"},{"location":"abstract/data_management/data-warehouse/#top-tier","title":"Top tier","text":"<p>The Top Tier (Analytics Layer) is represented by some kind of front-end user interface or reporting tool, which enables end users to conduct ad-hoc data analysis on their business data. It holds various tools like query tools, analysis tools, reporting tools, and data mining tools.</p> <p>Reporting Layer. This layer is responsible for presenting the data to end-users in a format that is easy to understand and analyze. This layer includes tools for querying, reporting, and visualization, which allow users to create custom reports and dashboards based on the data in the data warehouse.</p>"},{"location":"abstract/data_management/data-warehouse/#design","title":"Design","text":"<p>data warehouse design that can be applied within the framework of the design methods discussed earlier. Here is a brief overview of how each of these approaches relates to the design methods.</p> <p>Overall, the choice of design approach will depend on the specific needs and circumstances of the organization. A bottom-up approach may be more appropriate for organizations with complex and varied data sources, while a top-down approach may be more appropriate for organizations with well-defined business requirements. Hybrid design may be a good choice for organizations that need a flexible and adaptable data warehouse that can accommodate changing business requirements over time.</p>"},{"location":"abstract/data_management/data-warehouse/#bottom-up","title":"Bottom-up","text":"<p>Bottom-up design is an approach to data warehouse design that focuses on building small, specialized data marts first and then integrating them into a larger data warehouse. This approach is often used when there are different data sources with varying levels of complexity, and it allows for a more incremental and flexible approach to data warehouse development. Bottom-up design is often associated with dimensional modeling and may use hybrid modeling techniques to integrate the different data marts.</p> <p>Example</p> <p>Dimensional Model (Ralph Kimball):</p> <p>The Dimensional Model, also known as the Kimball model, is a bottom-up design approach that emphasizes the importance of simplicity and ease of use. This model is designed to support ad-hoc querying and analysis and is often used for data warehouse implementations in smaller organizations. The Kimball model involves creating a denormalized data model, which is optimized for querying and analysis, and building a star or snowflake schema that supports specific business functions.</p> <p>See more, Kimball Approach</p>"},{"location":"abstract/data_management/data-warehouse/#top-down","title":"Top-down","text":"<p>Top-down design is an approach to data warehouse design that starts with a comprehensive enterprise data model and then designs the data warehouse based on that model. This approach is often used when there is a well-defined set of business requirements and a clear understanding of the data sources and their relationships. Top-down design is often associated with data vault modeling and may use hybrid modeling techniques to accommodate the specific business requirements.</p> <p>Example</p> <p>Third Normal Form Model (Bill Inmon):</p> <p>The 3NF Model, also known as the Inmon model, is a top-down design approach that emphasizes the importance of a comprehensive enterprise data model. This model is designed to support complex business processes and is often used for data warehouse implementations in large organizations. The Inmon model involves creating a normalized data model, which is then used to build data marts that support specific business functions.</p> <p>See more, Inmon Approach</p>"},{"location":"abstract/data_management/data-warehouse/#hybrid","title":"Hybrid","text":"<p>Hybrid design is an approach to data warehouse design that combines elements of both bottom-up and top-down design. This approach recognizes that there may be benefits to both approaches and seeks to find a balance between the two. Hybrid design may use different modeling techniques for different parts of the data warehouse and may involve a mix of top-down and bottom-up development. Hybrid design is often associated with agile modeling and may use a variety of design methods to create a flexible and adaptable data warehouse.</p> <p>Example</p> <p>Data Vault 2.0 Model:</p> <p>The DV 2.0 Model is a hybrid design approach that combines elements of both the Inmon and Kimball models. This model is designed to support flexibility, scalability, and agility, and is often used for data warehouse implementations in organizations that need to handle large amounts of complex and varied data. The Data Vault 2.0 model involves creating a normalized data model that separates business entities and relationships into three types of tables (Hub, Link, and Satellite), which can then be used to build data marts that support specific business functions.</p> <p>See more, Data Vault Model</p> <p>Read more about Data Modeling</p>"},{"location":"abstract/data_management/data-warehouse/#conclusion","title":"Conclusion","text":"<p>Overall, the choice of design method will depend on the specific needs and circumstances of the organization. The Inmon model may be more appropriate for organizations with complex and varied data sources and a focus on enterprise-wide integration. The Kimball model may be more appropriate for organizations with a focus on ad-hoc querying and analysis and a need for simplicity and ease of use. The Data Vault 2.0 model may be a good choice for organizations that need a flexible and scalable data warehouse that can accommodate changing business requirements over time.</p> Design Method 3NF Model (Inmon) Dimensional Model (Kimball) Data Vault 2.0 Model Description Top-down design approach that emphasizes a comprehensive enterprise data model Bottom-up design approach that emphasizes simplicity and ease of use Hybrid design approach that combines elements of the Inmon and Kimball models Strengths Supports complex business processes, supports enterprise-wide integration, allows for data reuse Supports ad-hoc querying and analysis, easy to understand and use, can be quickly implemented Supports flexibility, scalability, and agility, accommodates changing business requirements, allows for data reuse Weaknesses Can be time-consuming to design and build, may not be well-suited for ad-hoc querying and analysis May not be well-suited for complex business processes or enterprise-wide integration, may not support as much data reuse as the Inmon model Can be more complex to design and build than the Inmon or Kimball models, may not be as well-suited for smaller organizations or simple business processes Focus Enterprise-wide integration Ad-hoc querying and analysis Ad-hoc querying and analysis Advantages Comprehensive data model supports complex business processes; Data is normalized, reducing data redundancy and ensuring data consistency Easy to understand and use for ad-hoc querying and analysis; Denormalized data model optimized for querying and analysis Separates business entities and relationships into three types of tables, providing flexibility and scalability; Supports complex and varied data sources <p>As a Data Engineer, the choice of design method will depend on the specific needs and circumstances of your organization. If you work in a large organization with complex and varied data sources and a focus on enterprise-wide integration, the 3NF Model (Inmon) may be a good choice. If your organization has a focus on ad-hoc querying and analysis and a need for simplicity and ease of use, the Dimensional Model (Kimball) may be a better fit. If your organization needs a flexible and scalable data warehouse that can accommodate changing business requirements over time, the Data Vault 2.0 model may be the best option.</p>"},{"location":"abstract/data_management/data-warehouse/#read-mores","title":"Read Mores","text":"<ul> <li>Guru99: Data Warehousing</li> <li>Guru99: Data Warehouse Architecture</li> <li>IBM: Data Warehouse</li> <li>A Complete Guide to Data Warehouse in 2022</li> <li>CodingNinjas: Inmon vs Kimball Approaches in DWH</li> <li>Nearshore: Data Warehouse Architecture</li> </ul>"},{"location":"abstract/data_management/data_modeling/","title":"Data Modeling","text":"<p>Data Modeling is a process of creating a conceptual representation of the data and its relationships within an organization or system. Dimensional modeling is an advanced technique that attempts to present data in a way that is intuitive and understandable for any user. It also allows for high-performance access, flexibility, and scalability to accommodate changes in business needs.</p> <p>Quote</p> <p>Data Modeling is a family of techniques used to describe the kinds of information that are important to an enterprise. Formal data modeling originally emerged to meet the demands of database designers.<sup>1</sup></p> <ul> <li>Guide With Problems</li> <li>https://medium.com/@dnyanesh.bandbe88/the-3-brothers-of-data-modelling-kimball-inmon-data-vault-5788863e98c8</li> <li>https://medium.com/@khedekarpratik123/data-vault-2-0-modern-data-modelling-methodology-part-1-ed91ed408d48</li> </ul>"},{"location":"abstract/data_management/data_modeling/#types","title":"Types","text":""},{"location":"abstract/data_management/data_modeling/#kimbal","title":"Kimbal","text":""},{"location":"abstract/data_management/data_modeling/#inmon","title":"Inmon","text":""},{"location":"abstract/data_management/data_modeling/#data-vault","title":"Data Vault","text":"<ul> <li>https://patrickcuba.medium.com/data-vault-is-not-a-monolith-3ea2014ffedc</li> </ul>"},{"location":"abstract/data_management/data_modeling/#one-big-tabel","title":"One Big-Tabel","text":"<ul> <li>What is One Big Table?</li> </ul>"},{"location":"abstract/data_management/data_modeling/#read-mores","title":"Read Mores","text":"<ul> <li>Medium: Data Modeling Techniques for Data Warehouse</li> <li>https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302</li> </ul> <ol> <li> <p>Gartner: Information Tech Glossary - Data Modeling \u21a9</p> </li> </ol>"},{"location":"abstract/data_management/data_modeling/dwh-anchor-approach/","title":"Anchor Approach","text":"<p>The Anchor Model further normalizes the data vault model. The initial intention of Lars. R\u00f6nnb\u00e4ck was to design a highly scalable model. His core concept is that all expansion involves adding rather than modifying. Therefore, he normalized the model to 6NF, and it becomes a K-V structural model.</p> <p>The Anchor Model consists of the following:</p> <ul> <li>Anchors: Anchors are similar to Hubs in the Data Vault Model. They stand for business entities and have only primary keys.</li> <li>Attributes: Attributes are similar to satellites in the Data Vault Model but are more normalized. They are in the K-V structure. Each table describes attributes of only one anchor.</li> <li>Ties: Ties indicate the relationship between Anchors and get described using a table. Ties are similar to links in the Data Vault Model and can improve the general model expansion capability.</li> <li>Knots: Knots stand for the attributes that may be shared by multiple anchors, for example, enumerated and public attributes such as gender and state.</li> </ul> <p>We can further subdivide these four basic objects into historical and non-historical objects, where historical objects record changes in the data using timestamps and keeping multiple records.</p> <p>This division allowed the author of the Anchor Model to achieve high scalability. However, this model also increases the number of join query operations. The creator believes that analysis and query in a data warehouse are performed only based on a small section of fields. This is similar to the array storage structure, which can significantly reduce data scanning and reduce the impact on query performance. Some databases with the table elimination feature, for example, MariaDB, can greatly reduce the number of join operations. However, this is still open to discussion.</p>"},{"location":"abstract/data_management/data_modeling/dwh-anchor-approach/#references","title":"References","text":"<ul> <li>Alibaba Cloud: Comparison of Data Modeling Methods for BigData</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/","title":"Data Vault Approach","text":"<p>In contrary to Inmon\u2019s view, Linstedt assumes that all available data from the entire time period should be loaded into the warehouse. This is known as the \"single version of the facts\" approach. As with Kimball\u2019s star schema, with the Data Vault Linstedt introduces some additional objects to organize the data warehouse structure. These objects are referred to as the hub, satellite and link.</p> <p>The Data Vault methodology is a hybrid approach that combines aspects of both the Kimball and Inmon methodologies.</p> <p>It uses a hub-and-spoke architecture to model the data and focuses on creating separate entities for business processes, data sources, and data types.</p> <p>The Data Vault methodology is known for its scalability, flexibility, and ability to handle complex data relationships.</p> <p>Quote</p> <p>Single version of the facts</p> <p>Data Vault modeling is a hybrid approach based on third normal form and dimensional modeling aimed at the logical Enterprise Data Warehouse (EDW). The data vault model is built as a ground-up, incremental, and modular models that can be applied to big data, structured, and unstructured data sets.</p> <p>Data Vault 2.0 (DV2) modeling is focused on providing flexible, scalable patterns that work together to integrate raw data by business key for the enterprise data warehouse. DV2 modeling includes minor changes to ensure the modeling paradigms can work within the constructs of big data, unstructured data, multi-structured data, and NoSQL.</p> <p>Data Vault focuses on Agile Data Warehouse Development where scalability, data integration/ETL and development speed are important. Most customers have a landing zone, Vault zone and a data mart zone which correspond to the Databricks organizational paradigms of Bronze, Silver and Gold layers. The Data Vault modeling style of hub, link and satellite tables typically fits well in the Silver layer of the Databricks Lakehouse.</p> <p>A data vault is a data modeling design pattern used to build a data warehouse for enterprise-scale analytics. The data vault has three types of entities: hubs, links, and satellites.</p> <p>Data Vault 1.0 Loading Pattern:</p> <pre><code>---\nData Vault 1.0 Loading Pattern\n---\n\nflowchart LR\n    A[Source] --&gt;|Loading| B(Staging)\n    B -- Hub load --&gt; C[Hub]\n    C --&gt; D[Satellite]\n    C --&gt; E[Link]\n    D -.-&gt; |ID Lookup| C\n    E -.-&gt; |ID Lookup| C</code></pre> <p>Data Vault 2.0 Loading Pattern:</p> <pre><code>flowchart LR\n    A[Source] --&gt;|Loading| B(Staging)\n    B -- Hub load --&gt; C[Hub]\n    B -- Link load --&gt; E[Link]\n    B -- Satellite load --&gt; D[Satellite]</code></pre> <p>Data Vault Modeling 2.0 changes the sequence numbers to hash keys. The hash keys provide stability, parallel loading methods, and decoupled computation of parent key values for records.</p> <p>Regarding the hashes, they have a great benefit for the Big Data workloads. The loading pattern for Data Vault 1.0 required to load the hubs first, and later, when the links and satellites were loaded, make the lookups on the hubs table to retrieve the surrogate keys. It changes in the Data Vault 2 because the hash key is deterministic, i.e. it can be resolved from already existing data, making the load parallel for all components</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#hash","title":"Hash","text":"<p>There are many hash functions to choose from: <code>MD5</code>, <code>MD6</code>, <code>SHA-1</code>, <code>SHA-2</code>, and some more. We recommend to use the MD5 Algorithm with a length of 128 bit in most cases, because it is most ubiquitously available across most platforms, and has a decently low percentage chance of duplicate collision with an acceptable storage requirement.</p> <p>Hash Keys are not optional in the Data Vault, they are a must. The advantages of</p> <ul> <li>Massively Parallel Processing (MPP) Architecture</li> <li>data load performance</li> <li>consistency</li> <li>auditability</li> </ul> <p>Note: \\ Hash values are the fundamental component of Data Vault 2.0 modeling. They are generated using system functions as data is loaded into the data vault. Hashes reduce dependencies, allow for quicker joins between tables (HashKey), and allow for fast comparisons to detect changes in data (HashDiff).</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#basic-structures-of-data-vault","title":"Basic Structures of Data Vault","text":""},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#hubs","title":"Hubs","text":"<p>Hub (Immutable Business Key): Contains a unique list of business objects that represents a domain or concept within the enterprise, everything else connected to the hub gives us more context.</p> <p>Note: \\ Business Keys that are supplied by users to identify, track, and locate information, such as a customer number, invoice number, or product number. Should be,</p> <ul> <li>Unique</li> <li>At the same level of granularity</li> </ul> Column Alias Description Constraints Inclusion HashKey <code>HUB_{name}_HK</code> HashKey generated from Business Key PK Required BusinessKey <code>{BS-name}_BK</code> Business defined business key UQ Required LoadDatetime <code>LDTS</code> Load Datetime from Stage to Data Vault Required RecordSource <code>RSCR</code> Specifics the source system from which the key originated Required LastSeenDate Date a record was last included on a data load Optional <p>Loading Pattern:</p> <pre><code>flowchart LR\n    A[(Source)] --&gt; B[\"Select Distinct&lt;br&gt;List of Business&lt;br&gt;Keys\"]\n    B --&gt; C{Key Exists in&lt;br&gt;Target Hub?}\n    C -- No --&gt; D[Generate Hash Key]\n    D --&gt; F[Insert Row&lt;br&gt;Into Target&lt;br&gt;Hub]\n    F --&gt; G[(\"Data Vault&lt;br&gt;Hub\")]\n    C -- Yes --&gt; E[Drop Row&lt;br&gt;From Data Flow]</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#links","title":"Links","text":"<p>Link (Unit of Work): Represents the relationship between two or more business objects, representing a part of one or many business processes or even value streams.</p> <p>Tables that show the relationships between Hubs. Their level of granularity is determined by the number of hubs they connect, and they are non-temporal. When thinking of a traditional star schema, links are often associated with fact tables.</p> <p>Keynotes on Links:</p> <ul> <li>Do not show effectivity</li> <li>Only support inserts</li> </ul> <p>Different types of links can be used to make models more flexible:</p> <ul> <li> <p>Same as Link: \\   helps to solve the data quality issues problem where a given concept can be misspelled   and has to be linked to the correct definition. An example of this fuzzy matching   modeling can be zip codes written in different formats (e.g., 00000 or 00-000)</p> </li> <li> <p>Hierarchical Link: \\   Represents hierarchies inside the same hub. An example of that hierarchy can be   a hierarchy of product categories.</p> </li> <li> <p>Standard Link: \\   Be a standard link created from business rules, so it can be easily discarded   without affecting the auditability. The official specification calls it also a   Deep Learning link because it can give confidence for the prediction and strength   for the correlation of 2 datasets, but if we stay with these 2 columns, we could   also try to use it to model a result of the ML recommendation system between a   user and an item. The link would then specify how possible it is that the user   will like the product. Here, the link is mutable since the prediction can change   or even be discarded if the algorithm detects other user preferences changes.</p> </li> <li> <p>Non-Historized Link (aka transactional): \\   That is perfectly adapted to the immutable facts, so something that happened in   the past can neither be undone nor modified. And since the changes will never happen,   they don't need to be stored in the satellites.</p> </li> </ul> Column Alias DESC Constraints Inclusion HashKey sha HashKey generated from Business Keys of Linked Hubs PK, UQ Required BusinessKey _key Concatenation of Business Keys from linked Hubs UQ Optional LoadDatetime <code>LDTS</code> Load Date &amp; Time from Stage to DV Required RecordSource <code>RSCR</code> Specifics the source system from which the key(s) originated Required HubHashKey1 HushKey from Hub Relationship 1 FK Required HubHashKey2 HushKey from Hub Relationship 2 FK Required ... HushKey from Hub Relationship ... FK Required <p>Loading Pattern:</p> <pre><code>flowchart LR\n\n    A[(Source)] --&gt; B1[\"Select Distinct&lt;br&gt;List of Business&lt;br&gt;Keys\"]\n    B1 --&gt; B[Generate Each&lt;br&gt;Contributing&lt;br&gt;Hub's Hash Key]\n    B --&gt; C{Key Exists in&lt;br&gt;Target Link?}\n    C --&gt; |No| D[\"Generate Hash Key&lt;br&gt;(if required)\"]\n    D --&gt; F[Insert Row&lt;br&gt;Into Target&lt;br&gt;Link]\n    F --&gt; G[(\"Data Vault&lt;br&gt;Link\")]\n    C --&gt; |Yes| E[Drop Row&lt;br&gt;From Data Flow]\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#satellites","title":"Satellites","text":"<p>Satellite (Descriptive Context): Descriptive change-tracking content eiter describing the business object (hab-satellite) or the unit of work (link-satellite).</p> <p>Tables that provide context to the business objects and relationships described in Hubs and Links. Each satellite is connected to only one Hub or Link, but a Hub or Link can have multiple satellites.</p> <p>Keynotes on Satellites:</p> <ul> <li>One per source system</li> <li>Stores all context</li> <li>Stores all history</li> <li>Delta driven, similar to SCD</li> <li>EndDate is only attribute that is updated</li> <li>Most flexible construct</li> </ul> <p>Different types of satellite can be used to make models more flexible:</p> <ul> <li> <p>Multi-active Satellite: \\   In this satellite given hub or link entity has multiple active values. For example,   it can be the case of a phone number that can be professional pr personnel, and both   may be active at a given moment.</p> </li> <li> <p>https://www.scalefree.com/scalefree-newsletter/using-multi-active-satellites-the-correct-way-1-2/</p> </li> <li> <p>https://www.scalefree.com/scalefree-newsletter/using-multi-active-satellites-the-correct-way-2-2/</p> </li> <li> <p>Effectivity Satellite: \\   A descriptive record that is valid (effective) only for some specific period of time.   You will there the start and end dates and an example of it can be an external   contractor working in a company in different periods.</p> </li> <li> <p>System-Driven Satellite: \\   Mutable, created from hard business rules (data-type rules, like enforcing integer type;   not involving pure business rules like classifying a revenue as poor or rich category of people)   for performance purposes. The examples of these satellites are the point-in-time table (PIT)   and bridge table, presented in the next paragraph.</p> </li> </ul> Column Alias DESC Constraints Inclusion HashKey sha{type} HashKey from parent Hub or Link PK, FK Required LoadDatetime LDTS Batch Load Date &amp; Time from Stage to DV PK Required EndDatetime EDTS Load Date &amp; Time the record became inactive Required RecordSource RSCR Specifics the source system from which the key(s) originated Required HashDiff HASH_DIFF Hushed value of all attributes data Optional ExtractDate Date data was extracted from source system Optional Status Insert (I)/Update (U)/Delete (D) Attributes1 Attributes column 1. Number and type will vary Optional Attributes2 Attributes column 2. Number and type will vary Optional ... Attributes column ... Number and type will vary Optional <p>Loading Pattern:</p> <pre><code>flowchart LR\n\n    A[(Source)] --&gt; B1[\"Select Distinct&lt;br&gt;List of Satellite&lt;br&gt;Records\"]\n    B1 --&gt; B2[\"Generate Single&lt;br&gt;Hub's or Hubs'&lt;br&gt;and Link's Hash&lt;br&gt;Key(s)\"]\n    B2 --&gt; B[\"Find Latest&lt;br&gt;Satellite&lt;br&gt;Record\"]\n    B --&gt; C{Find Latest&lt;br&gt;Record?}\n    C -- No --&gt; D[\"Insert Row&lt;br&gt;Into Target&lt;br&gt;Satellite\"]\n    D --&gt; F[(Data Vault Satellite)]\n    C -- Yes --&gt; E[Compare All&lt;br&gt;Fields/Columns]\n    E --&gt; G{All&lt;br&gt;Fields/Columns&lt;br&gt;Match?}\n    G -- No --&gt; I[\"Insert Row Into Target&lt;br&gt;Satellite &amp; Perform End-Date&lt;br&gt;Processing\"]\n    I --&gt; F\n    G -- Yes --&gt; H[\"Drop Row&lt;br&gt;From Data Flow\"]\n</code></pre> <p>The hash difference column applies to the satellites. The approach is the same as with the business keys, only that here all the descriptive data is hashed. That reduces the effort during an upload process because just one column has to be looked up. The satellite upload process first examines if the hash key is already in the satellite, and secondly if there are differences between the hash difference values.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#point-in-time","title":"Point-In-Time","text":"<p>PITs (Simplify with Equijoin): disposable table designed to simplify querying of business object or relationship state attributes for information marts.</p> <p>Note: \\ (Business Vault)</p> <p>Represents multiple satellite tables in one. But the idea is not to copy any of the context values but only their load dates. For example, if a hub has 3 different satellites, satA, satB and satC, a point-in-time table will store the most recent load date for every satellite and so for every business key. So the stored data will be built like (business key [BK] from the hub, MAX(loadDate from satA for BK), MAX(loadDate from satB for the BK), MAX(loadDate from satC for BK)</p> <p>The PIT serves two purposes:</p> <ul> <li> <p>Simplify the combination of multiple deltas at different \u201cpoint in time\u201d \\   A PIT table creates snapshots of data for dates specified by the data consumers   upstream. For example, it is often usual to report the current state of data each day.   To accommodate these requirements, the PIT table includes the date and time of   the snapshot, in combination with the business key, as a unique key of the entity   (a hashed key including these two attributes, named as CustomerKey).   For each of these combinations, the PIT table contains the load dates and the   corresponding hash keys from each satellite that corresponds best with the   snapshot date.</p> </li> <li> <p>Reduce the complexity of joins for performance reasons \\   The PIT table is like an index used by the query and provides information about   the active satellite entries per snapshot date. The goal is to materialize as   much of the join logic as possible and end up with an inner join with equi-join   conditions only. This join type is the most performant version of joining on   most (if not all) relational database servers. In order to maximize the   performance of the PIT table while maintaining low storage requirements,   only one ghost record is required in each satellite used by the PIT table.   This ghost record is used when no record is active in the referenced satellite   and serves as the unknown or NULL case. By using the ghost record, it is   possible to avoid NULL checks in general, because the join condition will   always point to an active record in the satellite table: either an actual   record which is active at the given snapshot date or the ghost record.</p> </li> </ul> <p>For example,</p> <p>We identify range of <code>PITForm</code> and <code>PITTo</code> and map the data in satellites to this range (greater or equal than).</p> <p>Note: \\ PIT tables are incrementally updated when new data becomes available in the Data Vault tables that support them.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#bridge","title":"Bridge","text":"<p>Bridges (Shorten the Distance): Disposable table designed to shorten the distance between business objects for information marts.</p> <p>Note: \\ (Business Vault)</p> <p>Similar to the PIT tables, bridge tables are also the tables designed with the performance in mind. But unlike a PIT table, a bridge table will only store the business keys from the hubs and links. If you are familiar with the star schema, you will see some similarities between bridge and fact tables. The difference is that the bridge stores only the keys, whereas the fact will also store the measures. A single accepted value in the bridge tables are aggregations computed at runtime. Together with PIT table, the bride is considered as a query-assist structure.</p> <p>There are some important notes to clarify on the structure of the Bridge table:</p> <ul> <li>Each Bridge table contains a <code>zero record</code>, which provides an 'unknown' record   that replaces to any NULL value.</li> <li>Each Bridge record receives a surrogate key that uniquely identifies a row in   the Bridge table. This is purely for identification purposes.</li> <li>The load date / time stamp used is derived from the involved Hub and Link tables.   When loading the Bridge table, it will reuse the lowest (earliest) value from   the tables that are in scope. This value contains the earliest moment in time   a relationship between Core Business Concepts could be established.</li> </ul> <p>Note: \\ Bridge tables are incrementally updated when new data becomes available in the Data Vault tables that support them.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#reference","title":"Reference","text":"<p>Note: \\ (Raw Vault)</p> <p>They are present to avoid data storage redundancy for the values used a lot. For example, a satellite can store a product category code instead of de-normalizing the whole relationship, so bring the category description, full name to the satellite itself, and use the reference table to retrieve this information when needed. The reference tables are built from the satellite, hub or links values entities but are often used only by the satellites since they're the most \"descriptive\" part of the data vault modeling.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#staging","title":"Staging","text":"<p>Exactly like a staging area, staging tables are temporary structures used to improve the load experience. They can also provide the Data Vault features if the raw data's loading process to the data warehouse storage doesn't support them. More exactly, you will find there a staging table and a second-level staging table that will load the raw data from the staging table and add all required attributes like a hash key, load times or hash difference key)</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#types-of-data-vault","title":"Types of Data Vault","text":""},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#raw-vault","title":"Raw Vault","text":"<p>Raw Vault provides the modelled outcome of business processes from source systems, business vault extends those business processes to how the business sees them. Business rules supplied to Raw Vault are idempotent, business vault\u2019s rules must be the same.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#business-vault","title":"Business Vault","text":"<p>Data that has been modified by the business rules and is modeled in DV style tables; sparsely created.</p> <p>So, the raw vault is the raw, unfiltered data from the source systems that has been loaded into hubs, links and satellites based on business keys. The business vault is an extension of a raw vault that applies selected business rules, de-normalizations, calculations and other query-assistance functions. It does this in order to facilitate user access and reporting.</p> <p>A Business Vault is modeled in DV style tables, such as hubs, links and satellites, but it is not a complete copy of all objects in the Raw Vault. Within it, there will only be structures that hold some significant business value.</p> <p>The data will be transformed in a way to apply rules or functions that most of the business users find useful as opposed to doing these repeatedly into multiple marts. This includes things like data cleansing, data quality, accounting rules or repeatable calculations.</p> <p>In Data Vault 2.0, the Business Vault also includes some special entities that help build more efficient queries against the Raw Vault. These are Point-in-Time and Bridge Tables.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#metric-vault","title":"Metric Vault","text":"<p>The Metrics Vault is an optional tier used to hold operational metrics data for the Data Vault ingestion processes. This information can be invaluable when diagnosing potential problems with ingestion. It can also act as an audit trail for all the processes that are interacting with the Data Vault.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#error-marts","title":"Error Marts","text":"<p>Error Marts are an optional layer in the Data Vault that can be useful for surfacing data issues to the business users. Remember that all data, correct or not, should remain as historical data in the Data Vault for audit and traceability.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#metrics-marts","title":"Metrics Marts","text":"<p>The Metrics Mart is an optional tier used to surface operational metrics for analytical o r reporting purposes.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#data-vault-pipeline","title":"Data Vault Pipeline","text":"<p>Let\u2019s run through each stage of the pipeline</p> <ol> <li> <p>Data is landed either as an INSERT OVERWRITE or INSERT ONLY. Without dropping    the target\u2019s contents then this is the first place we can use Streams to process    new records only downstream.</p> </li> <li> <p>Landed content is staged with data vault metadata tags, some of these are:</p> </li> <li> <p>Surrogate hash keys \u2013 for joining related data vault tables</p> </li> <li>Load date \u2013 the timestamp of when the data enters the data warehouse</li> <li>Applied date \u2013 the timestamp of the landed data</li> <li>Record source \u2013 where the data came from</li> <li> <p>Record hashes \u2013 a single column representing a collection of descriptive attributes</p> </li> <li> <p>Autonomous loads through</p> </li> <li> <p>Hub loaders \u2013 a template reused to load one or many hub tables</p> </li> <li>Link loaders \u2013 a template reused to load zero or many link tables</li> <li> <p>Sat loaders \u2013 a template reused to load zero or many satellite tables</p> </li> <li> <p>Test Automation measuring the integrity of all loaded (and related) data vault    artefacts from the staged content</p> </li> <li> <p>Snapshot is taken of the current load dates and surrogate hash keys from the    parent entity (hub or link) and adjacent satellite tables.</p> </li> <li> <p>Use the AS_OF date table to control the PIT manifold to periodically populate    target PIT tables at the configured frequency.</p> </li> <li> <p>PIT</p> <p></p> <p>A Point-in-Time (PIT) table is a physical collection of applicable surrogate  keys and load dates for a snapshot period. It must be a table otherwise the  potential benefits of EQUIJOIN are not realised.</p> <p>It improves join performance and forms the base for information mart views  and easily allows you to define marts to be built for specific logarithmic  time windows by a snapshot date. The traditional approach to building a PIT  table makes use of an adjacent date table to define the logarithmic period  and the PIT windows itself.</p> </li> <li> <p>AS_OF</p> <p></p> <p>AS_OF table controls the PIT snapshot in a combination of two ways</p> <ul> <li>By Window: defining the start and end date of the AS_OF period, the window    of snapshots to take. You can define a much larger than needed table but    subset the window in execution.</li> <li>By Frequency: defining at what frequency snapshot keys are to be sent to    a PIT target. This is tailored by the business and is a part of the report    frequency and depth. Ideally this would not have any involvement by engineering    teams, only to set this up. From there the business controls the 1 and 0 switches.</li> </ul> </li> <li> <p>Information Marts that are defined once as views over a designated PIT table.</p> </li> </ol>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#data-vault-architecture","title":"Data Vault Architecture","text":"<ul> <li> <p>Raw Source (Copy from your OLTP data sources)</p> </li> <li> <p>Staging (nowadays as Persistent Staging Area in a Datalake, because it is cheaper than a Relational DB)</p> </li> <li> <p>Raw Vault (applying so-called Hard Rules, like data type changes)</p> </li> <li> <p>Business Vault (applying so-called Soft Rules, all your Business Logic, Aggregations, Concatenation, ...)</p> </li> <li> <p>Information Mart (Data Mart sometimes virtualized, but not always ... usually Star/Snowflake Schema)</p> </li> <li> <p>Cube/Tabular Model</p> </li> <li> <p>BI Tool</p> </li> </ul> <p>Hard Rules: \\ These should be applied before data is stored in the DataVault. Any rules applied here do not alter the contents or the granularity of the data, and maintains auditability.</p> <ul> <li>Data typing</li> <li>Normalization / De-normalization</li> <li>Adding system fields (tags)</li> <li>De-duplication</li> <li>Splitting by record structure</li> <li>Trimming spaces from character strings</li> </ul> <p>Soft Rules: \\ Rules that change, or interpret the data, for example adds business logic. This changes the granularity of the data.</p> <ul> <li>Concatenating name fields</li> <li>Standardizing addresses</li> <li>Computing monthly sales</li> <li>Coalescing</li> <li>Consolidation</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#data-vault-modeling-approaches","title":"Data Vault Modeling Approaches","text":"<ul> <li> <p>Bottom-up approach (Source-Driven)   This is a source centric design approach, where you rely on understanding the   source systems and design your DV model based on that.</p> </li> <li> <p>Top-down approach (Business-Driven)   This is a business centric design approach, where you start by understanding the   business needs, use cases, and business keys in order to design the DV Model.</p> </li> <li> <p>Combined approach   This is a hybrid approach, where you use both Bottom-up and Top-down approaches.</p> </li> </ul> <p>Examples:</p> <ul> <li>Hubs and Links follow Top-down approach</li> <li>Satellite follows Bottom-up approach</li> </ul> <p></p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#unit-of-work","title":"Unit of Work","text":"<ul> <li>https://hanshultgren.wordpress.com/2011/05/04/unit-of-work-demystified/</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#advantages-of-data-vault","title":"Advantages of Data Vault","text":"<ul> <li>Scalable \\   One of the biggest advantages of Data Vault model is the ability to scale up or down   quickly - a huge asset for companies going through growth or transition periods.</li> </ul> <p>Because satellites are source system specific, adding sources is an easy as adding   satellites. No updates to existing satellite tables are required.</p> <p>Note: \\ Update may be necessary for views in the information mart</p> <ul> <li>Repeatable \\   Three main entities - Hubs, Links, and Satellites - all follow the same pattern.   Scripts to build tables or run ETL processes can be automated based on these patterns   and metadata.</li> </ul> <p>A number of services and programs exist to quickly automate these processes</p> <ul> <li>Auditable \\   Build on core principle that data is never deleted, and all data is loaded in   its original format. Record source column in entities allows for tracking back   to source system.</li> </ul> <p>Tracking of business keys and separation of business keys (Hubs) from context (Satellites)   allows for easier compliance with GDPR &amp; similar data protection regulations.</p> <ul> <li> <p>Adaptable \\   Separation of hard and soft rules allows for quicker updated.</p> </li> <li> <p>Hard: Any rule that does not change content of individual fields or gain.</p> </li> <li>Soft: Any rule that changes or interprets data, or changes the gain (turning data into information)</li> </ul> <p>Changes in business logic requires no change to ETL processes - only updates   to virtualized information mart layer.</p> <p>Fit within an Agile Framework</p> <ul> <li> <p>Optimized Loading</p> </li> <li> <p>Decreased process complexity</p> </li> <li>Decreased amount of data being processed</li> <li> <p>Increased opportunities for parallelization</p> </li> <li> <p>Platform Agnostic   A data vault architecture and model can be built on many platforms - both on premise   and on cloud. Initial design for Data Vaults were to handle batch processing,   but patterns also now exist for handling streaming data.</p> </li> </ul> <p>Good fit for:</p> <ul> <li>Enterprise teams where the ability to audit data is a primary concern</li> <li>Teams that need flexibility and who want to make large structural changes to their data without causing delays in reporting</li> <li>More technical data teams that can manage and govern the network-like growth of data vault models</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#disadvantages","title":"Disadvantages","text":"<ul> <li>Training &amp; Complexity   Because the Data Vault is not a well known modeling technique, hiring or training   staff may be an issue or expense.</li> </ul> <p>Data Vault models have a tendency to become very large and complex, which may be   a daunting process for those new to the technique.</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#conclusion","title":"Conclusion","text":"<ul> <li>Fix the SCD2 problem, because of SAT table can only insert strategy not update, or delete.</li> <li>Any record have unique key.</li> <li>Can do Change Data Capture (CDC) because all record have end_datetime tracking.</li> <li>\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e47\u0e19 Agile Data Warehouse \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e16\u0e49\u0e32 business process \u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 \u0e01\u0e47\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 data model \u0e43\u0e2b\u0e21\u0e48\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 \u0e41\u0e04\u0e48\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e2b\u0e23\u0e37\u0e2d\u0e41\u0e01\u0e49 Sat \u0e44\u0e21\u0e48\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e1a Dimension model \u0e17\u0e35\u0e48 fact table \u0e43\u0e2b\u0e0d\u0e48\u0e21\u0e32\u0e01\u0e41\u0e25\u0e30\u0e40\u0e01\u0e47\u0e1a relation \u0e15\u0e48\u0e32\u0e07\u0e46\u0e44\u0e27\u0e49 \u0e08\u0e36\u0e07\u0e41\u0e01\u0e49\u0e44\u0e02\u0e44\u0e14\u0e49\u0e22\u0e32\u0e01\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e08\u0e30\u0e01\u0e23\u0e30\u0e17\u0e1a Relation \u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19 fact \u0e19\u0e31\u0e49\u0e19\u0e44\u0e1b\u0e2b\u0e21\u0e14 \u0e07\u0e48\u0e32\u0e22\u0e17\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e44\u0e1b\u0e2a\u0e23\u0e49\u0e32\u0e07 Fact \u0e43\u0e2b\u0e21\u0e48\u0e17\u0e35\u0e48\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e0b\u0e49\u0e33\u0e0b\u0e49\u0e2d\u0e19\u0e01\u0e31\u0e1a Fact \u0e40\u0e01\u0e48\u0e32\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19 \u0e01\u0e47\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e25\u0e37\u0e2d\u0e07 Data Storage \u0e2d\u0e35\u0e01</li> <li>\u0e16\u0e49\u0e32\u0e21\u0e35 Entity Relation \u0e43\u0e2b\u0e21\u0e48 \u0e40\u0e02\u0e49\u0e32\u0e21\u0e32 \u0e08\u0e30\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e02\u0e49\u0e32\u0e17\u0e35\u0e48 Link table \u0e44\u0e14\u0e49\u0e17\u0e31\u0e19\u0e17\u0e35</li> <li>Data Sources \u0e21\u0e35\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22 \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e09\u0e30\u0e19\u0e31\u0e49\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 Record \u0e02\u0e2d\u0e07\u0e17\u0e38\u0e01 table \u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e23\u0e30\u0e1a\u0e38 Record Source \u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e21\u0e32\u0e08\u0e32\u0e01 data source \u0e44\u0e2b\u0e19 \u0e23\u0e30\u0e1a\u0e1a\u0e44\u0e2b\u0e19</li> <li>\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e01\u0e47\u0e1a data \u0e40\u0e1b\u0e47\u0e19 centralize data warehouse/hub \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e44\u0e27\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19 source \u0e02\u0e2d\u0e07 data mart, Adhoc analytics \u0e2b\u0e23\u0e37\u0e2d ETL/ELT application \u0e15\u0e48\u0e2d\u0e44\u0e1b</li> <li>\u0e44\u0e21\u0e48\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33 Reporting \u0e17\u0e35\u0e48 Data Vault \u0e40\u0e1e\u0e23\u0e32\u0e30 data \u0e17\u0e35\u0e48\u0e19\u0e35\u0e48\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23 Transform, Cleansing</li> <li>\u0e04\u0e27\u0e23\u0e17\u0e33 Data Marts, ETL / ELT \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e33 Analytics, Reporting</li> <li>\u0e04\u0e19\u0e17\u0e35\u0e48 Implement&amp;Maintain Data Vault \u0e15\u0e49\u0e2d\u0e07\u0e21\u0e35\u0e2a\u0e01\u0e34\u0e25\u0e23\u0e30\u0e14\u0e31\u0e1a\u0e19\u0e36\u0e07\u0e40\u0e1e\u0e23\u0e32\u0e30 Surrogate Key \u0e08\u0e30\u0e43\u0e0a\u0e49\u0e01\u0e32\u0e23 Hashing \u0e17\u0e33 \u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e40\u0e1b\u0e47\u0e19 run_id \u0e40\u0e09\u0e22\u0e46</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-approach/#references","title":"References","text":"<ul> <li>https://www.databricks.com/glossary/data-vault</li> <li>https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html</li> <li>https://atitawat.medium.com/data-vault-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-part1-2f4cf602ed6c</li> <li>https://medium.com/hashmapinc/getting-started-with-data-vault-2-0-c19945874fe3</li> <li>https://medium.com/hashmapinc/3nf-and-data-vault-2-0-a-simple-comparison-4b0694c9a1d1</li> <li>https://aginic.com/blog/modelling-with-data-vaults/</li> <li>https://www.waitingforcode.com/general-big-data/data-vault-2-big-data/read</li> <li>https://www.scalefree.com/scalefree-newsletter/point-in-time-tables-insurance/</li> <li>https://digitalcommons.georgiasouthern.edu/cgi/viewcontent.cgi?article=2402&amp;context=etd ***</li> <li>https://www.linkedin.com/pulse/data-vault-pit-flow-manifold-patrick-cuba/</li> <li>https://medium.com/snowflake/data-vault-naming-standards-76c93413d3c7</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-business-vault-act-schema/","title":"Business Vault &amp; Activity Schema","text":"<ul> <li>https://patrickcuba.medium.com/business-vault-activity-schema-03f9b30c11d9</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/","title":"Data Vault Implementation","text":""},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#hubs","title":"Hubs","text":"<pre><code>CREATE PROCEDURE [RV].[LOAD_PRODUCT_HUB]\n    @LOAD_PRCS      BIGINT\nAS\n    WITH NewProductBussKey AS\n    (\n        SELECT DISTINCT product_number\n        FROM [STG].[PRODUCT_TABLE_2022]     AS SRC\n        LEFT OUTER JOIN [RV].[HUB_PRODUCT]  AS TGT\n            ON SRC.product_number = TGT.product_key\n        WHERE TGT.product_key IS NULL\n    )\n    INSERT INTO [RV].[HUB_PRODUCT] WITH (TABLOCK) -- bulk load\n    SELECT\n        HASH(product_number)                AS product_hash_key,\n        product_number                      AS product_key,\n        GETDATE()                           AS LDTS,\n        @LOAD_PRCS                          AS LDID, -- unique ID for load\n        'PRODUCT_TABLE_2022'                AS Record_Source\n    FROM [STG].[PRODUCT_TABLE_2022]\nRETURN 0\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#links","title":"Links","text":"<pre><code>CREATE PROCEDURE [RV].[LOAD_SALES_PRODUCT_LINK]\n    @LOAD_PRCS      BIGINT\nAS\n    WITH NewDistinctBusinessKey AS\n    (\n        SELECT DISTINCT\n            H_SALES.sales_key,\n            H_PRODUCT.product_key\n        FROM [STG].[SALES_ORDER_DETAIL_2022]        AS SRC\n        LEFT OUTER JOIN [RV].[HUB_SALES]            AS H_SALES -- lookup sales key\n            ON SRC.sales_order_id = H_SALES.sales_key\n        LEFT OUTER JOIN [RV].[HUB_PRODUCT]          AS H_PRODUCT -- lookup product key\n            ON SRC.product_number = H_PRODUCT.product_key\n        LEFT OUTER JOIN [RV].[LINK_SALES_PRODUCT]     AS TGT\n            ON TGT.product_key  = H_PRODUCT.product_key\n            AND TGT.sales_key   = H_SALES.sales_key\n        WHERE TGT.sales_key IS NULL\n    )\n    INSERT INTO [RV].[LINK_SALES_PRODUCT] WITH (TABLOCK) -- bulk load\n    SELECT\n        HASH(sales_order_id, product_number)        AS link_sales_product_hash_key,\n        sales_order_id                              AS sales_key,\n        product_number                              AS product_key,\n        GETDATE()                                   AS LDTS,\n        @LOAD_PRCS                                  AS LDID, -- unique ID for load\n        'SALES_ORDER_DETAIL_2022'                   AS Record_Source\n    FROM [STG].[SALES_ORDER_DETAIL_2022]\nRETURN 0\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#satellites","title":"Satellites","text":"<pre><code>CREATE PROCEDURE [RV].[Load_SAT_ProductDetails]\n    @LOAD_PRCS BIGINT\nAs\n    DECLARE @RecordSource        nvarchar(100)\n    DECLARE @DefaultValidFrom    datetime2(0)     \u2013use datetime2(0) to remove milliseconds\n    Declare @DefaultValidTo      datetime2(0)\n    DECLARE @LoadDateTime        datetime2(0)\n    SET @RecordSource            = N\u2019AdventureWorks2012.Product.Product\u2018\n    SET @DefaultValidFrom        = \u201a1900-01-01\u2018\n    SET @DefaultValidTo          = \u201a9999-12-31\u2018\n    SET @LoadDateTime            = GETDATE()\nBEGIN TRY\nBegin Transaction\n    INSERT INTO [RV].[SAT_ProductDetails_DV20]\n    (\n         Product_Hsk,\n         LoadTimestamp,\n         Name,\n         ListPrice,\n         LOAD_PRCS,\n         RecordSource,\n         ValidFrom,\n         ValidTo,\n         IsCurrent\n    )\n    SELECT\n         Product_Hsk,                            \u2013Hash Key\n         @LoadDateTime,                          \u2013LoadDatetimeStamp\n         Name,\n         ListPrice,\n         @LOAD_PRCS as LOAD_PRCS,\n         @RecordSource as RecordSource,\n         @LoadDateTime,                          \u2013Actual DateTimeStamp\n         @DefaultValidTo,                        \u2013Default Expiry DateTimestamp\n         1                                       \u2013IsCurrent Flag\n    FROM\n    (\n        MERGE [RV].[SAT_ProductDetails_DV20]       AS Target     \u2013Target: Satellite\n        USING\n        (\n           \u2014 Query distinct set of attributes from source (stage)\n           SELECT DISTINCT\n               stage.Product_Hsk,\n               stage.Name,\n           stage.ListPrice\n           FROM stage.Product_Product_AdventureWorks2012_DV20 as stage\n        ) AS Source\n            ON Target.Product_Hsk = Source.Product_Hsk         \u2013Identify Columns by Hub/Link Hash Key\n            AND Target.IsCurrent = 1                           \u2013and only merge against current records in the target\n        \u2013when record already exists in satellite and an attribute value changed\n        WHEN MATCHED AND\n            (\n                 Target.Name &lt;&gt; Source.Name\n                 OR Target.ListPrice &lt;&gt; Source.ListPrice\n            )\n        \u2014 then outdate the existing record\n        THEN UPDATE SET\n            IsCurrent  = 0,\n            ValidTo    = @LoadDateTime\n        \u2014 when record not exists in satellite, insert the new record\n        WHEN NOT MATCHED BY TARGET\n        THEN INSERT\n        (\n            Product_Hsk,\n            LoadTimestamp,\n            Name,\n            ListPrice,\n            LOAD_PRCS,\n            RecordSource,\n            ValidFrom,\n            ValidTo,\n            IsCurrent\n        )\n        VALUES\n        (\n            Source.Product_Hsk,\n            @LoadDateTime,\n            Source.Name,\n            Source.ListPrice,\n            @LOAD_PRCS,\n            @RecordSource,\n            @DefaultValidFrom,     \u2013Default Effective DateTimeStamp\n            @DefaultValidTo,       \u2013Default Expiry DateTimeStamp\n            1                      \u2013IsCurrent Flag\n        )\n        \u2014 Output changed records\n        OUTPUT\n            $action AS Action\n            ,Source.*\n    ) AS MergeOutput\n    WHERE MergeOutput.Action = 'UPDATE' AND Product_Hsk IS NOT NULL;\n\n    COMMIT\n\n    SELECT\n        'Success' as ExecutionResult\n    RETURN;\nEND TRY\nBEGIN CATCH\n     IF @@TRANCOUNT &gt; 0\n     ROLLBACK\n     SELECT\n          'Failure' as ExecutionResult,\n          ERROR_MESSAGE() AS ErrorMessage;\n     RETURN;\nEND CATCH\nGO\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#dimension","title":"Dimension","text":"<pre><code>CREATE OR REPLACE VIEW v_curr_customer AS\nSELECT hub.h_customer_key\n     , hub.customer_no\n     , s1.preferred_contact\n     , s1.e_mail_address\n     , s1.phone_number\n     , s1.private_person\n     , s1.reseller\n     , s1.delivery_type\n     , s2.last_name cust_last_name\n     , s2.first_name cust_first_name\n     , s2.street cust_street\n     , s2.street_no cust_street_no\n     , s2.zip_code cust_zip_code\n     , s2.city cust_city\n     , s3.last_name bill_last_name\n     , s3.first_name bill_first_name\n     , s3.street bill_street\n     , s3.street_no bill_street_no\n     , s3.zip_code bill_zip_code\n     , s3.city bill_city\n  FROM h_customer hub\n  JOIN pit_customer pit\n    ON (hub.h_customer_key = pit.h_customer_key)\n  LEFT JOIN s_customer_info s1\n    ON (s1.h_customer_key = pit.h_customer_key AND s1.load_date = pit.s1_load_date)\n  LEFT JOIN s_customer_address s2\n    ON (s2.h_customer_key = pit.h_customer_key AND s2.load_date = pit.s2_load_date)\n  LEFT JOIN s_billing_address s3\n    ON (s3.h_customer_key = pit.h_customer_key AND s3.load_date = pit.s3_load_date)\n WHERE pit.load_end_date IS NULL;\n</code></pre> <pre><code>EXPLAIN PLAN FOR\nSELECT h_customer_key\n     , customer_no     \u2014 H_CUSTOMER\n     , e_mail_address  \u2014 S_CUSTOMER_INFO\n     , cust_first_name \u2014 S_CUSTOMER_ADDRESS\n     , bill_first_name \u2014 S_BILLING_ADDRESS\n  FROM v_curr_customer;\n\nSELECT * FROM dbms_xplan.display();\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n| Id  | Operation                     | Name               | Rows  |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n|   0 | SELECT STATEMENT              |                    | 20000 |\n|*  1 |  HASH JOIN RIGHT OUTER        |                    | 20000 |\n|   2 |   TABLE ACCESS STORAGE FULL   | S_CUSTOMER_INFO    | 27000 |\n|*  3 |   HASH JOIN RIGHT OUTER       |                    | 20000 |\n|   4 |    TABLE ACCESS STORAGE FULL  | S_BILLING_ADDRESS  |  4964 |\n|*  5 |    HASH JOIN OUTER            |                    | 20000 |\n|*  6 |     HASH JOIN                 |                    | 20000 |\n|   7 |      TABLE ACCESS STORAGE FULL| H_CUSTOMER         | 20000 |\n|*  8 |      TABLE ACCESS STORAGE FULL| PIT_CUSTOMER       | 20000 |\n|   9 |     TABLE ACCESS STORAGE FULL | S_CUSTOMER_ADDRESS | 33406 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\n</code></pre> <pre><code>EXPLAIN PLAN FOR\nSELECT h_customer_key\n   \u2013, customer_no     \u2014 H_CUSTOMER\n     , e_mail_address  \u2014 S_CUSTOMER_INFO\n     , cust_first_name \u2014 S_CUSTOMER_ADDRESS\n   \u2013, bill_first_name \u2014 S_BILLING_ADDRESS\n  FROM v_curr_customer;\n\nSELECT * FROM dbms_xplan.display();\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n| Id  | Operation                   | Name               | Rows  |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n|   0 | SELECT STATEMENT            |                    | 20000 |\n|*  1 |  HASH JOIN RIGHT OUTER      |                    | 20000 |\n|   2 |   TABLE ACCESS STORAGE FULL | S_CUSTOMER_INFO    | 27000 |\n|*  3 |   HASH JOIN OUTER           |                    | 20000 |\n|*  4 |    TABLE ACCESS STORAGE FULL| PIT_CUSTOMER       | 20000 |\n|   5 |    TABLE ACCESS STORAGE FULL| S_CUSTOMER_ADDRESS | 33406 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#loading-dimensions-from-data-vault-model","title":"Loading Dimensions From Data Vault Model","text":"<p>https://danischnider.wordpress.com/2021/12/20/multi-version-load-in-data-vault/ https://danischnider.wordpress.com/2015/11/12/loading-dimensions-from-a-data-vault-model/</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#insert-only","title":"Insert-only","text":"<p>https://www.scalefree.com/scalefree-newsletter/insert-only-in-data-vault/</p>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#tips-to-get-the-best-performance-out-of-a-data-vault-model-in-databricks-lakehouse","title":"Tips to get the best performance out of a Data Vault Model in Databricks Lakehouse","text":"<ul> <li> <p>Use Delta Formatted tables for Raw Vault, Business Vault and Gold layer tables.</p> </li> <li> <p>Make sure to use OPTIMIZE and Z-order indexes on all join keys of Hubs, Links   and Satellites.</p> </li> <li> <p>Do not over partition the tables -especially the smaller satellites tables.   Use Bloom filter indexing on Date columns, current flag columns and predicate   columns that are typically filtered on to ensure the best performance - especially   if you need to create additional indices apart from Z-order.</p> </li> <li> <p>Delta Live Tables (Materialized Views) makes creating and managing PIT tables very easy.</p> </li> <li> <p>Reduce the <code>optimize.maxFileSize</code> to a lower number, such as 32-64MB vs. the   default of 1 GB. By creating smaller files, you can benefit from file pruning   and minimize the I/O retrieving the data you need to join.</p> </li> <li> <p>Data Vault model has comparatively more joins, so use the latest version of DBR   which ensures that the Adaptive Query Execution is ON by default so that the best   Join strategy is automatically used. Use Join hints only if necessary.   (for advanced performance tuning).</p> </li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#implements","title":"Implements","text":"<ul> <li>Data Vault on Snowflake</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-data-vault-implement/#references","title":"References","text":"<ul> <li>https://dbtvault.readthedocs.io/en/latest/</li> <li>https://www.oraylis.de/blog/2016/data-vault-satellite-loads-explained</li> <li>https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html</li> <li>https://datavaultalliance.com/news/building-a-real-time-data-vault-in-snowflake/ ***</li> <li>https://jerryrun.wordpress.com/2018/09/12/chapter-4-data-vault-20-modeling/</li> <li>https://github.com/dbsys21/databricks-lakehouse/blob/main/lakehouse-buildout/data-vault/TPC-DLT-Data-Vault-2.0.sql</li> <li>https://aws.amazon.com/blogs/big-data/design-and-build-a-data-vault-model-in-amazon-redshift-from-a-transactional-database/</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/","title":"Dimension Model Implementation","text":""},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#example-01","title":"Example: 01","text":"<p>I will discuss Star Schema and how to create one using credit-debit transaction dataset in Snowflake data warehouse. This post is more suitable to those who are new to Star Schema or Dimensional Modeling. But if you are a seasoned data modeler, you are more than welcome to read this post and please leave your feedback in the comment section.</p> <p>So how do we build a Star Schema? As proposed by Kimball, there are 4 steps in designing of a dimensional model.</p> <ol> <li> <p>Select the business process. The first step is to identify the business    process that you want to model. Model the processes that are most significant    or relevant to the business first.</p> </li> <li> <p>Declare the grain. Grain refers to the level of detail of the information    that you will store in the fact table. The grain should be at the most atomic    or lowest level possible. For example, A line item on a grocery receipt. The    grocery owner might want to ask questions such as \u201cwhat are the items that    sold the best during the day in our grocery store?\u201d, and to answer this question    we need to dig into line-item level instead of the order-level.</p> </li> <li> <p>Identify the dimensions. You can identify the dimensions by looking at the    descriptive information or attributes that exist in your business process and    provide context to your measurable events. For example: payment method, customers,    locations, etc.</p> </li> <li> <p>Identify the facts. Facts are the quantitative measures in your business    process that are always in numeric. For example: price, minutes, speed, etc.    You should identify/select the measures that are true to your selected grain.</p> </li> </ol> <p>The dataset has 23 columns but for simplicity, I will exclude 4 irrelevant columns. Here are the descriptions of the columns.</p> Columns Descriptions TRANSACTION_REFERENCE The transaction identifier for each transaction made by consumer USER_REFERENCE The user identifier of the consumer AGE_BAND The consumer age range SALARY_BAND The consumer salary range. POSTCODE The postcode of where the consumer lives. LSOA Geographical hierarchy: small areas that has similar population size (average of approximately 1,500 residents or 650 households). MSOA Geographical hierarchy: medium areas where the minimum population size is 5000 (average of 7200). DERIVED_GENDER The consumer gender identity. TRANSACTION_DATE The transaction date made by the consumer. ACCOUNT_REFERENCE The consumer bank account identifier. PROVIDER_GROUP_NAME The consumer's bank for executing his/her transactions. ACCOUNT_TYPE The account type: current, savings, etc. CREDIT_DEBIT Type of transaction made by consumer: debit or credit. AMOUNT The amount of transaction. AUTO_PURPOSE_TAG_NAME The transaction purpose. MERCHANT_NAME The merchant's name. MERCHANT_BUSINESS_LINE The merchant's business category. ACCOUNT_CREATED_DATE The date of when the account first created. ACCOUNT_LAST_REFRESHED The date of when the account last updated."},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#dim_users","title":"DIM_USERS","text":"<p>DIM_USERS will store users\u2019 demographic information such as user id, age, salary, gender and address.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_users AS (\n    SELECT\n        DISTINCT user_reference AS user_id,\n        age_band,\n        salary_band,\n        postcode,\n        LSOA,\n        MSOA,\n        derived_gender AS gender\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#dim_accounts","title":"DIM_ACCOUNTS","text":"<p>DIM_ACCOUNTS stores account level attributes such as account id, bank name and account type.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_accounts AS (\n    SELECT\n        DISTINCT account_reference AS account_id,\n        provider_group_name AS bank_name,\n        account_type,\n        account_created_date,\n        account_last_refreshed\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#dim_merchants","title":"DIM_MERCHANTS","text":"<p>All information about the merchants such as merchant\u2019s name and business category will be stored in DIM_MERCHANTS. The dataset does not provide merchant identifier, so in this case I have decided to create a surrogate key for merchant\u2019s key identifier.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_merchants AS (\n    SELECT\n        DISTINCT HASH(merchant_name, merchant_business_line)::VARCHAR AS merchant_id,\n        merchant_name,\n        merchant_business_line\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#dim_transactions","title":"DIM_TRANSACTIONS","text":"<p>DIM_TRANSACTIONS stores information on transaction attributes such as transaction type and purpose.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_transactions AS (\n    SELECT\n        transaction_reference AS transaction_id,\n        credit_debit AS transaction_type,\n        auto_purpose_tag_name AS transaction_purpose\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#dim_dates","title":"DIM_DATES","text":"<p>I decided to create a data dimension that stores all date related parsed values such as day of the month, day name, month of the year, month name, etc. This will be handy when we need to generate time based reports.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.dim_dates AS (\n    SELECT\n        DISTINCT transaction_date,\n        DAY(transaction_date)::VARCHAR as day_of_month,\n        DAYNAME(transaction_date) as day_name,\n        MONTH(transaction_date)::VARCHAR as month_of_year,\n        MONTHNAME(transaction_date) as month_name,\n        YEAR(transaction_date)::VARCHAR as year\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#fct_transactions","title":"FCT_TRANSACTIONS","text":"<p>FCT_TRANSACTIONS will store the numeric information (transaction amount) and foreign keys that connect it to the dimension tables. To note, I also add transaction date column as a way to connect the FCT_TRANSACTIONS to DIM_DATES table. A better approach is to use surrogate key to generate date identifier but that is outside the scope of this post.</p> <pre><code>CREATE OR REPLACE TABLE analytics.marts_credit_debit_transaction.fct_transactions AS (\n    SELECT\n        transaction_date AS transaction_date,\n        transaction_reference AS transaction_id,\n        user_reference AS user_id,\n        account_reference AS account_id,\n        HASH(merchant_name, merchant_business_line)::VARCHAR AS merchant_id,\n        amount::NUMBER as amount\n    FROM raw_credit_debit_transaction.public_listing.transactions\n);\n</code></pre> <p></p>"},{"location":"abstract/data_management/data_modeling/dwh-dim-implement/#references","title":"References","text":"<ul> <li>https://danischnider.wordpress.com/2022/11/10/star-schema-design-in-oracle-fundamentals/</li> <li>https://blog.devgenius.io/implementing-star-schema-in-snowflake-data-warehouse-1f890cdda952</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-dim-rapidly-changing-dimension/","title":"Rapidly Changing Dimension","text":"<p>A dimension is a fast changing or rapidly changing dimension if one or more of its attributes in the table changes very fast and in many rows. Handling rapidly changing dimension in data warehouse is very difficult because of many performance implications.</p> <p>As you know slowly changing dimension type 2 is used to preserve the history for the changes. But the problem with type 2 is, with each and every change in the dimension attribute, it adds new row to the table. If in case there are dimensions that are changing a lot, table become larger and may cause serious performance issues. Hence, use of the type 2 may not be the wise decision to implement the rapidly changing dimensions.</p> <p>Separate Rapidly Changing Attribute by Implementing Junk Dimension</p> <p>Consider the fact table, in which not all the attributes of the table changes rapidly. There may be some attribute that may be changing rapidly and other not. The idea here is to separate the rapidly changing attribute from the slowly changing ones and move those attribute to another table called junk dimension and maintain the slowly changing attribute in same table. In this way, we can handle situation of increasing table size.</p> <p>For example:</p> <p>Consider patient dimension where there are 1000 rows in it. On average basis, each patient changes the 10 of attributes in a year. If you use the type 2 to manage this scenario, there will be 1000*10 = 10000 rows. Imagine if the table has 1 million rows, it\u2019ll become very hard to handle the situation with type 2. Hence, we use rapidly changing dimension approach.</p> <pre><code>erDiagram\n    PATIENT {\n        integer Patient_id PK\n        string Name\n        string Gender\n        string Marital_status\n        numeric Weight\n        numeric BMI\n    }</code></pre> <p>The attribute like patient_id, Name, Gender, Marital_status will not change or changes very rarely. And attribute like weight and BMI (body mass index) changes every month based on the patient visit to hospital. So, we need to separate the weight column out of the patient table otherwise we end up filling the table if we use SCD type 2 on PATIENT dimension. We can put the weight column which is rapidly changing into junk dimension table.</p> <p>Below is the structure of Junk dimension table:</p> <pre><code>erDiagram\n    PATIENT_JNK_DIM {\n        integer Pat_SK PK\n        numeric Weight\n        numeric BMI\n    }</code></pre> <p>Note: Pat_SK is the surrogate key and acts as a primary key for junk dimension table.</p> <p>Link Junk Dimension and PATIENT Table</p> <p>In this step, we must link the junk dimension and patient table. Keep in mind; we cannot simply refer the junk dimension table by adding its primary key to patient table as foreign key. Because any changes made to junk dimension will have to reflect in the patient table, this obviously increases the data in patient dimension. Instead, we create one more table called mini dimension that acts as a bridge between Patient and Junk dimension. We can also add the columns such as start and end date to track the change history. Below is the structure of the mini dimension:</p> <pre><code>erDiagram\n    PATIENT_MINI_DIM {\n        integer Pat_SK PK\n        integer Pat_id PK\n        datetime Start_Date\n        datetime End_Date\n    }</code></pre> <p>This table is just bridge between two tables and does not require any surrogate key in it. Below is the diagrammatic representation of the Rapidly Changing Dimension implementation.</p> <pre><code>erDiagram\n    PATIENT {\n        integer Patient_id PK\n        string Name\n        string Gender\n        string Marital_status\n    }\n    PATIENT_MINI_DIM {\n        integer Pat_SK PK\n        integer Pat_id PK\n        datetime Start_Date\n        datetime End_Date\n    }\n    PATIENT_JNK_DIM {\n        integer Pat_SK PK\n        numeric Weight\n        numeric BMI\n    }\n    PATIENT ||--o{ PATIENT_MINI_DIM : is\n    PATIENT_JNK_DIM ||--o{ PATIENT_MINI_DIM : is</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-dim-rapidly-changing-dimension/#references","title":"References","text":"<ul> <li>https://dwgeek.com/rapidly-changing-dimension-data-warehouse.html/</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-dim-slowly-changing-dimension/","title":"Slowly Change Dimension","text":"<p>It depends on the business requirement, where any particular feature history of changes in the data warehouse is preserved. It is called a slowly changing feature, and a quality dimension is called a slowly changing dimension.</p>"},{"location":"abstract/data_management/data_modeling/dwh-dim-techniques/","title":"Dimensional Modeling Techniques","text":"<p>Table of Contents:</p> <ul> <li>Dimension Hierarchy Techniques</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-dim-techniques/#dimension-hierarchy-techniques","title":"Dimension Hierarchy Techniques","text":""},{"location":"abstract/data_management/data_modeling/dwh-dim-techniques/#fixed-depth-positional-hierarchies","title":"Fixed Depth Positional Hierarchies","text":"<p>A fixed depth hierarchy is a series of many-to-one relationships, such as product to brand to category to department. When a fixed depth hierarchy is defined and the hierarchy levels have agreed upon names, the hierarchy levels should appear as separate positional attributes in a dimension table. A fixed depth hierarchy is by far the easiest to understand and navigate as long as the above criteria are met. It also delivers predictable and fast query performance. When the hierarchy is not a series of many-to-one relationships or the number of levels varies such that the levels do not have agreed upon names, a ragged hierarchy technique must be used.</p>"},{"location":"abstract/data_management/data_modeling/dwh-dim-techniques/#slightly-raggedvariable-depth-hierarchies","title":"Slightly Ragged/Variable Depth Hierarchies","text":"<p>Slightly ragged hierarchies don\u2019t have a fixed number of levels, but the range in depth is small. Geographic hierarchies often range in depth from perhaps three levels to six levels. Rather than using the complex machinery for unpredictably variable hierarchies, you can force-fit slightly ragged hierarchies into a fixed depth positional design with separate dimension attributes for the maximum number of levels, and then populate the attribute value based on rules from the business.</p>"},{"location":"abstract/data_management/data_modeling/dwh-dim-techniques/#references","title":"References","text":"<ul> <li>https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-inmon-approach/","title":"Bill Inmon Approach","text":"<p>The Bill Inmon design approach uses the normalized form (3NF) for building entity structure, avoiding data redundancy as much as possible. This results in clearly identifying business requirements and preventing any data update irregularities.</p> <p>The Inmon approach, also known as normalized modeling, is known as the top-down or data-driven strategy, in which we start with the data warehouse and break it down into data marts. These data marts are then specialised to satisfy the demands of other departments inside the firm, such as finance, accounting, and human resources.</p> <ul> <li>Begin with the corporation's data model. Identify all of the data sources that   the company has access to.</li> <li>Identify the essential entities (customer, product, order, etc.) and their respective   linkages based on the data and understanding of business needs.</li> <li>Create a thorough, logical model using the entity structure. The logical model   includes all the properties of each entity, as well as their respective interactions   and co-dependencies, in great detail. According to data modelling terminology,   the logical model creates logical schemas for entity relationships.</li> <li>Build the physical model from the logical one. Extract data from various sources,   alter it and integrate it into a normalised data model using ETL operations.   Each normalised data model stores data in the third normal form to avoid redundancy.   The data warehouse's core is the normalised data model.</li> <li>Create data marts for different departments. For all reporting needs, data marts   are used to access data, and the data warehouse serves as a single source of truth.</li> </ul> <p>Quote</p> <p>Single version of the truth</p>"},{"location":"abstract/data_management/data_modeling/dwh-inmon-approach/#advantages-of-the-inmon-approach","title":"Advantages of the Inmon Approach","text":"<p>The following are the main advantages of the Inmon method:</p> <ul> <li> <p>Because it is the only source for data marts and all data in the data warehouse is integrated, the data warehouse genuinely acts as the enterprise's single source of truth.</p> </li> <li> <p>Due to the limited redundancy, data update abnormalities are avoided. This simplifies the ETL(Extraction, transformation, and loading) procedure and reduces the risk of failure.</p> </li> <li> <p>Because the logical model represents the distinct business entities, We may easily understand the business processes.</p> </li> <li> <p>Very versatile \u2014 As business requirements change or source data changes, updating the data warehouse is simple because everything is in one location.</p> </li> <li> <p>Can handle a variety of reporting requirements within the organisation.</p> </li> <li> <p>Flexibility: Inmon's approach adapts faster to changing business needs and data source alterations. Inmon's architecture is more versatile due to the ETL process design that results in normalised data. The architects alter only a few normalised tables, communicating the modification downstream.</p> </li> <li>Single source of truth: Because of the normalised data model, the data warehouse serves as a single source of truth for the entire organisation.</li> <li>Less prone to errors: Because normalisation minimises data redundancy, both engineering and analytical procedures are less susceptible to errors.</li> <li>Completeness: Inmon's approach incorporates all Enterprise data, ensuring that all reporting requirements are met.</li> </ul> <p>Good fit for:</p> <ul> <li>Low complexity data that connects neatly together</li> <li>Simple, business-focused downstream use cases for the data</li> <li>Central data teams that have deep knowledge of the facets of their data</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-inmon-approach/#disadvantages-of-the-inmon-approach","title":"Disadvantages of the Inmon Approach","text":"<p>The following are some drawbacks of the Inmon method:</p> <ul> <li> <p>As more tables and joins are added, the model and implementation can grow increasingly complicated.</p> </li> <li> <p>We'll need people knowledgeable in data modelling and the business in general. These resources might be difficult to come by and can be rather costly.</p> </li> <li> <p>Management should be aware that the initial setup and delivery will take longer.</p> </li> <li> <p>More ETL work is required as the data marts are developed from the data warehouse.</p> </li> <li> <p>A vast team of professionals is required to manage the data environment efficiently.</p> </li> <li> <p>Cost of initial setup and regular maintenance: The time and cost required to set up and maintain Inmon's architecture are far greater than the time and investment needed for Kimball's architecture. Normalised schemas are more challenging to build and maintain than their denormalised counterparts.</p> </li> <li>Skill requirement: Highly skilled engineers are required for Inmon's method, which is harder to come by and more expensive to maintain on the payroll.</li> <li>Extra ETL is required: Separating data marts from the data warehouse necessitates the employment of more ETL processes to generate the data marts, resulting in increased engineering overhead.</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-inmon-approach/#reference","title":"Reference","text":"<ul> <li>Inmon Approach in DWH Designing</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/","title":"Ralph Kimball Approach","text":"<p>https://towardsdatascience.com/data-modeling-for-mere-mortals-part-2-dimensional-modeling-fundamentals-ae2f53622321</p> <p>The Kimball methodology, also known as dimensional modeling, is a bottom-up approach that focuses on designing the data warehouse around the business process or subject area.</p> <p>It uses a star schema or snowflake schema to model the data and focuses on creating dimension and fact tables to support analysis. The Kimball methodology is known for its simplicity, flexibility, and ease of use.</p> <p>In contrast, dimensional models or Kimball dimensional data models, data models based on the technique developed by Ralph Kimball, are denormalized structures designed to retrieve data from a data warehouse. They are optimized to perform the Select operation and are used in the basic design framework to build highly optimized and functional data warehouses.</p> <ul> <li> <p>Kimball\u2019s model follows a bottom-up approach. The Data Warehouse (DW) is provisioned   from Datamarts (DM) as and when they are available or required.</p> </li> <li> <p>The Datamarts are sourced from OLTP systems are usually relational databases in   Third normal form (3NF).</p> </li> <li> <p>The Data Warehouse which is central to the model is a de-normalized star schema.   The OLAP cubes are built on this DWH.</p> </li> </ul> <p>I will say that this is the latest model that serve to Data Science or Data Analytic for using to cube analysis process.</p> <p>In contrast, dimensional models or Kimball dimensional data models, data models based on the technique developed by Ralph Kimball, are denormalized structures designed to retrieve data from a data warehouse. They are optimized to perform the Select operation and are used in the basic design framework to build highly optimized and functional data warehouses.</p> <p>The Kimball approach is called bottom-up because we start with user-specific data marts, which are the core building blocks of our conceptual data warehouse. It's critical to know which model best meets your needs from the start; so that it can be incorporated into the data warehouse structure.</p> <ul> <li>Begin by identifying and documenting the most significant business operations,   demands, and queries that are being asked.</li> <li>All data sources available across the organisation should be documented.</li> <li>Create ETL pipelines that gather, modify, and load data into a de-normalised   data model from data sources. The dimensional model is constructed in the form   of either a star schema or a snowflake schema.</li> <li>The dimensional model has frequently constructed around and within dimensional   data marts for specific departments.</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#what-is-data-dimensional-modelling","title":"What is Data Dimensional Modelling?","text":"<p>Data Dimension Model (DDM) : is a technique that uses Dimensions and Facts to store the data in a Data Warehouse efficiently. The purpose is to optimize the database for faster retrieval of the data. Dimensional Models have a specific structure and organise the data to generate reports that improve performance.</p> <p>A dimensional model in data warehouse is designed to read, summarize, analyze numeric information like values, balances, counts, weights, etc. in a data warehouse. In contrast, relation models are optimized for addition, updating and deletion of data in a real-time Online Transaction System.</p> <p>The concept of Dimensional Modelling was developed by Ralph Kimball and consists of \"fact\" and \"dimension\" tables.</p> <p>This article will introduce the concepts and features of Dimensional Data Modelling, the components that make up a Dimensional Data Model, the types &amp; steps of Dimensional Data Modelling and also the benefits and limitations of Dimensional Data Modelling.</p> <p>Key Features of Dimensional Data Modelling:</p> <p>Data Dimensional Modelling has gained popularity because of its unique way of analysing data present in different Data Warehouses. The 3 main features of DDM are as follows:</p> <ul> <li> <p>Easy to Understand: \\   DDM helps developers create and design databases and Schemas easily interpreted   by business users. The relationship between Dimensions and Facts are pretty simple   to read and understand.</p> </li> <li> <p>Promote Data Quality: \\   DDM schemas enforce data quality before loading into Facts and Dimensions.   Dimension and Fact are tied up by foreign keys that act as a constraint for   referential integrity check to prevent fraudulent data from being loaded onto Schemas.</p> </li> <li> <p>Optimise Performance: \\   DDM breaks the data into Dimensions and Facts and links them with foreign keys,   thereby reducing the data redundancy. The data is stored in the optimised form   and hence occupies less storage and can be retrieved faster.</p> </li> </ul> <p>Hence, Dimensional models are used in data warehouse systems and not a good fit for relational systems.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#components-of-dimensional-data-modelling","title":"Components of Dimensional Data Modelling","text":"<p>There are 5 main components of any DDM.</p> <ol> <li> <p>Dimension \\    Dimensions are the assortment of information that contain data around one or    more business measurements. It may be topographical information, item data,    contacts, and so on. In simple terms, they give who, what, where, and the context    to the fact creation. \\    In other words, a dimension is a window to view information in the facts.</p> </li> <li> <p>Facts/Measures \\    Facts are the collection of measurements, metrics, transactions, etc.,    from different business processes and are almost always numeric.    It typically contains business transactions and measure values.    A single fact table row has a one-to-one relationship to a    measurement event as described by the fact table\u2019s grain. Thus, a fact table    corresponds to a physical observable event, and not to the demands of a particular report.</p> </li> </ol> <p>Within a fact table, only facts consistent with the declared grain are allowed.    For example, in a retail sales transaction, the quantity of a product sold and    its extended price are good facts, whereas the store manager\u2019s salary is disallowed.</p> <ol> <li> <p>Attributes \\    Attributes are the elements of the Dimension Table. For example, in an account Dimension,    the attributes can be:</p> </li> <li> <p>First Name</p> </li> <li>Last Name</li> <li>Phone, etc.</li> </ol> <p>Attributes are used to search, filter, or classify facts.    Dimension Tables contain Attributes</p> <ol> <li>Fact Tables \\    Fact tables are utilized to store measures or transactions in the business.    The Fact Tables are related to Dimension Tables with the keys known as foreign keys. \\    For example, in Internet business, a Fact can store the requested amount of items.    Fact Tables, as a rule, have huge rows and fewer columns.</li> </ol> <p>Note: \\ All fact tables have foreign keys that refers to the dimension tables primary keys to easily connect them to produce specific measure.</p> <ol> <li>Dimension Tables \\    Dimension Tables store the Dimensions from the business and establish the context    for the Facts (They are joined to fact table via a foreign key).    They contain descriptive data that is linked to the Fact Table.    Dimension Tables are usually optimized tables and hence contain large columns    and fewer rows.</li> </ol> <p>For example:</p> <ul> <li>Contact information can be viewed by name, address and phone dimension.</li> <li>Product information can be viewed by product-code, brand, color, etc.</li> <li>City, state, etc. can view store information.</li> </ul> <p>Note: \\ No set limit set for given for number of dimensions \\ The dimension can also contain one or more hierarchical relationships</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#types-of-dimensions-in-dimensional-data-modelling","title":"Types of Dimensions in Dimensional Data Modelling","text":"<p>There are 9 types of Dimensions/metrics when dealing with Dimensional Data Modelling.</p> <ol> <li> <p>Conformed Dimension \\    A Conformed Dimension is a type of Dimension that has the same meaning to all    the Facts it relates to. This type of Dimension allows both Dimensions and    Facts to be categorised across the Data Warehouse.</p> </li> <li> <p>Outrigger Dimension \\    An Outrigger Dimension is a type of Dimension that represents a connection between    different Dimension Tables.</p> </li> <li> <p>Shrunken Dimension \\    A Shrunken Dimension is a perfect subset of a more general data entity.    In this Dimension, the attributes that are common to both the subset and the    general set are represented in the same manner.</p> </li> <li> <p>Role-Playing Dimension \\    A Role-Playing Dimension is a type of table that has multiple valid relationships    between itself and various other tables. Common examples of Role-Playing Dimensions    are time and customers. They can be utilised in areas where certain Facts do not    share the same concepts.</p> </li> <li> <p>Dimension to Dimension Table \\    This type of table is a table in the Star Schema of a Data Warehouse. In a Star Schema,    one Fact Table is surrounded by multiple Dimension Tables. Each Dimension corresponds    to a single Dimension Table.</p> </li> <li> <p>Junk Dimension \\    A Junk Dimension is a type of Dimension that is used to combine 2 or more related    low cardinality Facts into one Dimension. They are also used to reduce    the Dimensions of Dimension Tables and the columns from Fact Tables.</p> </li> <li> <p>Degenerate Dimension \\    A Degenerate Dimension is also known as a Fact Dimension. They are standard    Dimensions that are built from the attribute columns of Fact Tables.    Sometimes data are stored in Fact Tables to avoid duplication.</p> </li> <li> <p>Swappable Dimension \\    A Swappable Dimension is a type of Dimension that has multiple similar versions    of itself which can get swapped at query time. The structure of this Dimension    is also different, and it has fewer data when compared to the original Dimension.    The input and output are also different for this Dimension.</p> </li> <li> <p>Step Dimension \\    This is a type of Dimension that explains where a particular step fits into the process.    Each step is assigned a step number and how many steps are required by that    step to complete the process.</p> </li> </ol> <p>To explore about the types of Dimensions in detail, click this link.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#types-of-dimension-tables","title":"Types of Dimension Tables","text":""},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#steps-to-carry-out-dimensional-data-modelling","title":"Steps to Carry Out Dimensional Data Modelling","text":"<p>Dimensional Data Modelling requires certain analysis on the data to understand data behaviour and domain. The main goal a Dimensional Data Model tries to address is that it tries to describe the Why, How, When, What, Who, Where of the business process in detail.</p> <pre><code>Select the business process (WHY)\n|\n|---&gt; Declare the Grain (HOW MUCH)\n      |\n      |---&gt; Identify the Dimension (3WS)\n            |\n            |---&gt; Identity (WHAT)\n                  |\n                  |---&gt; Build the Schema\n</code></pre> <p>The major steps to start Dimensional Data Modelling are:</p> <ol> <li>Identify the Business Process \\    A Business Process is a very important aspect when dealing with Dimensional Data Modelling.    The business process helps to identify what sort of Dimension and Facts are needed    and maintain the quality of data.</li> </ol> <p>To describe business processes, you can use    Business Process Modelling Notation (BPMN) or    Unified Modelling Language (UML).</p> <ol> <li>Identify Grain \\    Identification of Grain is the process of identifying how much normalisation    (the lowest level of information) can be achieved within the data for any    table in your data warehouse.</li> </ol> <p>It is the stage to decide the incoming frequency of data (i.e.daily, weekly, monthly, yearly),    how much data we want to store in the database (one day, one month, one year, ten years),    and how much the storage will cost.</p> <p>During this stage, you answer questions like</p> <ul> <li> <p>Do we need to store all the available products or just a few types of products? \\      This decision is based on the business processes selected for Data Warehouse</p> </li> <li> <p>Do we store the product sale information on a yearly, monthly, weekly, daily or hourly basis? \\      This decision depends on the nature of reports requested by executives</p> </li> <li> <p>How do the above two choices affect the database size?</p> </li> </ul> <p>Note: \\ Grain is the level of detail or the depth of the information that\u2019s stored in the data warehouse and answer this type of questions:</p> <ul> <li>Do we store all products or specific categories?</li> <li>We will use data from week or month or year?</li> <li>We will hold sales per day or per product or per store?</li> </ul> <p>Only facts consistent with the declared grain are allowed.</p> <ol> <li> <p>Identify the Dimensions \\    Dimensions are the key components in the Dimensional Data Modelling process.    It contains detailed information about the objects like date, store, name,    address, contacts, etc. For example, in an E-Commerce use case,    a Dimension can be:</p> </li> <li> <p>Product</p> </li> <li>Order Details</li> <li>Order Items</li> <li>Departments</li> <li>Customers (etc).</li> </ol> <p>The Dimensional Model for a customer conducting an E-Commerce transaction is shown below:</p> <pre><code>Customer Dimension  -----&gt; table name\n---\nCustomer ID         -----&gt; primary key\nCustomer Name\nGender\nAge\nAddress\nCity                --|\nState                 |--&gt; hierarchy level for location\nCountry             --|\nAnnual income\nPhone number\nPurchased day       --|\nPurchased month       |--&gt; hierarchy level for date\nPurchased quarter     |\nPurchased year      --|\n</code></pre> <ol> <li>Identify the Facts \\    Once the Dimensions are created, the measures/transactions are supposed to be linked    with the associated Dimensions. The Fact Tables hold measures and are linked to Dimensions    via foreign keys. Usually, Facts contain fewer columns and huge rows.</li> </ol> <p>For example, in an E-Commerce use case, one of the Fact Tables can be of orders,    which holds the products\u2019 daily ordered quantity. Facts may contain more than    one foreign key to build relationships with different Dimensions.</p> <p>This step is co-associated with the business users of the system because this    is where they get access to data stored in the data warehouse.    Most of the fact table rows are numerical values like price or cost per unit, etc.</p> <ol> <li>Build the Schema \\    The next step is to tie Dimensions and Facts into the Schema. Schemas are the table structure,    and they align the tables within the database.</li> </ol> <p>There are 2 popular types of Schemas:</p> <ul> <li> <p>Star Schema: \\       The Star Schema is the Schema with the simplest structure and easy to design.      In a Star Schema, the Fact Table surrounds a series of Dimensions Tables.      Each Dimension represents one Dimension Table. These Dimension Tables are not      fully normalised (The fact tables in a star schema which is third normal form      whereas dimensional tables are de-normalized).</p> <p>In this Schema, the Dimension Tables will contain a set of  attributes that describes the Dimension. They also contain foreign keys that  are joined with the Fact Table to obtain results.</p> <pre><code>erDiagram\n   DIM_DATE ||--o{ FCT_SALES : is\n   DIM_DATE {\n      string timestamp_id PK\n      string date\n      string month\n      string year\n   }\n   DIM_CUSTOMER ||--o{ FCT_SALES : is\n   DIM_CUSTOMER {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   FCT_SALES {\n      string sales_key PK\n      string date_key FK\n      string customer_key FK\n      string store_key FK\n      numeric sales_amount\n   }\n   DIM_STORE ||--o{ FCT_SALES : is\n   DIM_STORE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_MOVIE ||--o{ FCT_SALES : is\n   DIM_MOVIE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }</code></pre> </li> <li> <p>Snowflake Schema: \\       A Snowflake Schema is the extension of a Star Schema, and includes more Dimensions.      Unlike a Star Schema, the Dimensions are fully normalised and are split down      into further tables.      This Schema uses less disk space because they are already normalised.      It is easy to add Dimensions to this Schema and the data redundancy      is also less because of the intricate Schema design.</p> <pre><code>erDiagram\n   DIM_DATE ||--o{ FCT_SALES : is\n   DIM_DATE {\n      string timestamp_id PK\n      string date\n      string month\n      string year\n   }\n   DIM_CUSTOMER ||--o{ FCT_SALES : is\n   DIM_CUSTOMER {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   FCT_SALES {\n      string sales_key PK\n      string date_key FK\n      string customer_key FK\n      string store_key FK\n      numeric sales_amount\n   }\n   DIM_STORE ||--o{ FCT_SALES : is\n   DIM_STORE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_MOVIE ||--o{ FCT_SALES : is\n   DIM_MOVIE {\n      string customer_id PK\n      string firstname\n      string lastname\n      int age\n   }\n   DIM_COUNTRY ||--o{ DIM_STORE : in</code></pre> </li> </ul> Star Schema Snowflake Schema Hierarchies for the dimensions are stored in the dimensional table. Hierarchies are divided into separate tables. It contains a fact table surrounded by dimension tables. One fact table surrounded by dimension table which are in turn surrounded by dimension table In a star schema, only single join creates the relationship between the fact table and any dimension tables. A snowflake schema requires many joins to fetch the data. Simple DB Design. Very Complex DB Design. Denormalized Data structure and query also run faster. Normalized Data Structure. High level of Data redundancy Very low-level data redundancy Single Dimension table contains aggregated data. Data Split into different Dimension Tables. Cube processing is faster. Cube processing might be slow because of the complex join. Offers higher performing queries using Star Join Query Optimization. Tables may be connected with multiple dimensions. The Snowflake schema is represented by centralized fact table which unlikely connected with multiple dimensions. <p>More Detail: Star and Snowflake Schema in Data Warehouse with Model Examples</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#types-of-fact-tables","title":"Types of Fact Tables","text":"<p>There are three main types of fact tables:</p> <ol> <li>Transaction \\    The transaction fact table is the most basic and the simplest type, In the Transaction    fact table, every row corresponding to a measurement event at a point in space    and time which means a grain set at a single transaction. The row is added only    if there\u2019s a transaction is happened by a customer or for a product.</li> </ol> <p>Note: Row in Transaction fact table represents transaction, and it dimensions.</p> <p>For example, \\    If we have a retail store, on Sunday we sold 40 items, and on Monday we sold 15 items,    So on Sunday there are 40 transaction rows had been added to our fact table and on Monday    there are 15 transaction rows had been added to our table, There are no aggregate values,    we are storing the transaction's data.</p> <p>Because of low granularity, we can monitor detailed business processes.    But because we have rows for each transaction that happened, It causes performance    issues due to the large size of data.</p> <ol> <li>Periodic Snapshot \\    In the periodic table, We have lower granularity which means that the row in    the periodic table is a summarization of data over a period of time (day, week, month, etc).    Our grain here is periodic data summarization, not single transactions.    It helps to review the aggregate performance of the business process at intervals    of time.</li> </ol> <p>Then those unavailable measurements can be kept empty (Null) or can be filled    up with the last available measurements.</p> <p>For example, \\    If we want to know the quantity that been sold from specific product through    the last week. Our grain is a week.</p> <p>Because of summarized data, our performance now is better than the transaction    fact table. But now we have higher grain, So we lost the detailed business    processes that we had in the transaction fact table.</p> <p>Note: Rows in Periodic table represent performance of an activity at the end of specific period.</p> <ol> <li>Accumulated Snapshot \\    In Accumulating snapshot fact table, Row represents an Entire process,    which means that our row corresponding to measurements that occurred at defined    steps between the beginning and end of the process. we use it when users need    to perform pipeline and workflow analysis like Order fulfillment.</li> </ol> <p>Order fulfillment covers the complete process from when a sale process takes    place all the way through delivery to the customer. So here we have multiple    activities in the process, First when we receive an order from a customer,    then send the order to the inventory to organize it, then move the order to    the shipment activity, and finally, the customer receives his order.</p> <p>Note:</p> <ul> <li>Row in Accumulating snapshot represents an Entire process</li> <li>There is a date foreign key in the fact table for each critical activity in the process.</li> </ul> <p>Accumulating Snapshot fact table helps us in complex analysis, workflow, and    pipeline process, on another hand It needs high ETL process complexity.</p> Transaction Periodic Accumulating Row Transaction and It dimensions. summarized data over a period of time. Entire process activities. Granularity Lowest granularity 1 row / Transaction Higher than transaction.1 row / period Highest granularity 1 Row / Entire process Table Size Largest Smaller than Transaction Smallest Example Sales amount of products on a daily basis. Total sales amount of product through May. Order fulfillment <p>Fact-less Fact Table: \\ Fact-less facts are fact tables that haven\u2019t any measures, It only has a foreign key for each dimension. We can say that Fact-less fact is only an intersection of Dimensions Or A bridge between dimension keys.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#rules-for-dimensional-modelling","title":"Rules for Dimensional Modelling","text":"<p>Following are the rules and principles of Dimensional Modeling:</p> <ul> <li> <p>Load atomic data into dimensional structures.</p> </li> <li> <p>Build dimensional models around business processes.</p> </li> <li> <p>Need to ensure that every fact table has an associated date dimension table.</p> </li> <li> <p>Ensure that all facts in a single fact table are at the same grain or level of detail.</p> </li> <li> <p>It\u2019s essential to store report labels and filter domain values in dimension tables</p> </li> <li> <p>Need to ensure that dimension tables use a surrogate key</p> </li> <li> <p>Continuously balance requirements and realities to deliver business solution to   support their decision-making</p> </li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#benefits-of-dimensional-data-modelling","title":"Benefits of Dimensional Data Modelling","text":"<p>As now, you understand the process of Dimensional Data Modelling, you can imagine why it is so important and how many benefits DDM provides for the company. Some of those benefits are given below:</p> <ul> <li> <p>The Dimension Table stores the history information and a standard Dimension Table   holds good quality data and allows easy access across the business.</p> </li> <li> <p>You can introduce new Dimensions without affecting other Dimensions and Facts in the Schema.</p> </li> <li> <p>Dimension and Fact Tables are easier to read and understand as compared to a normal table.</p> </li> <li> <p>Dimensional Models are built based on business terms, and hence it is quite   understandable by the business.</p> </li> <li> <p>Dimensional Data Modelling in a Data Warehouse creates a Schema which is optimised   for high performance. It means fewer joins between tables, and it also helps with   minimised data redundancy.</p> </li> <li> <p>The Dimensional Data Model also helps to boost query performance. It is more denormalized;   therefore, it is optimized for querying.</p> </li> <li> <p>Dimensional Data Models can comfortably accommodate the change. Dimension Tables   can have more columns added to them without affecting existing Business Intelligence   applications using these tables.</p> </li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#limitations-of-dimensional-data-modelling","title":"Limitations of Dimensional Data Modelling","text":"<p>Although Dimensional Data Modelling is very crucial to any organisation, it has a few limitations that companies need to take care of when incorporating the concept into their applications. Some of those limitations are given below:</p> <ul> <li> <p>Designing and creating Schemas require domain knowledge about the data.</p> </li> <li> <p>To maintain the integrity of Facts and Dimensions, loading the Data Warehouses   with a record from various operational systems is complicated.</p> </li> <li> <p>It is severe to modify the Data Warehouse operations if the organisation adopts   the Dimensional technique and changes the method in which they do business.</p> </li> </ul> <p>Despite these limitations, the DDM technique has proved to be one of the simplest and most efficient techniques to handle data in Data Warehouses to date.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#advantages","title":"Advantages","text":"<p>Kimball's architecture has several advantages.</p> <ul> <li>Simplicity and speed: Kimball's architecture is significantly easier and faster to construct and establish.</li> <li>Understandable: Non-technical and technical staff both may understand the dimensional data model.</li> <li>Relevancy: Kimball's bottom-up methodology, unlike Inmon's, makes all data linkages relevant to the business needs.</li> <li>Engineering team needs: In comparison to Inmon's technique, Kimball requires fewer engineers with less specific technical abilities to set up and operate the data warehouse.</li> </ul> <p>Good fit for:</p> <ul> <li>Medium-to-large number of data sources</li> <li>Centralized data teams</li> <li>End-use case for data is primarily around business intelligence and providing insights</li> <li>Teams that want to create an easily navigable and predictable data warehouse design</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#disadvantages","title":"Disadvantages","text":"<p>Kimball's architecture has some drawbacks.</p> <ul> <li>Data redundancy: There is more data redundancy and hence a higher likelihood of errors since data is fed into a dimensional model.</li> <li>No single source of truth: Data marts are used to design and organise data in the data warehouse. When combined with data redundancy, Kimball's architecture prevents the company from having a single source of truth.</li> <li>Less adaptable: Kimball's architecture is less flexible and adaptable to modifications when data demands change, business requirements vary, and incoming data sources alter their payloads.</li> <li>Incomplete: The strategy taken begins (and concludes) with important business processes. As a result, it does not provide a complete 360-degree picture of business data. Instead, it helps report on particular subject areas in the corporate world.</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#what-is-multi-dimensional-data-model-in-data-warehouse","title":"What is Multi-Dimensional Data Model in Data Warehouse?","text":"<p>Multidimensional data model in data warehouse is a model which represents data in the form of data cubes. It allows to model and view the data in multiple dimensions, and it is defined by dimensions and facts.</p> <p>Multidimensional data model is generally categorized around a central theme and represented by a fact table.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#conclusion","title":"Conclusion","text":"<p>This article gave an in-depth knowledge about Dimensional Data Modelling, its types, features, components and also the steps required for any company to set up a DDM approach. It also gave a brief understanding of the benefits and limitations of the DDM approach. Overall, adopting any new approach can be a tedious task for any company, but, by having systematic techniques put in place the company can monitor those parameters carefully and also optimise the performance.</p> <ul> <li> <p>A dimensional model is a data structure technique optimized for   Data warehousing tools.</p> </li> <li> <p>A fact table is a primary table in a dimensional model. \\    There are 4 types of facts/measures:</p> </li> <li> <p>Additive: \\      Business measures that can be aggregated across all dimensions</p> </li> <li> <p>Non-additive: \\      Business measures that can be aggregated across some dimensions and not across      others (usually date and time dimensions).\\      For example, <code>Items Inventory</code> (can be summed through product, But it can\u2019t be summed through date)</p> </li> <li> <p>Semi-additive: \\      Business measures that cannot be aggregated across any dimension.\\      For example, <code>Sales Tax</code>, or <code>Unit Price</code></p> </li> <li> <p>Fact-less</p> </li> </ul> <p>The most common form of dimensional modeling is the star schema. A star schema is a multidimensional data model used to organize data so that it is easy to understand and analyze, and very easy and intuitive to run reports on. Kimball-style star schemas or dimensional models are pretty much the gold standard for the presentation layer in data warehouses and data marts, and even semantic and reporting layers. The star schema design is optimized for querying large data sets.</p>"},{"location":"abstract/data_management/data_modeling/dwh-kimball-approach/#reference","title":"Reference","text":"<ul> <li>https://hevodata.com/learn/dimensional-data-modelling</li> <li>https://www.guru99.com/dimensional-model-data-warehouse.html</li> <li>https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/</li> <li>https://datavalley.technology/dimensional-modeling-part-1-introduction-and-fact-types/</li> <li>https://dwgeek.com/types-of-dimension-tables-data-warehouse.html/</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/","title":"One Big Table (OBT) Approach","text":"<p>In the dynamic landscape of data warehousing, the methodologies employed for structuring data play a pivotal role in unlocking timely insights for informed decision-making. Since the foundational theories of the 1990s and 2000s by Inmon (Inmon W.H., Building the Data Warehouse, 1990), Kimball (Kimball R, The Data Warehouse Toolkit, 1996), and later Linstedt (Linstedt D, Data Vault Series 1 \u2014 Data Vault Overview, 2002), various data modeling techniques for traditional data warehousing have evolved and debated. This blog post delves into two data modeling techniques: Dimensional Modeling and One Big Table (OBT).</p> <p>Through this blogpost, we will explore the strengths and challenges of these two techniques and discuss the best practices for implementing them on Databricks.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#dimensional-modeling-essential-then-optional-now-understanding-the-shift","title":"Dimensional Modeling: essential then, optional now \u2014 understanding the shift","text":"<p>Designing a dimensional data model requires a bottom-up understanding of the business requirements and high-level profiling of data sources. The most common implementation of Dimensional Modeling is Star Schema, which has become widely adopted as the presentation layer in most data warehouses in the past decades. This method denormalizes the data into measurable business process events called Facts and the contextual details surrounding the business process events called Dimensions.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#the-rise-of-dimensional-modeling-why-it-became-the-gold-standard","title":"The rise of Dimensional Modeling: why it became the gold standard","text":"<p>Dimensional Modeling was introduced to optimize the data model for analytics in the logical layer of the Relational Databases Management Systems (RDBMS) without redesigning the physical layer. The logical and physical layers of the RDBMS were purposefully designed for Online Transaction Processing (OLTP), facilitating efficient row-oriented data entry, and used primarily for transactional purposes, maintaining ACID (Atomicity, Consistency, Isolation, Durability) properties in normalized data. On the other hand, Dimensional Modeling optimizes the logical layer for Online analytical processing (OLAP) to process and aggregate historical data with better performance. Dimensional Modeling in the logical layer offered various benefits, such as:</p> <ul> <li>Simplified Data Usability: Dimensional Modeling simplifies data understanding by structuring it into fact and dimensional tables, making it intuitive for end-users to map real-world processes to the data model. Dimensional Modeling supports efficient data aggregation, which can serve the Business Intelligence (BI) tools as the semantic layer.</li> <li>Query Performance: The primary advantage of Dimensional Modeling lies in its ability to optimize for query performance without compromising the depth of historical analysis. The Star Schema, a common implementation of Dimensional Modeling, optimizes analytics by denormalizing data into business-grained facts and dimensions, improving query performance and data aggregation.</li> <li>Scalability and Consistency: Dimensional Models excel at scalability, accommodating growing data volumes and changing business requirements. The Star Schema can adapt changes in the dimensions and facts, facilitate Slowly Changing Dimensions and integration of incremental data. There are further approaches to reducing the redundancy in the data, such as implementing a Snowflake Schema or using Surrogate Keys in the dimension tables.</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#dimensional-modeling-under-scrutiny-navigating-new-technological-landscapes","title":"Dimensional Modeling under scrutiny: navigating new technological landscapes","text":"<p>Dimensional Modeling achieved a level of optimization on the logical layer effectively balancing redundancy and query performance, which was crucial when storage and computing resources were costly and limited row-oriented databases were insufficient for analytical processing. The advancement in Data technologies has brought to light certain considerations associated with traditional Dimensional Modeling.</p> <ul> <li>Operational and Design Limitations: Dimensional Modeling requires a substantial initial investment in schema design, as well as ongoing maintenance of data pipelines, typically managed through ETL tools. Beyond operational overhead, Dimensional Modeling faces inherent design limitations, such as the complexity of managing Slowly Changing Dimensions and Fact-to-Fact joins. In the past, these challenges were considered worthwhile trade-offs for the purpose of improved query performance. However, with the advancements in modern data technologies, these issues can often be circumvented, reducing the necessity of complex schema designs and operational overhead.</li> <li>Technological Evolution in Data Storage and Processing: Modern data warehousing allows for flexibility and scalability by decoupling storage and processing. These technologies leverage Massive Parallel Processing (MPP) and physical columnar storage, which inherently optimize historical data aggregation and analytics by default. Also, the drawbacks of denormalization are mitigated since the storage cost has significantly reduced over time. Advanced optimization technologies, e.g., data compression on the storage layer and clustering, outweigh the complexity of keeping the normalized data.</li> </ul>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#dimensional-modeling-best-practices-on-databricks","title":"Dimensional Modeling best practices on Databricks","text":"<p>Considering the advanced Data Warehousing capabilities of Databricks, adhering strictly to Dimensional Modeling e.g. Star Schema is not a necessity anymore; however possible and very well supported. We\u2019ll review various Databricks technologies here that make it possible to implement and optimize the Dimensional Modeling technique.</p> <ul> <li>ACID properties: Databricks Delta Lake supports ACID transactions on Delta Tables, simplifying the maintenance and quality of the Dimensional Models.</li> <li>Data layers: The Star Schema can be deployed into a Gold Layer of the medallion architecture in Databricks to power analytics and fast decision-making.</li> <li>ELT pipelines: The pipeline to transform the transactional data into dimensional data models is supported by Databricks SQL and Delta Live Tables (DLT).</li> <li>Relational constraints: Unlike usual data lakes, Databricks supports schema enhancement such as relational constraints e.g. Primary Keys, Foreign Keys, and Identity Columns in Databricks SQL as Surrogate Keys and enforced CHECK constraints for data quality. The physical and virtual constraints can exist as meta-objects in the Unity Catalog.</li> <li>Unified governance: All data models, Dimension tables, Fact tables, and their relations are registered centrally into the Unity Catalog. With Unity Catalog as the unified governance layer, Dimensions and Facts can be discovered and shared across organizations without the need to duplicate it.</li> <li>Optimization: Databricks supports Liquid Clustering, which incrementally optimizes the data layout without rewriting the data, which can be applied to fact tables as well as dimension tables.</li> </ul> <p>In this blogpost, you can follow a simple 5-step guideline to implement Dimensional Modeling on Databricks.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#embracing-simplicity-the-one-big-table-obt-approach-as-a-modern-alternative","title":"Embracing simplicity: The One Big Table (OBT) approach as a modern alternative","text":"<p>One Big Table emerges as another alternative concept to Dimensional Modeling. This method, characterized by its simplicity, involves storing data in a single, expansive table, offering a much-simplified data model by significantly reducing the number of tables that need to be governed and updated.</p> <p>Reconsidering OBT: unveiling the benefits</p> <ul> <li>Simplicity: In contrast to designing a Dimensional Model, setting up One Big Table is quick and easy. It facilitates flexibility for swift integration and modification of data while simplifying workflows with data at high velocities. As complex joins are usually not required, finding, querying, and exploring a single wide big table is also straightforward, increasing the discoverability and readability of data assets.</li> <li>Data Consistency &amp; Governance: Since all data is stored and updated in One Big Table, the risks of data duplication and data inconsistencies are reduced and can be easily mitigated and monitored with quality constraints applied to a single table. By reducing the amount of tables to govern, One Big Table can also significantly decrease administrative workloads.</li> <li>Performance: Although the OBT approach originally required scanning and filtering of an entire large table, it avoids complex joins and transformations, reducing the amount of data shuffling, and often delivering faster query results compared to Dimensional Modeling. This benefit is additionally capitalized by a much simpler reduction of data skews.</li> </ul> <p>Addressing the challenges of OBT</p> <ul> <li>Performance: Despite previously highlighted performance benefits of the One Big Table approach by redundancy of joins and shuffles, as tables grow in size, they can introduce performance bottlenecks for queries where the entire table needs to be scanned. This has historically been the most prominent challenge of the One Big Table concept, which can be particularly challenging for serving the BI tools. With columnar storage in Delta, OBT can lead to high data compression for the table, making storage of repeated values less of a concern compared to classical row-based databases. However, since it is indeed One Big Table, if you need to prune files by more than one to two dimensions for ALL attributes you need in the table, OBT can quickly become inefficient. If you have five relational tables, each with one or two clustered columns, you can efficiently prune files using up to ten dimensions. If you have an OBT model, you can only use two to three dimensions before clustering becomes useless.</li> <li>Data exploration: One Big Table lacks the structure inherent to Dimensional Modeling by mapping specific business requirements to data architecture. Although this can also pose a benefit, as tables grow, particularly in width, navigating and exploring data in One Big Table can become complicated, particularly using BI tools. Moreover, this approach can lead to duplicated analysis and results in writing numerous window functions instead of joins.</li> <li>Data privacy: Storing all data in One Big Table concentrates a lot of data, including sensitive information, in one place. As the number of users querying data from a single big table is larger than in Dimensional Models, the concept requires fine-grained access controls, data masking, and anonymization, which can become complex to manage for very wide tables.</li> <li>Data Quality: The level of data quality and observability can seriously influence the decision to utilize OBT. If you need to implement data quality checks beyond your filtering/clustered columns, and if your data quality rules / checks need to be measured across multiple rows, this can get expensive and complex fast. Overall for OBT, keep data quality simple: check quality statelessly (data quality checks across rows at a point in time is stateful) or keep the data quality rules to a window that utilizes the clustering columns. Try to measure / enforce data quality within the write-side of the pipeline, especially since OBT models tend to be much larger.</li> </ul> <p>Overall, OneBigTable can be incredibly useful for certain use cases, but should be used with caution. The OBT model works well for use cases where you only need to filter the table on 1\u20133 dimensions, and the rest of your analytics / apps are built on those filters. Real world examples include IoT data, logging systems, or single use case / data view applications. In IoT data, you usually only ever need to filter the data by an event timestamp, sensor type, and then the rest of the downstream analytics are built on that. This is a perfect example of a string use case for OBT because you can have 100s/1000s of attributes that a single sensor row carries, but you only ever need to filter by the initial timestamp/sensor type columns. In the Sensor IoT example, modeling the data for 100s+ number of data attributes is not worth the lift, and the OBT model is built perfectly for this.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#obt-best-practices-on-databricks","title":"OBT best practices on Databricks","text":"<p>Let\u2019s create OBT from a dimensional Databricks sample dataset using a Databricks SQL Serverless (XS) warehouse and explore the benefits and challenges of OBT and how we can address them in Databricks.</p> <pre><code>CREATE TABLE tpch_obt AS\nSELECT\n  *\nFROM\n  samples.tpch.customer\n  JOIN saples.tpch.orders ON c_custkey == o_custkey\n</code></pre> <p>The resulting Delta table comprises 730 MB stored across 7 files.</p> <p>Let\u2019s analyze the average order price for the AUTOMOBILE market segment:</p> <pre><code>SELECT\n MEAN(o_totalprice)\nFROM\n tpch_obt\nWHERE\n c_mktsegment == \"AUTOMOBILE\"\n</code></pre> <p>The wall-clock duration time on a Databricks SQL Serverless XS Warehouse was 3.5 seconds.</p> <p>Looking at the query profile in the Query History provides further insights:</p> <p></p> <p>Note that 7 files were scanned and the total task time was 9.7 seconds.</p> <p>Querying the Dimensional Model on the same Databricks SQL Serverless XS warehouse with a slightly faster wall-clock duration of 2.6 seconds looks like the following (if you want to reproduce the performance test, make sure to restart your SQL Serverless Warehouse after the OBT creation to avoid CACHING):</p> <pre><code>SELECT\n MEAN(a.o_totalprice)\nFROM\n samples.tpch.orders a JOIN samples.tpch.customer b ON a.o_custkey == b.c_custkey\nWHERE\n b.c_mktsegment == \"AUTOMOBILE\"\n</code></pre> <p>Looking at the query history reveals further details:</p> <p></p> <p>Despite the latter query requiring a join, it\u2019s faster as fewer files have to be scanned.</p> <p>When executing both queries again, one can capitalize on Databricks SQL automatic caching capabilities (see this blog post), effectively reducing wall-clock duration to &lt;500ms for both queries.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#optimize-query-performance-using-liquid-clustering","title":"Optimize query performance using Liquid Clustering","text":"<p>To optimize the query performance for the OBT beyond Caching, Databricks Liquid Clustering offers an easy and automated approach to simplify data layout, resulting in largely optimized query performance.</p> <pre><code>ALTER TABLE tpch_obt CLUSTER BY (c_mktsegment);\nOPTIMIZE tpch_obt;\n</code></pre> <p>Subsequently querying the table (same statement as before) results in &gt;20x task speed up and &gt;3x wall-clock duration, bringing the latter down to 1.13 seconds.</p> <p>The optimized data layout by liquid clustering effectively reduced the number of files to read from 7 to 2.</p> <p></p> <p>As datasets increase in size, this effect becomes even more prominent in the absolute speedup time.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#data-privacy-with-row-filters-and-column-masks","title":"Data Privacy with Row Filters and Column Masks","text":""},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#row-filters","title":"Row filters","text":"<p>Row filters allow you to apply a filter to a table so that subsequent queries only return rows for which the filter predicate evaluates to true. This facilitates overcoming the privacy challenges of OBT. A row filter is implemented as an SQL user-defined function (UDF).</p> <p>Create the row filter function:</p> <pre><code>CREATE FUNCTION mktsegment(c_mktsegment STRING)\nRETURN IF(IS_ACCOUNT_GROUP_MEMBER('pl_obt'), true, c_mktsegment = 'HOUSEHOLD');\n</code></pre> <p>Apply the row filter to a table:</p> <pre><code>ALTER TABLE tpch_obt SET ROW FILTER mktsegment ON (c_mktsegment);\n</code></pre> <p>Subsequent queries of users who are members of the group pl_obt will return only rows where the <code>c_mktsegment</code> column equals <code>'HOUSEHOLD'</code>.</p>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#column-masks","title":"Column masks","text":"<p>Column masks let you apply a masking function to a table column. The masking function gets evaluated at query runtime, substituting each reference of the target column with the results of the masking function. In the following example, we create a user-defined function that masks the total_price column so that only users who are members of the pl_obt group can view values in that column.</p> <p>Create the column masking function:</p> <pre><code>CREATE FUNCTION price_mask(o_totalprice DECIMAL)\n    RETURN CASE WHEN is_member('pl_obt')\n    THEN o_totalprice ELSE '***-**-****' END;\n</code></pre> <p>Apply the column masking function to a table:</p> <pre><code>ALTER TABLE tpch_obt ALTER COLUMN o_totalprice SET MASK price_mask;\n</code></pre> <p>Subsequent queries of users who are not members of the <code>pl_obt</code> group will result in masked values:</p> <pre><code>SELECT o_totalprice FROM tpch_obt LIMIT 10;\n</code></pre>"},{"location":"abstract/data_management/data_modeling/dwh-obt-approach/#references","title":"References","text":"<ul> <li>https://medium.com/@hubert.dulay/one-big-table-obt-vs-star-schema-a9f72530d4a3</li> <li>https://medium.com/dbsql-sme-engineering/one-big-table-vs-dimensional-modeling-on-databricks-sql-755fc3ef5dfd</li> </ul>"},{"location":"abstract/data_mesh/","title":"Data Mesh","text":"<p>Data Mesh is a modern architectural approach in data management and analytics. It shifts away from traditional centralized data management models (like data warehouses and lakes) and advocates for a decentralized approach.</p> <p>Here are the key characteristics and principles of Data Mesh:</p> <ul> <li> <p>Domain-Oriented Decentralized Data Ownership and Architecture</p> <p>Data Mesh emphasizes that data should be managed and owned by domain-specific teams (e.g., sales, marketing, logistics) rather than a centralized data team. This approach allows each team to control and optimize their data based on their specific needs and expertise.</p> </li> <li> <p>Data as a Product</p> <p>Data is treated as a product, with each domain team responsible for the lifecycle of the data products they own. This includes ensuring data quality, reliability, and usability. Data products are built to be discoverable, understandable, trustworthy, and usable by other teams.</p> <p>Read more about Data as a Product</p> </li> <li> <p>Self-Serve Data Infrastructure as a Platform</p> <p>To enable domain teams to manage their data products effectively, a self-serve data platform is provided. This platform offers tools and capabilities for data storage, processing, and analytics, ensuring teams can access and use data with autonomy but without needing to manage complex data infrastructure.</p> </li> <li> <p>Federated Computational Governance</p> <p>Data Mesh also involves a federated approach to governance. While teams have autonomy over their data, there are overarching guidelines and policies to ensure compliance, security, and interoperability across the organization.</p> </li> </ul>"},{"location":"abstract/data_mesh/#noted","title":"Noted","text":"<ul> <li> Data Mesh in Practice</li> <li> Navigating Your Data Platform\u2019s Growing Pains: A Path from Data Mess to Data Mesh</li> <li> How Data Mesh Architecture changed our Engineering Teams</li> <li> Challenges and Solutions in Data Mesh \u2013 Part 1</li> </ul>"},{"location":"abstract/data_mesh/#questions","title":"Questions","text":"<ul> <li>Is data mesh only for analytical data?</li> </ul>"},{"location":"abstract/data_mesh/#read-mores","title":"Read Mores","text":"<ul> <li>Data Mesh: A modern architectural approach</li> </ul>"},{"location":"abstract/data_mesh/data-as-a-product/","title":"Data as a Product","text":"<p>Quote</p> <p>\"Data product\" is a generic concept and \"data as a product\" is a subset of all possible data products.</p> <p>\"Data as a product\", by contrast, is a mindset or approach that applies product-like thinking to a dataset. In other words, it ensures that a dataset has all the properties of discoverability, accessibility, self-description, and so on. Furthermore, it fosters thinking about \"data product releases\" much like how software developers approach software releases - i.e., as discrete, shipped products with distinct versions.<sup>1</sup></p>"},{"location":"abstract/data_mesh/data-as-a-product/#data-as-a-product-vs-data-product","title":"Data as a Product vs. Data Product","text":"<p>Here is a list of example data products including the category they belong to and the interfaces used to access it:</p> <ul> <li>A company dashboard to visualise the main KPIs of your business. This data   product is of the type decision support system and the interface to access it   is a visualisation.</li> <li>A data warehouse. This data product is a mix of raw, derived data and decision   support system. The interface to access it are probably SQL queries.</li> <li>A list of recommended restaurants nearby. Since the list is curated specifically   for you, this data product is an automated decision-making one. The interface   to access it is an app or website.</li> <li>A \"faster route now available\" notification on Google Maps is a decision   support data product (as you are the one making the decision) and its interface   is a web/app.</li> <li>A self-driving car is a data product too. Since it drives automatically,   it is of the type automated decision-making. Its interface is, well, the car   itself.</li> </ul>"},{"location":"abstract/data_mesh/data-as-a-product/#getting-started","title":"Getting Started","text":"<p>Data as a Product (DaaP) is the result of applying product thinking into datasets, making sure they have a series of capabilities including discoverability, security, explorability, understandability, trustworthiness, etc.</p> <p>The vast majority of articles on the concept of data products describe the \"Data as an Application\" approach. This is unfortunate, as this method has significant drawbacks compared to the \"Data as a Pure Structure\" approach.</p> <p>Independently from the concept of data products, Yehonathan Sharvit described the principles of using pure data structures in his book called \"Data-Oriented Programming\" (DOP):</p> <ul> <li>Separating code (behavior) from data.</li> <li>Treating data as immutable.</li> <li>Separating data schema from data representation.</li> <li>Representing data with generic data structures.</li> </ul>"},{"location":"abstract/data_mesh/data-as-a-product/#data-as-an-application","title":"Data as an Application","text":"<p>In the Data as an Application approach, data is accessed through an interface (API) that allows clients to retrieve data by making API calls to an application instance.</p> <p>This application instance can be a bespoke enterprise application delivering specific data or even a full-fledged database or AI model (LLMs are increasingly popular) offering a rather generic data abstraction. Regardless, a running application instance (the \"server\") is required for client access to the data. This instance manages the data stored in the underlying files, prohibiting direct file access through operating system calls or custom library functions.</p> <p>Instead, you must use the predefined interfaces provided by the application instance.</p>"},{"location":"abstract/data_mesh/data-as-a-product/#data-as-a-pure-structure","title":"Data as a Pure Structure","text":"<p>Separating code (behavior) from data requires the definition of data products as pure data structures.</p> <p></p> <p>\"Data as a Pure Structure\" refers to data that exists independently and outside of any application. It includes all necessary metadata to transform raw data into a product, but remains pure data structures independent of an accompanying application. This data structure can be accessed without making API calls to an application instance.</p> <p>For example, in Unix-like systems, such a structure would be stored as a file (essentially a byte stream), which can be directly accessed via operating system calls. Although these are technically system calls, they differ from API calls since you can\u2019t, quite frankly, do anything at all without a running operating system.</p> <p>Often, we maintain a distributed version of the hierarchical file system at a low level in the operating system to enable distributed file access, such as HDFS in Hadoop. This layer (a data product storage infrastructure if you like) integrates closely with the operating system rather than functioning as a separate application.</p> <p>Note</p> <p>It\u2019s important to note that applications can optionally include their own data stores in addition to the data products they produce and consume (Application 4 and 5 in the image). Also note, these data products contain comprehensive information about their usage and lineage. For instance, Pure Structure 2 includes details about being created by Application 2 using Pure Structures 1 and 3 as inputs.</p>"},{"location":"abstract/data_mesh/data-as-a-product/#examples","title":"Examples","text":""},{"location":"abstract/data_mesh/data-as-a-product/#discoverable","title":"Discoverable","text":"<p>In order for data as a product to be discoverable, a search engine is needed and users must be able to register datasets in this engine and request access to them (this will increase security, another capability explained below).</p> <p>The first iteration for this capability could be just a list of datasets in your de facto internal intranet and you can iterate and build incrementally from that. Remember that processes and culture are more important than deploying the ultimate data catalogue tool too early (which can be too complex for employees to use).</p> <p></p>"},{"location":"abstract/data_mesh/data-as-a-product/#addressable","title":"Addressable","text":"<p>Having addressable datasets makes your teams more productive. On one side, Data Analysts and Data Scientists are autonomous in finding and using the data they need. On the other side, Data Engineers have far less interruptions from people asking where they can find data about X.</p> <p></p>"},{"location":"abstract/data_mesh/data-as-a-product/#self-describing-and-interoperable","title":"Self-describing and interoperable","text":"<p>As we commented in the blog post where we explained Adevinta\u2019s data mesh journey, datasets need to contain metadata that make them understandable and follow the same naming conventions (which will make the datasets interoperable). We found these pieces of metadata to be super useful to our Data Analysts:</p> <ul> <li>Data location (as seen above)</li> <li>Data provenance and data mapping</li> <li>Sample data</li> <li>Execution time and freshness</li> <li>Input preconditions</li> <li>Example notebook or SQL queries using the data set</li> </ul>"},{"location":"abstract/data_mesh/data-as-a-product/#trustworthy-and-secure","title":"Trustworthy and secure","text":"<p>Checking data quality regularly and automatically is a must to fulfil the trustworthy characteristic of data as a product. And owners of the datasets need to react accordingly to the results of these checks.</p> <p>Quality checks must be done at pipeline input and output and it doesn\u2019t hurt to provide contextual data quality information to consumers of the data; like for example in Tableau dashboards.</p> <p></p>"},{"location":"abstract/data_mesh/data-as-a-product/#read-mores","title":"Read Mores","text":"<ul> <li>Data as a product vs data products. What are the differences?</li> <li>Deliver Your Data as a Product, But Not as an Application</li> <li>IBM: What is data as a product (DaaP)?</li> </ul> <ol> <li> <p>GetDBT: Data products vs. data as a product \u21a9</p> </li> </ol>"},{"location":"abstract/data_mesh/data-domain-usage-monitoring/","title":"Data Domain Usage Monitoring","text":""},{"location":"abstract/data_mesh/data-domain-usage-monitoring/#read-mores","title":"Read Mores","text":"<ul> <li> Data Domain Usage Monitoring \u2014 \u0e27\u0e31\u0e14\u0e1c\u0e25\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19\u0e2d\u0e07\u0e04\u0e4c\u0e01\u0e23</li> </ul>"},{"location":"abstract/data_observability/","title":"Data Observability","text":"<p>Data Observability is an organization's ability to fully understand the health of the data in their systems. Data observability eliminates data downtime by applying best practices learned from DevOps to data pipeline observability.</p>"},{"location":"abstract/data_observability/#getting-started","title":"Getting Started","text":"<p> Data observability Tools use automated monitoring, automated root cause analysis, data lineage and data health insights to detect, resolve, and prevent data anomalies. This leads to healthier pipelines, more productive teams, better data management, and happier customers.</p> <p>For data engineers and developers, data observability is important because data downtime means wasted time and resources; for data consumers, it erodes confidence in your decision-making.</p> <p>Quote</p> <p>Data Downtime: periods of time when data is partial, erroneous, missing, or otherwise inaccurate. Only multiplies as data systems become increasingly complex, supporting an endless ecosystem of sources and consumers.</p> <p>The 5 pillars of data observability:</p> <ol> <li> <p>Freshness:</p> <p>Freshness seeks to understand how up-to-date your data tables are, as well as the cadence at which your tables are updated. Freshness is particularly important when it comes to decision-making; after all, stale data is basically synonymous with wasted time and money.</p> </li> <li> <p>Quality:</p> <p>Your data pipelines might be in working order but the data flowing through them could be garbage. The quality pillar looks at the data itself and aspects such as percent NULLS, percent uniques and if your data is within an accepted range. Quality gives you insight into whether or not your tables can be trusted based on what can be expected from your data.</p> </li> <li> <p>Volume:</p> <p>Volume refers to the completeness of your data tables and offers insights on the health of your data sources. If 200 million rows suddenly turns into 5 million, you should know.</p> </li> <li> <p>Schema:</p> <p>Changes in the organization of your data, in other words, schema, often indicates broken data. Monitoring who makes changes to these tables and when is foundational to understanding the health of your data ecosystem.</p> </li> <li> <p>Lineage:</p> <p>When data breaks, the first question is always \u201cwhere?\u201d Data lineage provides the answer by telling you which upstream sources and downstream ingestors were impacted, as well as which teams are generating the data and who is accessing it. Good lineage also collects information about the data (also referred to as metadata) that speaks to governance, business, and technical guidelines associated with specific data tables, serving as a single source of truth for all consumers.</p> </li> </ol> <p>Together, these components provide valuable insight into the quality and reliability of your data. Let\u2019s take a deeper dive.</p>"},{"location":"abstract/data_observability/#the-key-features-of-data-observability-tools","title":"The key features of data observability tools","text":"<p>Evaluation criteria can be tricky when you may not even have a strong answer to the basic question, \"what are data observability tools?\" A great data observability platform has the following features:</p> <ul> <li> <p>It connects to your existing stack quickly and seamlessly and does not require   modifying your data pipelines, writing new code, or using a particular programming   language. This allows quick time to value and maximum testing coverage without   having to make substantial investments.</p> </li> <li> <p>It monitors your data at-rest and does not require extracting the data from where   it is currently stored. This allows the data observability solution to be performant,   scalable and cost-efficient. It also ensures that you meet the highest levels   of security and compliance requirements.</p> </li> <li> <p>It requires minimal configuration and practically no threshold-setting. Data   observability tools should use machine learning models to automatically learn   your environment and your data. It uses anomaly detection techniques to let you   know when things break. It minimizes false positives by taking into account not   just individual metrics, but a holistic view of your data and the potential impact   from any particular issue. You do not need to spend resources configuring and   maintaining noisy rules within your data observability platform.</p> </li> <li> <p>It requires no prior mapping of what needs to be monitored and in what way.   It helps you identify key resources, key dependencies and key invariants so that   you get broad data observability with little effort.</p> </li> <li> <p>It provides rich context that enables rapid triage and troubleshooting, and   effective communication with stakeholders impacted by data reliability issues.   Data observability tools should not stop at \u201cfield X in table Y has values lower   than Z today.\u201d</p> </li> <li> <p>It prevents issues from happening in the first place by exposing rich information   about data assets so that changes and modifications can be made responsibly and   proactively.</p> </li> </ul>"},{"location":"abstract/data_observability/#references","title":"References","text":"<ul> <li>(TODO): https://sanjmo.medium.com/is-data-observability-critical-to-successful-data-analytics-d09b983b95c6</li> <li>MontecarloData: What is Data Observability</li> <li>https://www.montecarlodata.com/blog-data-observability-tools/</li> <li>https://snowplow.io/blog/data-observability-dashboard/</li> <li>https://www.youtube.com/watch?v=4K33fP46vDw</li> <li>https://www.montecarlodata.com/blog-what-is-data-observability/</li> </ul>"},{"location":"abstract/data_observability/dobs-data-consistency/","title":"Data Consistency","text":"<p>Data Consistency is one of ten dimensions of data quality. Data is considered consistent if two or more values in different locations are identical. Ask yourself: Is the data internally consistent? If there are redundant data values, do they have the same value? Or, if values are aggregations of each other, are the values consistent with each other?</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#what-are-some-examples-of-inconsistent-data","title":"What are some examples of inconsistent data?","text":"<p>Imagine you\u2019re a lead analytics engineer at Rainforest, an ecommerce company that sells hydroponic aquariums to high-end restaurants. An example of data inconsistency here would be if the engineering team records aquarium models from database transactions that don\u2019t match the models recorded by the sales team from the CRM.</p> <p></p> <p>Another example would be if the monthly profit number is not consistent with the monthly revenue and cost numbers. Some of the ways that this could happen would be if you have concurrent workloads, which could be in the form replication pipelines themselves, or downstream SQL transformations that lead to additional nodes (forks) in your end to end pipelines. The solution to all of this would be proper data management, starting with measuring for data consistency.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-do-you-measure-data-consistency","title":"How do you measure data consistency?","text":"<p>To test your any data quality dimension, you must measure, track, and assess a relevant data quality metric. In the case of data consistency, you can measure the number of passed checks to track the uniqueness of values, uniqueness of entities, corroboration within the system, or whether referential integrity is maintained. Codd\u2019s Referential Integrity constraint is one example of a consistency check.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-to-ensure-data-consistency","title":"How to ensure data consistency?","text":"<p>One way to ensure data consistency is through anomaly detection, sometimes called outlier analysis, which helps you to identify unexpected values or events in a data set.</p> <p>Using the example of two numbers that are inconsistent with one another, anomaly detection software would notify you instantly when data you expect to match doesn\u2019t. The software knows it\u2019s unusual because its machine learning model learns from your historical metadata.</p> <p>Quote</p> <p>\u201cThe important thing is that when things break, I know immediately \u2014 and I can usually fix them before any of my stakeholders find out.\u201d<sup>1</sup></p> <p>In other words, you can say goodbye to the dreaded WTF message from your stakeholders. In that way, automated, real-time anomaly detection is like a friend who has always got your back.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#how-does-data-consistency-differ-from-application-consistency-vs-strong-consistency","title":"How does data consistency differ from application consistency vs strong consistency?","text":"<p>For purposes of this article, we\u2019ve be focused solely on data consistency as it relates to the actual values themselves. You may see some overlap with Strong Consistency and Application Consistency, other terms in the data space:</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#strong-consistency","title":"Strong Consistency","text":"<p>You may run into this term when looking up database consistency as well, particularly in complex database system architectures. Strong consistency is all about ensuring that everyone in a distributed system is on the same page when it comes to data, and includes the concepts from CAP theorem. It means that no matter which node or replica you\u2019re looking at, they all have the most up-to-date view of the data at any given time. It\u2019s like making sure everyone sees things happening in the same order, as if there\u2019s only one copy of the data. So, when you read something, you can always trust that you\u2019re getting the latest version. Achieving strong consistency usually involves using coordination mechanisms like distributed transactions or consensus algorithms to make sure the data stays intact and in sync across the entire distributed system.</p> <p>Note</p> <p>It\u2019s important here to maintain atomicity of timestamps to ensure you don\u2019t miss any data changes.</p>"},{"location":"abstract/data_observability/dobs-data-consistency/#application-consistency","title":"Application Consistency","text":"<p>Application consistency refers to making sure that the data within an application (app), typically hosted in your database system, is in good shape and follows the rules and requirements set by that application. It\u2019s like ensuring that everything is in order and makes sense according to how the app is supposed to work. When an app is consistent, you can trust that the data is accurate, complete, and meets the specific rules or relationships defined by the app. It\u2019s all about making sure things run smoothly and produce reliable results. To achieve application consistency, developers need to implement checks and safeguards to validate data, handle errors effectively, and enforce the application\u2019s unique rules.</p> <p>Note</p> <p>A crossover here in data governance may be to utilize data validation during a user\u2019s data entry (e.g. email) to ensure that downstream usage of that field can be maintained.</p> <ol> <li> <p> What is Data Consistency? Definition, Examples, and Best Practices \u21a9</p> </li> </ol>"},{"location":"abstract/data_observability/dobs-data-lineage/","title":"Data Lineage","text":""},{"location":"abstract/data_observability/dobs-data-lineage/#references","title":"References","text":"<ul> <li>https://pub.towardsai.net/understanding-data-lineage-from-source-to-destination-e505f2cd19ba</li> <li>https://medium.com/alvin-ai/what-is-data-lineage-and-techniques-to-implement-it-4f1939b11327</li> <li>Mastering Data Lineage: Techniques, Best Practices, and Tools for Success</li> </ul>"},{"location":"abstract/data_observability/dobs-data-orchestration/","title":"Data Orchestration","text":"<p>Quote</p> <p>Data Orchestration is a Conductor for the Modern Data Platform.<sup>1</sup></p> <p>At the core of a Data Orchestration Platform lies a workflow management system for dependably handling tasks that move and transform data so that it can ultimately be used to generate artifacts to inform business decisions, including dashboards, reports, and machine-learning (ML) predictions. However, a data-driven organization requires capabilities beyond workflow management, in order to increase the availability of trusted data to a broad base of business users beyond the centralized information technology group.</p> <ol> <li> <p>Data Orchestration: A Conductor for the Modern Data Platform \u21a9</p> </li> </ol>"},{"location":"abstract/data_observability/dobs-data-quality-metrics/","title":"Data Quality Metrics","text":""},{"location":"abstract/data_observability/dobs-data-quality-metrics/#references","title":"References","text":"<ul> <li>Data Quality Metrics</li> <li>Data Quality Metrics for Data Warehouse</li> </ul>"},{"location":"abstract/data_observability/dobs-data-quality-pyramid/","title":"Data Observability: Data Quality Pyramid","text":"<ul> <li>The Data Quality Pyramid: A Path to Getting Started with Data Observability</li> </ul>"},{"location":"abstract/data_observability/dobs-maintaining-a-viable-monitoring-sys/","title":"Data Observability: Maintaining a Viable Monitoring System","text":"<p>https://medium.com/@wyaddow/maintaining-a-viable-monitoring-system-for-data-observability-b510152ecfa8</p>"},{"location":"abstract/data_pipeline/","title":"Data Pipeline","text":"<ul> <li>Pipeline Design Patterns</li> <li>Workflow Orchestration</li> <li>Data Lineage Tracking</li> <li>Pipeline Monitoring and Observability</li> </ul>"},{"location":"abstract/data_pipeline/#noted","title":"Noted","text":"<ul> <li>Creating a Data Pipeline from Scratch</li> <li>https://medium.com/@kxnk/data-pipelines-pocket-reference-key-learning-points-bb9225ee95a9</li> <li>Mastering Data Engineering: A breakdown of Data Pipeline Stages and Tools</li> </ul>"},{"location":"abstract/data_pipeline/#type-of-data-pipelines","title":"Type of Data Pipelines","text":"<ul> <li>https://hardiks.medium.com/types-of-data-pipelines-you-need-to-look-at-efa9eaac4a79</li> </ul>"},{"location":"abstract/data_pipeline/#optimization","title":"Optimization","text":"<ul> <li>How we think about Data Pipelines is changing</li> </ul>"},{"location":"abstract/data_pipeline/declarative-data-pipeline/","title":"Declarative","text":"<p>Declarative Data Pipeline</p>"},{"location":"abstract/data_pipeline/declarative-data-pipeline/#getting-started","title":"Getting Started","text":""},{"location":"abstract/data_pipeline/declarative-data-pipeline/#read-mores","title":"Read Mores","text":""},{"location":"abstract/data_strategy/","title":"Data Strategy","text":"<p>Data Strategy is a long-term plan that defines the technology, processes, people, and rules required to manage an organization's information assets. All types of businesses collect large amounts of raw data today. However, they need a well-thought-out data management and analytics plan if they want to use this information to make informed decisions and create machine learning (ML) or generative artificial intelligence (AI) applications.</p> <p>A Data Strategy outlines an organization's long-term vision for collecting, storing, sharing, and usage of its data. It makes working with data easier to do at every step of the data journey for everyone who needs it in your organization.</p> <ul> <li>Query Optimization</li> <li>Indexing Strategies</li> <li>Caching Mechanisms</li> <li>Distributed Computing for Big Data</li> </ul>"},{"location":"abstract/data_strategy/#references","title":"References","text":"<ul> <li>What is Data Strategy?</li> <li>TODO - A Data Strategy Comprehensive Guide for a Step-by-Step Data Maturity Assessment</li> </ul>"},{"location":"abstract/data_strategy/strategy-data-driven/","title":"Data Driven","text":""},{"location":"abstract/data_strategy/strategy-data-driven/#references","title":"References","text":"<ul> <li>Medium: Data Driven Management - The Why Who What and How</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/","title":"Semantic Layer","text":"<p>One of the key components of modern data warehouses is the \"democratization\" of data. Rather than have a centralized data team curate and publish reports &amp; dashboards, organizations have found immense value from creating self-serve reports &amp; dashboards that users across an organization can query and access the data they need to make data-driven decisions.</p> <p>Below is a typical architecture for a modern data stack using a medallion data warehouse architecture</p> <p></p>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#pain-points-with-the-modern-data-stack-mds","title":"Pain points with the Modern Data Stack (MDS):","text":"<p>However, many organizations have found that in trying to democratize their data, it has caused another set of data problems. Many end users find that the data they need is NOT in the correct format for a visualization or they need to make further calculations to get the results they need to create a report or dashboard.</p> <p>Moreover, analysts may have different names for essentially the same data record. The marketing team, for example, may refer to a user as a \u201cprospect,\u201d while the sales team might call that same business a \u201cclient,\u201d while the finance team calls this user entity a \u201ccounter party.\u201d However, a machine learning model or analytics team might want to analyze all this data and relate the data back to a single user.</p> <p>This often leads analysts to create their own metrics within their local environment or business intelligence tool.</p> <p></p> <p>However, this can create major problems:</p> <ul> <li>localized metrics have no oversight and can be inaccurate.</li> <li>changes in data structure upstream can cause these metrics to break.</li> <li>the work required to calculate metrics is duplicated.</li> <li>metrics created in one BI tool cannot be integrated with other BI tools or used by the data scientist or data engineering team for automation.</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#what-is-a-semantic-layer","title":"What is a Semantic layer?","text":"<p>Quote</p> <p>A semantic layer is a business representation of data and offers a unified and consolidated view of data across an organization. Its important to note that the semantic layer does not hold or store the actual data, it is a metadata and abstraction layer built on the source data.<sup>1</sup></p> <p>This abstraction over the data warehouse maps different data definitions from various data sources into a unified, consistent, and single view of data for analytics and other business purposes. Semantic layers create pre-defined views of processed data that abstract complexity and apply business-oriented definitions. Metrics like revenue or cost are defined.</p> <p>Hence, the need for centralized metrics to give users a single source of truth and semantic layer. Organizations can codify their metrics in a centralized place and have confidence that they\u2019re getting the same number and context around that number wherever they consume data.</p> <p></p> <p>Benefits of the dbt\u2019s semantic layer include:</p> <ul> <li>centralized metrics defined once and used across the organization ensuring accuracy, clarity, and molecularity.</li> <li>metrics can be used in downstream applications including machine learning models, automation tools, reverse-etl tools, and even spreadsheets.</li> <li>increased ease in calculating metrics (dbt uses YAML files).</li> <li>automated documentation to provide metric definition and context.</li> </ul>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#when-not-to-use-a-semantic-layer","title":"When NOT to use a Semantic Layer:","text":"<p>Although the semantic layer can help data teams creating functional self-serving reports and dashboards, it is really only needed when the data infrastructure is relatively mature and there are lots of data users using multiple BI tools. The data warehouse used to build metrics must be relatively clean and formatted correctly into appropriate dimension and fact tables.</p> <p>If you are a data engineer in a startup, building a semantic layer may not be the best use of your time since building a data infrastructure to support a semantic layer will require significant effort.</p> <p>However, the semantic layer can be a great value add for organizations using dbt that want to standardize metric definitions. If you would like to delve deeper into dbt\u2019s semantic layer, please check out their blog. In my next blog post, I will show how you can use the dbt semantics layer in practice in dbt-Cloud.</p>"},{"location":"abstract/data_strategy/strategy-semantic-layer/#conclusion","title":"Conclusion","text":"<p>In conclusion, a semantic layer is an innovative approach to solving the metric decentralization problem associated with self-serving reporting &amp; dashboards. It provides a centralized control mechanism to ensure metrics calculations are consistent &amp; accurate while allowing users the flexibility to use data to improve decision making in their day-to-day operations.</p> <ol> <li> <p>Why data teams need a semantic layer? \u21a9</p> </li> </ol>"},{"location":"abstract/data_strategy/strategy-sensitive-data/","title":"Sensitive Data","text":"<ul> <li>Data Engineering: Architectures &amp; Strategies for Handling Sensitive Data</li> </ul>"},{"location":"abstract/dataops/","title":"DataOps","text":"<p>Note</p> <p>DataOps is a collection of practices that focuses on breaking down silos between data producers and consumers, improving data quality and transparency of results to be seamless.<sup>1</sup></p> DataOps Flow <p>DataOps (Data Operations) provides a collaboration of data engineering, data science and operations team. It aims to automate the delivery of the right and reliable data to appropriate teams through a much faster approach. And this leads to better data productivity and enhanced human communication.</p> <p>Quote</p> <p>DataOps aims to bridge the gap between data engineering, data science, and operations teams by promoting continuous integration and delivery, collaboration and automation of data-related processes.<sup>2</sup></p> DataOps"},{"location":"abstract/dataops/#getting-started","title":"Getting Started","text":""},{"location":"abstract/dataops/#dataops-vs-devops","title":"DataOps vs. DevOps","text":"<p>The key difference is DevOps is a methodology that brings development and operations teams together to make software development and delivery more efficient, while DataOps focuses on breaking down silos between data producers and data consumers to make data more reliable and valuable.</p> <p>Quote</p> <p>To keep a constant pulse on the overall health of their systems, DevOps engineers leverage observability to monitor, track, and triage incidents to prevent application downtime.</p> <p>Software observability consists of three pillars:</p> <ul> <li>Logs: A record of an event that occurred at a given timestamp. Logs also provide context to that specific event that occurred.</li> <li>Metrics: A numeric representation of data measured over a period of time.</li> <li>Traces: Represent events that are related to one another in a distributed environment.</li> </ul> <p></p> <p>Data Observability is an organization\u2019s ability to fully understand the health of the data in their systems. It reduces the frequency and impact of data downtime (periods of time when your data is partial, erroneous, missing or otherwise inaccurate) by monitoring and alerting teams to incidents that may otherwise go undetected for days, weeks, or even months.</p> <p>Like software observability, data observability includes its own set of pillars:</p> <ul> <li>Freshness: Is the data recent? When was it last updated?</li> <li>Distribution: Is the data within accepted ranges? Is it in the expected format?</li> <li>Volume: Has all the data arrived? Was any of the data duplicated or removed from tables?</li> <li>Schema: What\u2019s the schema, and has it changed? Were the changes to the schema made intentionally?</li> <li>Lineage: Which upstream and downstream dependencies are connected to a given data asset? Who relies on that data for decision-making, and what tables is that data in?</li> </ul> <p>Read More about Data Observability</p>"},{"location":"abstract/dataops/#dataops-framework","title":"DataOps Framework","text":"<p>To facilitate faster and more reliable insight from data, DataOps teams apply a continuous feedback loop, also referred to as the DataOps lifecycle.</p> <p>Here is what the DataOps lifecycle looks like in practice:</p> <ul> <li> <p>Planning: Partnering with product, engineering, and business teams to set   KPIs, SLAs, and SLIs for the quality and availability of data   (more on this in the next section).</p> </li> <li> <p>Development: Building the data products and machine learning models that   will power your data application.</p> </li> <li> <p>Integration: Integrating the code and/or data product within your existing   tech and or data stack. (For example, you might integrate a dbt model with   Airflow so the dbt module can automatically run.)</p> </li> <li> <p>Testing: Testing your data to make sure it matches business logic and meets   basic operational thresholds (such as uniqueness of your data or no null values).</p> </li> <li> <p>Release: Releasing your data into a test environment.</p> </li> <li> <p>Deployment: Merging your data into production.</p> </li> <li> <p>Operate: Running your data into applications such as Looker or Tableau   dashboards and data loaders that feed machine learning models.</p> </li> <li> <p>Monitor: Continuously monitoring and alerting for any anomalies in the data.</p> </li> </ul> <p>This cycle will repeat itself over and over again. However, by applying similar principles of DevOps to data pipelines, data teams can better collaborate to identify, resolve, and even prevent data quality issues from occurring in the first place.</p>"},{"location":"abstract/dataops/#building-blocks","title":"Building Blocks","text":"<ol> <li>Collaboration    DataOps promotes cross-functional collaboration and brings together different    data engineering, data science, analyst and operations personnel.    This collaboration helps streamline communication, align goals, and share    knowledge and expertise across different business domains.</li> <li>Automation    Automation plays a vital role in DataOps. It involves automating repetitive tasks, such as data ingestion, transformation and validation as well as deployment of infrastructure, applications and pipelines. This helps minimize manual errors, improve quality, and accelerate the overall data lifecycle.</li> <li>CICD    DataOps borrows from Agile and DevOps methodologies, making continuous integration and delivery a natural fit. CI/CD focuses on delivering data quickly and predictably while ensuring quality, reliability, and security. This approach enables organizations to respond rapidly to changing business needs and deliver insights in a timely manner.</li> <li>Monitoring    DataOps borrows from quality control approaches like Statistical Process Control and Total Quality Management. DataOps emphasizes the importance of monitoring data pipelines and processes to identify issues and bottlenecks. Monitoring tools and techniques help track data quality, performance, and availability, which enables proactive troubleshooting and timely response to any potential problems.</li> <li>Version Control    Similar to software development practices, DataOps promotes the use of version control systems like Git so you can manage changes to data infrastructure, application configurations, code, and sometimes data itself. This ensures auditability, while enabling teams to roll back to previous versions if needed and maintain a history of changes.</li> <li>Data Governance    DataOps emphasizes the need for proper data governance practices. It includes establishing data quality standards, data lineage, and data cataloging to boost data\u2019s usability and value. Data governance ensures compliance with regulations, controls access, maintains data integrity, and enhances trust in the data being used for decision-making.</li> </ol> <p>Note</p> <p>By adopting DataOps practices, organizations can accelerate data delivery, improve data quality, and keep costs in check.</p>"},{"location":"abstract/dataops/#functions-of-a-dataops-platform","title":"Functions of a DataOps Platform","text":"<p>With a strong DataOps platform, organizations can solve inefficient data-generation and processing problems and improve poor data quality caused by errors and inconsistencies.</p>"},{"location":"abstract/dataops/#data-ingestion","title":"Data Ingestion","text":"<p>Generally, the first step in the lifecycle of data starts by ingesting it into a data lake or data warehouse to transform it into usable insights through the pipeline. Organizations need a competent tool that can handle ingestion at scale. As an organization grows, an efficient solution for data ingestion is required.</p>"},{"location":"abstract/dataops/#data-orchestration","title":"Data Orchestration","text":"<p>Data volume and type within organizations will continue to grow and it's important to manage that growth before it gets out of hand. Infinite resources are an impossibility, so data orchestration focuses on organizing multiple pipeline tasks into a single end-to-end process that enables data to move predictably through a platform when and where it's needed without requiring an engineer to code manually.</p>"},{"location":"abstract/dataops/#data-transformation","title":"Data Transformation","text":"<p>Data transformation is where raw data is cleaned, manipulated and prepared for analysis. Organizations should invest in tools that make creating complex models faster and manage them reliably as teams expand and the data volume grows.</p>"},{"location":"abstract/dataops/#data-catalog","title":"Data Catalog","text":"<p>A data catalog is like a library for all data assets within an organization. It organizes, describes and makes data easy to find and understand. In DataOps, a data catalog can help build a solid foundation for smooth data operations. Data catalogs serve as a single point of reference for all data needs.</p>"},{"location":"abstract/dataops/#data-observability","title":"Data Observability","text":"<p>Without data observability, an organization is not implementing a proper DataOps practice. Observability protects the reliability and accuracy of data products being produced and makes reliable data available for upstream and downstream users.</p> <p>Read More about Data Observability</p>"},{"location":"abstract/dataops/#noted","title":"Noted","text":"<ul> <li>Continuous Integration/Continuous Deployment for Data</li> <li>Automated Testing for Data Pipelines</li> <li>Data Observability</li> <li>Incident Management for Data Systems</li> </ul> <ul> <li>Data Pipeline Orchestration</li> <li>Continuous Integration/Continuous Deployment (CI/CD) for Data</li> <li>Data Observability</li> <li>Automated Data Quality Checks</li> </ul>"},{"location":"abstract/dataops/#roles","title":"Roles","text":""},{"location":"abstract/dataops/#dataops-engineer","title":"DataOps Engineer","text":"<p>DataOps Engineer create and implement the processes that enable successful teamwork within the data organization. They design the orchestrations that enable work to flow seamlessly from development to production. They make sure that environments are aligned and that hardware, software, data, and other resources are available on demand.</p> <ul> <li> 10 New DevOps Tools to Watch in 2024</li> <li> Is Data Observability Critical to Successful Data Analytics?</li> </ul>"},{"location":"abstract/dataops/#examples","title":"Examples","text":"<ul> <li> DataOps for the Modern Data Warehouse</li> </ul>"},{"location":"abstract/dataops/#read-mores","title":"Read Mores","text":"<ul> <li>What is DataOps?</li> </ul> <ol> <li> <p> Data Engineering concepts: Part 7, DevOps, DataOps and MLOps \u21a9</p> </li> <li> <p>CNDI: What is DataOps? \u21a9</p> </li> </ol>"},{"location":"abstract/dataops/data-cicd/","title":"Data CICD","text":"<p>https://blog.stackademic.com/ci-cd-for-modern-data-engineering-b64d9e76393a</p>"},{"location":"abstract/dataops/data-cicd/#the-data-lifecycle","title":"The Data Lifecycle","text":"<p>Application development starts with Ideation and product requirement and ends with product release and monitoring. On the other hand, data life cycle is unique because it starts with data creation, transformation, deployment and all the way to data deletion (thanks to data privacy regulations that require organizations to delete personal data of users, if requested).</p> <p></p> <ul> <li>BlogDetLeft; CICD for Data</li> </ul>"},{"location":"abstract/dataops/data-product/","title":"Data Product","text":"<p>Quote</p> <p>A Data Product is a product or service that leverages data as its core component to deliver value to end users or customers (data consumers).<sup>1</sup></p>"},{"location":"abstract/dataops/data-product/#getting-started","title":"Getting Started","text":"<p>Data Product involves collecting, analyzing, and using data to provide insight, information, or functionality that helps address specific needs or business challenges. Data products integrate data from source systems, process it, ensure compliance, and make it instantly available to consumers.</p> <p>Quote</p> <p>A Data Product is a valuable data asset that your company uses to make decisions and all the technologies and metadata associated with that asset.<sup>2</sup></p> <p></p>"},{"location":"abstract/dataops/data-product/#principles-of-an-enterprise-grade-data-product","title":"Principles of an Enterprise-Grade Data Product","text":"<ol> <li>Inherently valuable: Complete and valuable without any other data required.</li> <li>Product managed: A product owner manages each product through the entire lifecycle,     like any other digital product.</li> <li>Developable: Should be structured to allow an Agile and well-governed development     process.</li> <li>Backward compatible: Must be versioned, co-existent in multiple versions,     and backward compatible.</li> <li>Exclusive: End users can only access data through the product; there are no     back doors.</li> <li>Trustworthy: There must be commitments to consumers, including completeness,     accuracy, and timeliness.</li> <li>Interoperable and composable: Combining one data product with others must be     easy, including creating new ones.</li> <li>Secure: Must meet access, confidentiality, and compliance requirements.</li> <li>Accessible: Must be accessible in a useful way for target consumers.</li> <li>Discoverable: Must be easy for target users to find.</li> </ol>"},{"location":"abstract/dataops/data-product/#the-data-product-contract","title":"The Data Product Contract","text":"<p>As you share data across organizational boundaries, you are faced with the questions outlined above. Depending on your role in the organization, you may care more or less about certain ones. If you are a data product producer, you want a developable product. If you are a data product consumer, you want some guarantees about the product. An easy way to capture the criteria both parties care about is a data product contract.</p> <p>Quote</p> <p>A data product contract needs to encapsulate a set of contract items.</p> <p>To make a data product findable, be sure to add awe require name, description, and version to be published to a data product registry. To share a data product and make it inherently valuable, you need to capture the datasets. To make a data product accessible, share the desired output ports. To provide the necessary guarantees,  define the Service Level Objectives (SLO), the desired KPIs, and Service Level Indicators (SLI), as well as the current value of the KPI.</p> <p></p> <p>As you deploy many data products across domains and the organization, you want to compose data products and ensure interoperability as defined in the metadata of the datasets. If you are sharing data outside your organization, you will want to define the license types. If you want to monetize data, your clients will likely ask you to provide sample data so that they can try it before they buy.</p> <p>Defining the data product contract allows you to foster communication between data product producers and consumers effectively. The data product owner can improve data quality over time. The data product consumer can enjoy a service level agreement with defined guarantees supporting them throughout the entire product lifecycle.</p>"},{"location":"abstract/dataops/data-product/#examples","title":"Examples","text":"<p>Data products can take various forms depending on the industry and application and target internal or external audiences. Here are a few examples:</p> <ol> <li>Data Analytics Platforms: These systems collect and analyze data from multiple sources to give businesses comprehensive insights into their operations, customer behavior, or market trends. They often involve data visualization tools and reporting capabilities.</li> <li>Recommendation Systems:  Platforms that leverage user data and algorithms to suggest personalized recommendations for products (a la Amazon), movies (like Netflix), music (think Spotify), or content (like Instagram) based on user preferences and historical data.</li> <li>Predictive Models: Data products can use machine learning models to build predictive models that forecast future outcomes based on historical data. For instance, predictive analytics models can be used in finance to forecast stock prices or in healthcare to predict disease outcomes.</li> <li>Generative AI: Customer support can use conversational AI or chatbot systems that leverage natural language processing and generative AI techniques to simulate human-like conversations and provide automated responses to customer inquiries. They use machine learning algorithms, like language models, to generate contextually relevant responses tailored to the specific inquiry.</li> <li>Real-time Dashboards: display real-time data metrics and key performance indicators (KPIs) to provide instant insights into various aspects of a business, such as supply chain health, website traffic, or social media engagement.</li> <li>Data APIs: Application Programming Interfaces (APIs) that enable access to structured and unstructured data for developers to build their data products applications.</li> </ol> <ol> <li> <p>DataOps Live: What is a Data Product? \u21a9</p> </li> <li> <p>CNDI: What is a Data Product? \u21a9</p> </li> </ol>"},{"location":"abstract/emerging_trends/","title":"Emerging Trends &amp; Challenges","text":"<ul> <li>Edge Computing for Data</li> <li>Quantum Data Processing</li> <li>Blockchain in Data Management</li> <li>Artificial Intelligence in Data Engineering</li> </ul>"},{"location":"abstract/emerging_trends/#getting-started","title":"Getting Started","text":"<p>As data sources multiply, the process of ingesting, processing, and transforming data becomes cumbersome. Systems must scale to avoid becoming bottlenecks. Automation tools are stepping in to streamline data engineering processes, ensuring data pipelines remain robust and efficient. Data engineers are increasingly adopting distributed data storage and processing systems like Hadoop or Spark. Netflix's adoption of a microservices architecture to manage increasing data is a testament to the importance of scalable designs.</p> <p>The shift towards cloud-based storage and processing solutions has also revolutionized data engineering. Platforms like AWS, Google Cloud, and Azure offer scalable storage and high-performance computing capabilities. These platforms support the vast computational demands of data engineering algorithms and ensure data is available and consistent across global architectures.</p> <p>AI's rise has paralleled the evolution of data-driven decision-making in businesses. Advanced algorithms can sift through vast datasets, identify patterns, and offer previously inscrutable insights. However, these insights are only as good as the data they're based on.</p> <p>Quote</p> <p>The fundamentals of data engineering are evolving with AI.<sup>1</sup></p>"},{"location":"abstract/emerging_trends/#using-data-engineering-in-ai","title":"Using data engineering in AI","text":"<p>AI applications process large amounts of visual data. For example, optical character recognition converts typed or handwritten text images into machine-encoded text. Computer vision applications train machines to interpret and understand visual data. Images and videos from different sources, resolutions, and formats need harmonization. The input images must be of sufficient quality, and data engineers often need to preprocess these images to enhance clarity. Many computer vision tasks require labeled data, demanding efficient tools for annotating vast amounts of visual data.</p> <p>AI applications can also learn and process human language. For instance, they can identify hidden sentiments in content, summarize and sort documents, and translate from one language to another. These AI applications require data engineers to convert text into numerical vectors using embeddings. The resulting vectors can be extensive, demanding efficient storage solutions. Real-time applications require rapid conversion into these embeddings, challenging data infrastructure's processing speed. Data pipelines have to maintain the context of textual data. It also involves data infrastructure capable of handling varied linguistic structures and scripts.</p> <p>Large language models(LLMs)like OpenAI's GPT series are pushing the boundaries of what's possible in natural language understanding and generation. These models, trained on extensive and diverse text corpora, require:</p> <ul> <li> <p>Scale \u2014 The sheer size of these models necessitates data storage and processing   capabilities at a massive scale.</p> </li> <li> <p>Diversity \u2014 To ensure the models understand the varied nuances of languages,   data sources need to span numerous domains, languages, and contexts.</p> </li> <li> <p>Quality \u2014 Incorrect or biased data can lead LLMs to produce misleading or   inappropriate outputs.</p> </li> </ul>"},{"location":"abstract/emerging_trends/#using-ai-for-data-engineering","title":"Using AI for data engineering","text":"<p>The relationship between AI and data engineering is bidirectional. While AI depends on data engineering for quality inputs, data engineers also employ AI tools to refine and enhance their processes. The inter-dependency underscores the profound transformation businesses are undergoing. As AI continues to permeate various sectors, data engineering expectations also evolve, necessitating a continuous adaptation of skills, tools, and methodologies.</p> <p></p>"},{"location":"abstract/emerging_trends/#automated-data-cleansing","title":"Automated data cleansing","text":"<p>AI models can learn the patterns and structures of clean data. They can automatically identify and correct anomalies or errors by comparing incoming data to known structures. This ensures that businesses operate with clean, reliable data without manual intervention, thereby increasing efficiency and reducing the risk of human error.</p>"},{"location":"abstract/emerging_trends/#predictive-data-storage","title":"Predictive data storage","text":"<p>AI algorithms analyze the growth rate and usage patterns of stored data. By doing so, they can predict future storage requirements. This foresight allows organizations to make informed decisions about storage infrastructure investments, avoiding over-provisioning and potential storage shortages.</p>"},{"location":"abstract/emerging_trends/#anomaly-detection","title":"Anomaly detection","text":"<p>Machine learning models can be trained to recognize \"normal\" behavior within datasets. When data deviates from this norm, it's flagged as anomalous. Early detection of anomalies can warn businesses of potential system failures, security breaches, or even changing market trends.</p>"},{"location":"abstract/emerging_trends/#imputation","title":"Imputation","text":"<p>Along with detecting anomalies, AI can also help with discovering and completing missing data points in a given dataset. Machine learning models can predict and fill in missing data based on patterns and relationships in previously known data. For instance, if a dataset of weather statistics had occasional missing values for temperature, an ML model could use other related parameters like humidity, pressure, and historical temperature data to estimate the missing value.</p>"},{"location":"abstract/emerging_trends/#data-categorization-and-tagging","title":"Data categorization and tagging","text":"<p>NLP models can automatically categorize and tag unstructured data like text, ensuring it's stored appropriately and is easily retrievable. This automates and refines data organization, allowing businesses to derive insights faster and more accurately.</p>"},{"location":"abstract/emerging_trends/#optimizing-data-pipelines","title":"Optimizing data pipelines","text":"<p>AI algorithms can analyze data flow through various pipelines, identifying bottlenecks or inefficiencies. By optimizing the pipelines, businesses can ensure faster data processing and lower computational costs.</p>"},{"location":"abstract/emerging_trends/#semantic-data-search","title":"Semantic data search","text":"<p>Rather than relying on exact keyword matches, AI-driven semantic searches understand the context and intent behind search queries, allowing users to find data based on its meaning. This provides a more intuitive and comprehensive data search experience, especially in vast data lakes.</p>"},{"location":"abstract/emerging_trends/#data-lineage-tracking","title":"Data lineage tracking","text":"<p>AI models can trace the journey of data from its source to its final destination, detailing all transformations along the way. This ensures transparency, aids in debugging, and ensures regulatory compliance.</p> <p>In essence, the integration of AI into data engineering is a game-changer. As AI simplifies and enhances complex data engineering tasks, professionals can focus on strategic activities, pushing the boundaries of what's possible in data-driven innovation. The potential of this synergy is vast, promising unprecedented advancements in data efficiency, accuracy, and utility.</p> <ol> <li> <p>Data engineering 101 \u21a9</p> </li> </ol>"},{"location":"abstract/mlops/","title":"ML Ops","text":"<ul> <li>Data Preparation for Machine Learning</li> <li>Feature Engineering</li> <li>Data Versioning for ML</li> <li>Serving ML Models in Production</li> </ul>"},{"location":"abstract/mlops/#what-is-mlops","title":"What is MLOps?","text":"<p>What is MLOps?</p> <p>MLOps = DevOps + DataOps + ModelOps<sup>1</sup></p> <p>Machine Learning Operations (MLOps) is a set of practices that includes Machine Learning, DevOps and Data Engineering elements. The main aim of the union is reliable and efficient deployment and maintenance of Machine Learning systems in production.</p> Basic Model Training Flow <p>Features of MLOps:</p> <ul> <li>Ensures validation of data, data schemas, and models along with testing and     validation of code</li> <li>Facilitates automated deployment of a ML pipeline that should automatically     deploy a model and corresponding prediction service.</li> <li>Expedites the process of automatically re-training and serving the models.</li> </ul> MLOps Relation Overview"},{"location":"abstract/mlops/#components","title":"Components","text":"MLOps Components"},{"location":"abstract/mlops/#workflow","title":"Workflow","text":"MLOps High-Level Workflow Architecture[^1] <p>The following table summarizes MLOps main practices and how they relate to DevOps and Data Engineering practices:</p> MLOps Main Practices"},{"location":"abstract/mlops/#what-is-mlops-engineer","title":"What is MLOps Engineer?","text":"<p>Quote</p> <p>ML Engineers build and retrain machine learning models. MLOps Engineers enable the ML Engineers. MLOps Engineers build and maintain a platform to enable the development and deployment of machine learning models. They typically do that through standardization, automation, and monitoring. MLOps Engineers reiterate the platform and processes to make the machine learning model development and deployment quicker, more reliable, reproducible, and efficient.</p> <p>However, ML Engineers focus on building, training and validating machine learning models, while MLOps Engineers concentrate primarily on testing, deploying and monitoring models in production environments.</p>"},{"location":"abstract/mlops/#noted","title":"Noted","text":"<ul> <li> Practical MLOps \u2014 MLflow</li> </ul>"},{"location":"abstract/mlops/#read-mores","title":"Read Mores","text":"<ul> <li> MLOps - What, Why and How</li> <li> MLOps - Benefits That Make It An Upcoming Industry Trend</li> <li> Orchestrate MLOps by using Azure Databricks</li> <li> Architecting MLOps on the Lakehouse</li> <li> MLOps - workflows on Databricks</li> </ul> <ol> <li> <p> Architecting MLOps on the Lakehouse \u21a9</p> </li> </ol>"},{"location":"abstract/mlops/mlops-challenge/","title":"Challenge","text":"<p>Data engineering and machine learning pipelines are both very different but oddly can feel very similar. Many ML engineers I have talked to in the past rely on tools like Airflow to deploy their batch models.</p>"},{"location":"abstract/mlops/mlops-challenge/#data-engineering-pipelines","title":"Data Engineering Pipelines","text":"<p>First, let\u2019s dive into data pipelines. Data pipelines are a series of processes that extract data from different sources, clean it, and then store it in a data warehouse or database. This is important for companies because it helps them make informed decisions. A data pipeline is made up of four parts:</p> <p></p>"},{"location":"abstract/mlops/mlops-challenge/#machine-learning-pipelines","title":"Machine Learning Pipelines","text":"<p>Machine Learning (ML) pipelines don\u2019t work in a straight line like data pipelines. Instead, they involve building, training, and deploying ML models. This is used to automate the entire process of building an ML model, from collecting data to deploying it in production. An ML pipeline is made up of five parts:</p>"},{"location":"abstract/mlops/mlops-challenge/#read-mores","title":"Read Mores","text":"<ul> <li>Data Engineering Vs Machine Learning Pipelines</li> </ul>"},{"location":"abstract/mlops/mlops-cicd/","title":"CICD","text":"<p>Building and deploying code to production environments is a fundamental aspect of software development. This process is equally pivotal in the realm of production-grade Machine Learning, where models undergo regular retraining with new data and are deployed for serving predictions.</p>"},{"location":"abstract/mlops/mlops-cicd/#read-mores","title":"Read Mores","text":"<ul> <li>CI/CD for Machine Learning in 2024: Best Practices to Build, Train, and Deploy</li> </ul>"},{"location":"blogs/","title":"Blogs","text":"<p>This  Blog Site will be sharing data knowledge that not included in Data Engineering Abstraction.</p>"},{"location":"blogs/2024/03/24/change-data-capture-cdc/","title":"Change Data Capture (CDC)","text":"<p>Change Data Capture (CDC) is a design pattern that identifies and tracks changes in data so that action can be taken using this change data. It\u2019s particularly crucial in data-driven architectures where it\u2019s essential to promptly and reliably capture the modifications in the source data store and propagate them to downstream systems. CDC can be applied in various scenarios, including data replication, data warehousing, real-time analytics, and more.</p>","tags":["Data"]},{"location":"blogs/2024/03/24/change-data-capture-cdc/#implementing-change-data-capture","title":"Implementing Change Data Capture","text":"<p>CDC can be implemented using different approaches based on the source system and its capabilities:</p> <ol> <li> <p>Log-Based CDC: This method utilizes the transaction logs generated by    databases to capture changes. It offers high performance and minimal impact    on the source system.</p> </li> <li> <p>Trigger-Based CDC: Here, triggers are set up on tables to detect changes,    and when triggered, they capture the changes and push them to the target    system. This approach is relatively easy to implement but might affect the    source system\u2019s performance.</p> </li> <li> <p>Replication-Based CDC: Some databases offer built-in replication features    that can be used for CDC. This approach provides an efficient and integrated    solution but may be limited to specific database technologies.</p> </li> <li> <p>API-Based CDC: For applications without built-in CDC support, custom APIs    can be created to extract and propagate changes.</p> </li> </ol>","tags":["Data"]},{"location":"blogs/2024/03/24/change-data-capture-cdc/#example","title":"Example","text":"<p>Implementing Change Data Capture (CDC) in Python involves monitoring changes in a source database and capturing them for propagation to a target system. To demonstrate a simple example of CDC using Python, we will use a log-based approach to monitor an SQLite database and print the changes made to the database.</p> <pre><code>import sqlite3\nimport time\n\n# Function to monitor and capture changes in the database\ndef capture_changes():\n    connection = sqlite3.connect('your_database.db')\n    cursor = connection.cursor()\n\n    last_rowid = 0\n\n    while True:\n        # Get the maximum rowid from the source table\n        cursor.execute(\"SELECT MAX(rowid) FROM your_source_table\")\n        max_rowid = cursor.fetchone()[0]\n\n        # If the maximum rowid is greater than the last captured rowid, there are new changes\n        if max_rowid &gt; last_rowid:\n            cursor.execute(\"SELECT * FROM your_source_table WHERE rowid &gt; ?\", (last_rowid,))\n            changes = cursor.fetchall()\n\n            print(\"Changes captured:\")\n            for change in changes:\n                print(change)\n\n            # Update the last captured rowid to the maximum rowid for the next iteration\n            last_rowid = max_rowid\n\n        # Wait for a short period before checking for new changes again\n        time.sleep(1)\n\nif __name__ == \"__main__\":\n    capture_changes()\n</code></pre>","tags":["Data"]},{"location":"blogs/2024/03/24/change-data-capture-cdc/#references","title":"References","text":"<ul> <li>Medium: Change Data Capture (CDC)</li> <li>Medium: The Change Data Capture (CDC) Design Pattern</li> <li>Medium: CDC from zero to hero</li> <li>Medium: Change Data Capture (CDC): Empowering Real-time Data Integration</li> </ul>","tags":["Data"]},{"location":"services/","title":"Services","text":"<p>Warning</p> <p>I will filter Data Engineering Services on this session that do not relate and unnecessary for the most Data Architect and Modern Data Strack.</p> <p> Service and Cloud Provider that use on a Modern Data Stack concept.</p> <p>This services topic, I will focus with below contents:</p> <ul> <li>Setting Connections (Python API)</li> <li>Hands-On its Services</li> <li>Adjustment &amp; Optimization Setting</li> </ul>"},{"location":"services/#providers-comparison","title":"Providers Comparison","text":"<ul> <li> Compare Cloud Service Providers</li> </ul>"},{"location":"services/#data-processing","title":"Data Processing","text":"<ul> <li> Databricks vs Snowflake: A Complete 2024 Comparison</li> </ul>"},{"location":"services/#iac-infra","title":"IaC &amp; Infra","text":"<ul> <li> Pulumi vs Terraform: The Definitive Guide to Choosing Your IaC Tool</li> <li> Pulumi vs. Terraform: Choosing your IaC Tool</li> </ul>"},{"location":"services/#finops","title":"FinOps","text":"<ul> <li>Proven in Production: A Cost-Effective Modern Data Architecture for Small and Medium Enterprises</li> </ul>"},{"location":"services/ansible/","title":"Ansible","text":""},{"location":"services/ansible/#read-mores","title":"Read Mores","text":"<ul> <li> Ansible \u0e02\u0e2d\u0e41\u0e1a\u0e1a\u0e40\u0e1a\u0e32\u0e46</li> </ul>"},{"location":"services/aws/","title":"AWS","text":"<p>Amazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow.</p>"},{"location":"services/aws/#read-mores","title":"Read Mores","text":"<ul> <li> AWS Data Analytics and Ingestion Services\u2014 (SAA-CO3 Summary 2024)</li> </ul>"},{"location":"services/aws/aws-iam/","title":"AWS IAM","text":""},{"location":"services/aws/aws-iam/#getting-started","title":"Getting Started","text":""},{"location":"services/aws/aws-iam/#policies","title":"Policies","text":"<p>https://docs.aws.amazon.com/mediaconnect/latest/ug/iam-policy-examples-asm-secrets.html</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetResourcePolicy\",\n                \"secretsmanager:GetSecretValue\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecretVersionIds\"\n            ],\n            \"Resource\": [\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes128-1a2b3c\",\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes192-4D5e6F\",\n                \"arn:aws:secretsmanager:us-west-2:111122223333:secret:aes256-7g8H9i\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:ListSecrets\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"services/aws/aws-vpc/","title":"VPC","text":"<p>Virtual Private Cloud (VPC) is service to manage all networks in AWS in virtual, that mean you do not need to do anything on the physical layer.</p> <p>Note</p> <p>The reason why it is named VPC instead of the more meaningful term \"virtual network\" is that the Cloud wants to convey that \"users can create their own private network\".<sup>1</sup></p>"},{"location":"services/aws/aws-vpc/#getting-started","title":"Getting Started","text":"<p>VPC \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e27\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e19\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e2a\u0e48\u0e27\u0e19 Network \u0e41\u0e17\u0e1a\u0e08\u0e30\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48\u0e1a\u0e19 AWS Cloud. Region \u0e04\u0e37\u0e2d\u0e20\u0e39\u0e21\u0e34\u0e20\u0e32\u0e04\u0e17\u0e35\u0e48\u0e17\u0e32\u0e07 AWS \u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23 \u0e42\u0e14\u0e22\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e25\u0e37\u0e2d\u0e01 region \u0e17\u0e35\u0e48\u0e43\u0e01\u0e25\u0e49\u0e01\u0e31\u0e1a\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e40\u0e23\u0e32\u0e21\u0e32\u0e01\u0e2a\u0e38\u0e14\u0e44\u0e14\u0e49 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49 user \u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e23\u0e27\u0e14\u0e40\u0e23\u0e47\u0e27\u0e41\u0e25\u0e30\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e</p> <p>\u0e0b\u0e36\u0e48\u0e07 VPC \u0e2d\u0e22\u0e39\u0e48\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49 Region \u0e16\u0e49\u0e32\u0e40\u0e23\u0e32\u0e25\u0e2d\u0e07\u0e41\u0e1b\u0e25\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e23\u0e39\u0e1b\u0e20\u0e32\u0e1e\u0e07\u0e48\u0e32\u0e22\u0e46 \u0e08\u0e30\u0e44\u0e14\u0e49\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49</p> <p></p>"},{"location":"services/aws/aws-vpc/#create","title":"Create","text":"<ul> <li>Go to VPC &gt; Click Your VPCs &gt; Start Create VPC</li> <li> <p>Add <code>Name tag</code> that is your VPC name and its naming convention should be;</p> <pre><code>Name tag: &lt;prefix&gt;-&lt;resource&gt;-&lt;project&gt;-&lt;environment&gt;-vpc\n</code></pre> <ul> <li><code>&lt;prefix&gt;</code>: \u0e04\u0e37\u0e2d\u0e04\u0e33\u0e02\u0e36\u0e49\u0e19\u0e15\u0e49\u0e19\u0e40\u0e08\u0e49\u0e32\u0e02\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e0a\u0e37\u0e48\u0e2d \u0e25\u0e39\u0e01\u0e04\u0e49\u0e32\u0e2b\u0e23\u0e37\u0e2d\u0e1a\u0e23\u0e34\u0e29\u0e31\u0e17\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e27\u0e01\u0e33\u0e2b\u0e19\u0e14</li> <li><code>&lt;resource&gt;</code>: \u0e04\u0e37\u0e2d\u0e0a\u0e37\u0e48\u0e2d\u0e02\u0e2d\u0e07 service \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e40\u0e0a\u0e48\u0e19 ec2, radis, rds \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19</li> <li><code>&lt;project&gt;</code>: \u0e04\u0e37\u0e2d\u0e0a\u0e37\u0e48\u0e2d project \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e08\u0e30\u0e43\u0e0a\u0e49 vpc \u0e15\u0e31\u0e27\u0e19\u0e35\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23</li> <li><code>vpc</code>: \u0e04\u0e37\u0e2d suffix \u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e33\u0e25\u0e07\u0e17\u0e49\u0e32\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e07\u0e48\u0e32\u0e22\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e40\u0e27\u0e25\u0e32\u0e43\u0e0a\u0e49 vpc \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a\u0e15\u0e31\u0e27\u0e2d\u0e37\u0e48\u0e19</li> </ul> </li> <li> <p>Add <code>IPv4 CIDR block</code> value for your VPC</p> <p>\u0e04\u0e37\u0e2d\u0e15\u0e31\u0e27\u0e01\u0e33\u0e2b\u0e19\u0e14 IPv4 CIDR block \u0e43\u0e2b\u0e49\u0e01\u0e31\u0e1a VPC \u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 /16 \u0e16\u0e36\u0e07 /28 \u0e42\u0e14\u0e22\u0e40\u0e08\u0e49\u0e32\u0e02\u0e2d\u0e07\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e43\u0e0a\u0e49 10.0.0.0/16 \u0e0b\u0e36\u0e48\u0e07\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e0a\u0e49 IPs \u0e44\u0e14\u0e49\u0e16\u0e36\u0e07 65,536 IPs \u0e40\u0e25\u0e22\u0e17\u0e35\u0e40\u0e14\u0e35\u0e22\u0e27 &lt;&lt;\u0e2d\u0e48\u0e32\u0e19 CIDR \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21&gt;&gt;</p> </li> </ul> <p>\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e40\u0e23\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07 VPC \u0e40\u0e2a\u0e23\u0e47\u0e08\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e08\u0e30\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 resource \u0e17\u0e35\u0e48\u0e2d\u0e22\u0e39\u0e48\u0e20\u0e32\u0e22\u0e43\u0e19 VPC \u0e44\u0e14\u0e49 \u0e2b\u0e19\u0e36\u0e48\u0e07\u0e43\u0e19 resource \u0e19\u0e31\u0e49\u0e19\u0e01\u0e47\u0e04\u0e37\u0e2d AZ \u0e2b\u0e23\u0e37\u0e2d Availability Zone \u0e17\u0e32\u0e07\u0e40\u0e08\u0e49\u0e32\u0e02\u0e2d\u0e07\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e19\u0e31\u0e49\u0e19\u0e43\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01\u0e44\u0e14\u0e49\u0e40\u0e25\u0e37\u0e2d\u0e01 region: Singapore \u0e0b\u0e36\u0e48\u0e07\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 AZ \u0e44\u0e14\u0e49 3 Zone \u0e04\u0e37\u0e2d ap-southeast-1a, ap-southeast-1b \u0e41\u0e25\u0e30 ap-southeast-1c</p>"},{"location":"services/aws/aws-vpc/#availability-zone","title":"Availability Zone","text":"<p>Availability Zone (AZ) \u0e01\u0e47\u0e04\u0e37\u0e2d data center \u0e02\u0e2d\u0e07 cloud \u0e17\u0e35\u0e48\u0e15\u0e31\u0e49\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e15\u0e36\u0e01\u0e2b\u0e23\u0e37\u0e2d\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19 region \u0e19\u0e31\u0e49\u0e19 \u0e42\u0e14\u0e22\u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48\u0e41\u0e25\u0e49\u0e27\u0e43\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07 region \u0e08\u0e30\u0e21\u0e35 data center \u0e2d\u0e22\u0e39\u0e48\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 2\u20133 \u0e41\u0e2b\u0e48\u0e07 \u0e08\u0e38\u0e14\u0e1b\u0e23\u0e30\u0e2a\u0e07\u0e04\u0e4c\u0e04\u0e37\u0e2d \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19\u0e20\u0e31\u0e22\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34 \u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e44\u0e21\u0e48\u0e04\u0e32\u0e14\u0e1d\u0e31\u0e19 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e41\u0e2b\u0e48\u0e07\u0e19\u0e36\u0e07\u0e25\u0e48\u0e21\u0e44\u0e1b \u0e22\u0e31\u0e07\u0e21\u0e35\u0e2d\u0e35\u0e01 1\u20132 \u0e41\u0e2b\u0e48\u0e07\u0e04\u0e2d\u0e22 support \u0e2d\u0e22\u0e39\u0e48</p> <p>\u0e0b\u0e36\u0e48\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e17\u0e35\u0e48\u0e08\u0e30\u0e43\u0e0a\u0e49 AZ \u0e40\u0e14\u0e35\u0e22\u0e27\u0e01\u0e47\u0e44\u0e14\u0e49 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e22\u0e2d\u0e21\u0e23\u0e31\u0e1a\u0e19\u0e30 \u0e27\u0e48\u0e32\u0e2b\u0e32\u0e01\u0e40\u0e01\u0e34\u0e14\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e44\u0e21\u0e48\u0e04\u0e32\u0e14\u0e1d\u0e31\u0e19\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2a\u0e39\u0e0d\u0e2b\u0e32\u0e22\u0e44\u0e14\u0e49\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19</p> <ol> <li> <p> AWS VPC \u0e04\u0e37\u0e2d\u0e2d\u0e30\u0e44\u0e23? \u0e21\u0e32\u0e17\u0e33\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e08\u0e31\u0e01\u0e01\u0e31\u0e19 \u21a9</p> </li> </ol>"},{"location":"services/aws/athena/athena-with-delta-lake/","title":"AWS Athena: With DeltaLake","text":"<p>https://medium.com/@gauravthalpati/athena-spark-with-delta-lake-f8cb71616c64</p>"},{"location":"services/aws/ec2/ec2-domain-with-route53/","title":"EC2: Connect Domain Name from Route53","text":"<p>https://medium.com/@arif.w/aws-%E0%B9%80%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%95%E0%B9%88%E0%B8%AD-domain-name-%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B8%81%E0%B8%B1%E0%B8%9A-ec2-instance-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-route-53-a73b90dc5864</p>"},{"location":"services/aws/ecs/ecs-deploy-with-fargate/","title":"Deploy with Fargate","text":"<p>ETL stands for Extract, Transform, and Load. An ETL pipeline is essentially just a data transformation process \u2014 extracting data from one place, doing something with it, and then loading it back to the same or a different place.</p>"},{"location":"services/aws/ecs/ecs-deploy-with-fargate/#references","title":"References","text":"<ul> <li>Deploy Long-Running ETL Pipelines to ECS with Fargate</li> </ul>"},{"location":"services/aws/emr/emr-compare-with-databricks/","title":"Elastic MapReduce: Compare with Databricks","text":"<ul> <li>https://medium.com/insiderengineering/benchmarking-amazon-emr-vs-databricks-4c2f7d209d3d</li> </ul>"},{"location":"services/aws/glue/","title":"AWS Glue","text":""},{"location":"services/aws/glue/#getting-started","title":"Getting Started","text":"<p>Let's start by briefly introducing the key concepts we'll cover:</p> <ul> <li>AWS Glue Data Catalog: A centralized metadata repository that stores metadata about data sources, transformations, and targets.</li> <li>AWS Glue Database: A logical container that organizes tables, allowing for better data management.</li> <li>AWS Glue Tables: The structure that represents data in the AWS Glue Data Catalog.</li> <li>Partition in AWS: A way to organize data within a table based on the values of one or more columns.</li> <li>AWS Glue Crawlers: Tools that scan various data stores, extract metadata, and create table definitions.</li> <li>AWS Glue Connection: A resource that contains the properties needed to connect to your source or target data store.</li> <li>AWS Glue Jobs: An ETL process that extracts data from the source, transforms it and loads it into the target.</li> <li>AWS Glue Triggers: Events or conditions that can automatically invoke AWS Glue workflows.</li> <li>AWS Glue Endpoints: URLs that allow external systems to call AWS Glue API operations.</li> </ul>"},{"location":"services/aws/glue/#optimization","title":"Optimization","text":"<p>https://levelup.gitconnected.com/optimizing-and-reducing-aws-glue-costs-e7426fa732af</p>"},{"location":"services/aws/glue/#read-mores","title":"Read Mores","text":"<ul> <li>A Guide to AWS Glue: Data Catalog, Databases, Crawler, Triggers, with S3</li> </ul>"},{"location":"services/aws/glue/glue-data-quality/","title":"Glue Data Quality","text":"<p>Data Quality is fundamental for a variety of reasons, spanning across business, science, government, and numerous other sectors. There are many reasons why it is essential to maintain high data quality, including:</p> <ul> <li>Conveying business decisions: business decisions must be based on accurate and reliable data. Low quality data could lead to incorrect decisions that negatively impact business operations.</li> <li>Precise analyses: data analysis is a fundamental part of many business activities. Low-quality data could lead to inaccurate results and misinterpretations.</li> <li>Regulatory compliance: many companies are subject to strict regulations on data management. Lack of data quality could lead to regulatory violations and financial penalties.</li> <li>Time savings and efficiency: high-quality data simplify business processes. Cleaning and correcting data takes significant time and effort. High-quality data therefore reduce the need for such activities.</li> <li>Customer satisfaction: data quality directly affects customer satisfaction. Incorrect data can lead to errors in customer reports and communications.</li> </ul>"},{"location":"services/aws/glue/glue-data-quality/#what-is-aws-glue-data-quality","title":"What is AWS Glue Data Quality?","text":"<p>AWS Glue Data Quality is a feature of AWS Glue, Amazon\u2019s fully managed extract, transform, and load (ETL) service. This feature provides users with the ability to validate and monitor the quality of data sources, making it easier to maintain high-quality data for analytics and machine learning applications.</p> <p>Below are the main features of Glue Data Quality.</p>"},{"location":"services/aws/glue/glue-data-quality/#automatic-recommendations-of-custom-rules-for-your-data","title":"Automatic recommendations of custom rules for your data","text":"<p>...</p>"},{"location":"services/aws/glue/glue-data-quality/#references","title":"References","text":"<ul> <li>AWS Glue Data Quality: the ultimate guide to turning data into reliable decisions</li> </ul>"},{"location":"services/aws/glue/glue-local-env/","title":"Glue: Setup Local Environment","text":"<p>https://medium.com/@datasanity/setting-up-a-local-environment-for-aws-glue-development-bdb8ca74e608</p>"},{"location":"services/aws/glue/glue-with-iceberg/","title":"With Iceberg","text":""},{"location":"services/aws/glue/glue-with-iceberg/#getting-started","title":"Getting Started","text":""},{"location":"services/aws/glue/glue-with-iceberg/#define-the-important-libraries","title":"Define the important libraries","text":"<pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\ncatalog_nm = \"glue_catalog\"\n\n# The Glue Database Name which has the source table\nin_database=\"&lt;glue-database-input&gt;\"\n\n# The input Glue Table which we will be using as a source for the Iceberg data\nin_table_name=\"covid_19_data\"\n\n# The Glue Database Name which will be used to create an output Iceberg table\ndatabase_op='database_ib'\n\n# The Glue Table Name which will be used as a destination for Iceberg table\ntable_op='covid_dataset_iceberg'\n\n# The S3 path which will be used to store the Iceberg files as output\ns3_output_path='s3://&lt;your-destination-bucket-name&gt;/iceberg-output/'\n\ntable = str(catalog_nm)+ '.`' + str(database_op) + '`.' + str(table_op)\n\nprint(\"\\nINPUT Database : \" + str(in_database))\nprint(\"\\nINPUT Table : \" + str(in_table_name))\nprint(\"\\nOUTPUT IceBerg Database : \" + str(database_op))\nprint(\"\\nOUTPUT IceBerg Table : \" + str(table))\nprint(\"\\nOUTPUT IceBerg S3 Path : \" + str(s3_output_path))\n</code></pre> <p>In line with the script we need to define a important job parameter in the glue which will indicate the Glue job executer to leverage the Iceberg table format as output for the data. For this you need to define a parameter named as</p> <pre><code>--datalake-formats : iceberg\n</code></pre> <p></p>"},{"location":"services/aws/glue/glue-with-iceberg/#define-spark-and-glue-context","title":"Define Spark and Glue context","text":"<pre><code>def create_spark_iceberg(catalog_nm: str = \"glue_catalog\"):\n    \"\"\"\n    Function to initialize a session with iceberg by default\n    :param catalog_nm:\n    :return spark:\n    \"\"\"\n    from pyspark.sql import SparkSession\n    # You can set this as a variable if required\n    warehouse_path = s3_output_path\n\n    spark = (\n        SparkSession.builder\n            .config(f\"spark.sql.catalog.{catalog_nm}\", \"org.apache.iceberg.spark.SparkCatalog\")\n            .config(f\"spark.sql.catalog.{catalog_nm}.warehouse\", warehouse_path)\n            .config(f\"spark.sql.catalog.{catalog_nm}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n            .config(f\"spark.sql.catalog.{catalog_nm}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n            .config(f\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n            .getOrCreate()\n    )\n    return spark\n\nibspark = create_spark_iceberg(catalog_nm)\nibsc = ibspark.sparkContext\nibglueContext = GlueContext(ibsc)\nibjob = Job(ibglueContext)\nibjob.init(args[\"JOB_NAME\"], args)\n</code></pre>"},{"location":"services/aws/glue/glue-with-iceberg/#read-the-source-glue-table-and-write-into-a-destination-glue","title":"Read the source Glue table and write into a destination Glue","text":"<pre><code>#Read the Glue inout table from thr Catalog using a Glue DynamicFrame\nInputDynamicFrameTable = (\n    ibglueContext.create_dynamic_frame\n        .from_catalog(database=in_database, table_name=in_table_name)\n)\n\n#Convert the Glue DynamicFrame into a Spark DataFrame\nInputDynamicFrameTable_DF = InputDynamicFrameTable.toDF()\n\n#Register the Spark DataFrame as TempView\nInputDynamicFrameTable_DF.createOrReplaceTempView(\"InputDataFrameTable\")\nibspark.sql(\"select * from InputDataFrameTable LIMIT 10\").show()\n\n#Filter the source table with country as 'Australia'\ncolname_df = ibspark.sql(\"SELECT * FROM InputDataFrameTable WHERE country='Australia'\")\ncolname_df.createOrReplaceTempView(\"OutputDataFrameTable\")\n\n#Write the filtered Data into an ICEBERG table format in Glue destination table\nib_Write_SQL = f\"\"\"\n    CREATE OR REPLACE TABLE {catalog_nm}.{database_op}.{table_op}\n    USING iceberg\n    TBLPROPERTIES (\"format-version\"=\"2\", \"write_compression\"=\"gzip\")\n    AS SELECT * FROM OutputDataFrameTable;\n    \"\"\"\n\n#Run the Spark SQL query\nibspark.sql(ib_Write_SQL)\n</code></pre>"},{"location":"services/aws/glue/glue-with-iceberg/#read-mores","title":"Read Mores","text":"<ul> <li> Deploy Apache Iceberg Data Lake on Amazon S3 using AWS Glue Spark job</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-timestream-grafana/","title":"AWS IoT: Timestream and Grafana","text":"<ul> <li>https://www.youtube.com/watch?v=z8T4hAERuOg</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-to-kinesis/","title":"AWS IoT Core: Rule to S3 via Kinesis","text":""},{"location":"services/aws/iot_core/iot-rule-to-kinesis/#references","title":"References","text":"<ul> <li>https://www.thingrex.com/iot_ingest_cost/</li> <li>https://www.cloudthat.com/resources/blog/a-guide-to-ingest-iot-data-to-kinesis-data-firehose-and-store-in-s3</li> </ul>"},{"location":"services/aws/iot_core/iot-rule-to-s3/","title":"AWS IoT Core: Rule","text":""},{"location":"services/aws/iot_core/iot-rule-to-s3/#set-up-aws-iot","title":"Set up AWS IoT","text":"<ol> <li> <p>Set up AWS IoT Core</p> <ul> <li>Go to AWS IoT Core  Manage    Thing types  Click <code>Create thing type</code></li> <li>On Thing groups  Click <code>Create thing group</code></li> <li> <p>On Security  Policies    Click <code>Create policy</code></p> </li> <li> <p>Create Connect Policy</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"iot:Connect\"],\n      \"Resource\": [\n        \"arn:aws:iot:&lt;region&gt;:&lt;aws_account_id&gt;:client/${iot:Connection.Thing.ThingName}\"\n      ]\n    }\n  ]\n}\n</code></pre> </li> </ul> </li> </ol>"},{"location":"services/aws/iot_core/iot-rule-to-s3/#references","title":"References","text":"<ul> <li>https://docs.aws.amazon.com/iot/latest/developerguide/iot-moisture-tutorial.html</li> <li>https://www.cloudthat.com/resources/blog/step-by-step-guide-to-register-a-new-iot-device-in-aws-cloud?utm_source=blog-website&amp;utm-medium=text-link&amp;utm_campaign=%2Fstep-by-step-guide-to-register-a-new-iot-device-in-aws-cloud%2F</li> <li>https://www.cloudthat.com/resources/blog/step-by-step-guide-to-store-aws-iot-data-into-s3-bucket</li> <li>https://fanchenbao.medium.com/integrate-iot-device-with-aws-iot-using-python-part-i-upload-data-to-mqtt-topic-3f2b30ec6a6</li> <li> <p>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/cost-effectively-ingest-iot-data-directly-into-amazon-s3-using-aws-iot-greengrass.html#cost-effectively-ingest-iot-data-directly-into-amazon-s3-using-aws-iot-greengrass-best-practices</p> </li> <li> <p>https://docs.aws.amazon.com/solutions/latest/constructs/aws-iot-s3.html</p> </li> <li> <p>https://github.com/awslabs/aws-solutions-constructs/tree/main/source/patterns/@aws-solutions-constructs/aws-iot-s3</p> </li> <li> <p>https://www.youtube.com/watch?v=SN4_2ua6_Ko</p> </li> <li>https://www.youtube.com/watch?v=kGqbqYMjAIE</li> </ul>"},{"location":"services/aws/kinesis/kinesis-data-firehose/","title":"Kinesis Data Firehose","text":"<p>Kinesis Data Firehose (Amazon Data Firehose) is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, Coralogix, and Elastic. With Amazon Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Amazon Data Firehose, and it automatically delivers the data to the destination that you specified. You can also configure Amazon Data Firehose to transform your data before delivering it.</p>"},{"location":"services/aws/kinesis/kinesis-data-firehose/#references","title":"References","text":"<ul> <li> What Is Amazon Data Firehose?</li> </ul>"},{"location":"services/aws/kinesis/kinesis-data-streams/","title":"Kinesis Data Streams","text":"<p>Kinesis Data Stream is a set of shards. Each shard has a sequence of data records. Each data record has a sequence number that is assigned by Kinesis Data Streams.</p> <p></p> Shard <p>A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity. Each shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.</p> Partition Key <p>A partition key is used to group data by shard within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to. Partition keys are Unicode strings, with a maximum length limit of 256 characters for each key. An MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards using the hash key ranges of the shards. When an application puts data into a stream, it must specify a partition key.</p> <p>Note</p> <p>Sequence numbers cannot be used as indexes to sets of data within the same stream. To logically separate sets of data, use partition keys or create a separate stream for each dataset.</p>"},{"location":"services/aws/kinesis/kinesis-data-streams/#references","title":"References","text":"<ul> <li> What Is Amazon Kinesis Data Streams?</li> <li> Streaming in Data Engineering</li> </ul>"},{"location":"services/aws/lambda/","title":"AWS Lambda","text":""},{"location":"services/aws/lambda/#getting-started","title":"Getting Started","text":""},{"location":"services/aws/lambda/#stop-using-for-everything","title":"Stop Using for Everything","text":"<ul> <li>https://medium.com/@pooyan_razian/stop-using-aws-lambda-for-everything-9d9b2d3a9763</li> </ul>"},{"location":"services/aws/lambda/lambda-cicd/","title":"CICD","text":""},{"location":"services/aws/lambda/lambda-cicd/#read-mores","title":"Read Mores","text":"<ul> <li>CI/CD and AWS Lambda : A primer</li> </ul>"},{"location":"services/aws/lambda/lambda-with-docker/","title":"AWS Lambda: With Docker","text":"<p>https://towardsdatascience.com/using-poetry-and-docker-to-package-your-model-for-aws-lambda-cd6d448eb88f</p>"},{"location":"services/aws/s3/s3-filter-content/","title":"Filter Content","text":""},{"location":"services/aws/s3/s3-filter-content/#read-mores","title":"Read Mores","text":"<ul> <li> AWS - Selecting Content from Objects</li> </ul>"},{"location":"services/aws/s3/s3-transform-lambda/","title":"Transform Lambda","text":"<p>https://eoins.medium.com/using-s3-object-lambdas-to-generate-and-transform-on-the-fly-874b0f27fb84</p>"},{"location":"services/aws/s3/s3-trigger-lambda/","title":"Trigger Lambda","text":""},{"location":"services/aws/s3/s3-trigger-lambda/#read-mores","title":"Read Mores","text":"<ul> <li> Comparing 2 Ways to Trigger Lambda from S3</li> </ul>"},{"location":"services/aws/secrets/secrets-across-account/","title":"AWS Secrets","text":"<p>https://itarunkumar.medium.com/securely-access-aws-secrets-manager-across-different-aws-accounts-3b0821f4e2e1</p>"},{"location":"services/aws/step_functions/stfn-combine-data-parallel-state/","title":"Combine Data from Parallel State","text":"<p>Note</p> <p>The elements of the output array correspond to the branches in the same order that they appear in the \"Branches\" array</p> <p>Below is an example of a State Machine that looks up user information, and in the parallel state provides the Name and Address in parallel.</p> <p></p> State Machine<pre><code>{\n  \"Comment\": \"Parallel Example.\",\n  \"StartAt\": \"LookupCustomerInfo\",\n  \"States\": {\n    \"LookupCustomerInfo\": {\n      \"Type\": \"Parallel\",\n      \"Next\": \"ReturnCombinedData\",\n      \"Branches\": [\n        {\n          \"StartAt\": \"Name\",\n          \"States\": {\n            \"Name\": {\n              \"Type\": \"Pass\",\n              \"Comment\": \"This can be any state that returns data, since it's a Pass, the values are static\",\n              \"Result\": {\n                \"fname\": \"Jack\",\n                \"lname\": \"Johnson\"\n              },\n              \"ResultPath\": \"$.Name\",\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"Address\",\n          \"States\": {\n            \"Address\": {\n              \"Type\": \"Pass\",\n              \"Comment\": \"This can be any state that returns data, since it's a Pass, the values are static\",\n              \"Result\": {\n                \"Number\": \"123\",\n                \"Street\": \"Fake St\",\n                \"Zip\": \"90210\"\n              },\n              \"ResultPath\": \"$.Address\",\n              \"End\": true\n            }\n          }\n       }\n      ]\n    },\n    \"ReturnCombinedData\": {\n      \"Type\": \"Pass\",\n      \"Parameters\": {\n        \"comment\": \"Combining the result\",\n        \"UserDetails\": {\n          \"Name.$\": \"$[0].Name\",\n          \"Address.$\": \"$[1].Address\",\n          \"StaticMessage\": \"This is a static message\"\n        }\n      },\n      \"End\": true\n    }\n  }\n}\n</code></pre> <p>From the previous state machine we would get the following data from the Parallel state:</p> <pre><code>[\n  {\n    \"Name\": {\n      \"fname\": \"Jack\",\n      \"lname\": \"Johnson\"\n    }\n  },\n  {\n    \"Address\": {\n      \"Number\": \"123\",\n      \"Street\": \"Fake St\",\n      \"Zip\": \"90210\"\n    }\n  }\n],\n</code></pre> <p>In the state that comes after the parallel state, which I used a Pass state in this example, I used the Parameters field to combine the user information out from the array and into an object. In the Parameters field, setting the key value pair requires the following format:</p> <pre><code>\"Name.$\": \"$[0].Name\"\n</code></pre> <p>Since this is an array, you would require to select the appropriate location within the array that has the data you need. Since we are setting the name, the data that has the name is in the first position <code>\"$[0]\"</code> since the name operation was done on the first branch of the array. For the address, it is in position <code>\"$[1]\"</code> of the array. In the above state machine, the final successful output would look like the following:</p> <pre><code>{\n  \"comment\": \"Combining the result\",\n  \"UserDetails\": {\n    \"StaticMessage\": \"This is a static message\",\n    \"Address\": {\n      \"Number\": \"123\",\n      \"Street\": \"Fake St\",\n      \"Zip\": \"90210\"\n    },\n    \"Name\": {\n      \"fname\": \"Jack\",\n      \"lname\": \"Johnson\"\n    }\n  }\n}\n</code></pre> Use ResultSelector State Machine<pre><code>{\n  \"StartAt\": \"ParallelBranch\",\n  \"States\": {\n    \"ParallelBranch\": {\n      \"Type\": \"Parallel\",\n      \"ResultPath\": \"$\",\n      \"InputPath\": \"$\",\n      \"OutputPath\": \"$\",\n      \"ResultSelector\": {\n        \"UsersResult.$\": \"$[1].UsersUpload\",\n        \"CustomersResult.$\": \"$[0].customersDataUpload\"\n      },\n      \"Branches\": [\n        {\n          \"StartAt\": \"customersDataUpload\",\n          \"States\": {\n            \"customersDataUpload\": {\n              \"Type\": \"Pass\",\n              \"ResultPath\": \"$.customersDataUpload.Output\",\n              \"Result\": {\n                \"CompletionStatus\": \"success\",\n                \"CompletionDetails\": null\n              },\n              \"Next\": \"Wait2\"\n            },\n            \"Wait2\": {\n              \"Comment\": \"A Wait state delays the state machine from continuing for a specified time.\",\n              \"Type\": \"Wait\",\n              \"Seconds\": 2,\n              \"End\": true\n            }\n          }\n        },\n        {\n          \"StartAt\": \"UsersUpload\",\n          \"States\": {\n            \"UsersUpload\": {\n              \"Type\": \"Pass\",\n              \"Result\": {\n                \"CompletionStatus\": \"success\",\n                \"CompletionDetails\": null\n              },\n              \"ResultPath\": \"$.UsersUpload.Output\",\n              \"Next\": \"Wait1\"\n            },\n            \"Wait1\": {\n              \"Comment\": \"A Wait state delays the state machine from continuing for a specified time.\",\n              \"Type\": \"Wait\",\n              \"Seconds\": 1,\n              \"End\": true\n            }\n          }\n        }\n      ],\n      \"End\": true\n    }\n  },\n  \"TimeoutSeconds\": 129600,\n  \"Version\": \"1.0\"\n}\n</code></pre> <p>And the output will be like:</p> <pre><code>{\n  \"UsersResult\": {\n    \"Output\": {\n      \"CompletionStatus\": \"success\",\n      \"CompletionDetails\": null\n    }\n  },\n  \"CustomersResult\": {\n    \"Output\": {\n      \"CompletionStatus\": \"success\",\n      \"CompletionDetails\": null\n    }\n  }\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-combine-data-parallel-state/#references","title":"References","text":"<ul> <li>AWS Step Functions \u2014 Combine Data from Parallel State</li> <li>Parallel States Merge the output in Step Function</li> </ul>"},{"location":"services/aws/step_functions/stfn-getting-started/","title":"Getting Started","text":""},{"location":"services/aws/step_functions/stfn-getting-started/#orchestration","title":"Orchestration","text":"<p> Invoke AWS Step Functions from other services</p>"},{"location":"services/aws/step_functions/stfn-getting-started/#scheduler","title":"Scheduler","text":"Amazon EventBridge Scheduler <p>To solve this challenge, you can run a serverless workflow on a time-based schedule. Amazon EventBridge is a serverless event bus that helps you receive, filter, transform, route, and deliver events from AWS services, your own applications, and software-as-a-service (SaaS) applications. Many AWS services generate events that EventBridge receives. Amazon EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks at scale from one central, managed service. With AWS Step Functions, you can define state machines that describe your workflow as a series of steps, their relationships, and their inputs and outputs.</p> <p> Schedule a Serverless Workflow with AWS Step Functions and Amazon EventBridge Scheduler</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/","title":"State Machine Language","text":"<p>Warning</p> <p>This topic copy information from Amazon States Language<sup>1</sup>. I think this knowledge is powerfull to reuse this concept for create any state declarative object in the Data Pipeline workflow.</p> <p>Quote</p> <p>A State Machine is represented by a JSON Object.</p> Example: Hello World<pre><code>{\n  \"Comment\": \"A simple minimal example of the States language\",\n  \"StartAt\": \"Hello World\",\n  \"States\": {\n    \"Hello World\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorld\",\n      \"End\": true\n    }\n  }\n}\n</code></pre> <p>Top-level fields:</p> <ul> <li>A State Machine MUST have an object field named \"States\", whose fields represent   the states.</li> <li>A State Machine MUST have a string field named \"StartAt\", whose value MUST exactly   match one of names of the \"States\" fields. The interpreter starts running the   machine at the named state.</li> <li>A State Machine MAY have a string field named \"Comment\", provided for human-readable   description of the machine.</li> <li>A State Machine MAY have a string field named \"Version\", which gives the version   of the States language used in the machine. This document describes version 1.0,   and if omitted, the default value of \"Version\" is the string \"1.0\".</li> <li>A State Machine MAY have an integer field named \"TimeoutSeconds\". If provided,   it provides the maximum number of seconds the machine is allowed to run. If the   machine runs longer than the specified time, then the interpreter fails the   machine with a States.Timeout Error Name.</li> </ul>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#concepts","title":"Concepts","text":""},{"location":"services/aws/step_functions/stfn-state-machine-language/#states","title":"States","text":"<p>States are represented as fields of the top-level \"States\" object. The state name, whose length MUST BE less than or equal to 80 Unicode characters, is the field name; state names MUST be unique within the scope of the whole state machine. States describe tasks (units of work), or specify flow control (e.g. Choice).</p> <p>Here is an example state that executes a Lambda function:</p> <pre><code>\"HelloWorld\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloWorld\",\n  \"Next\": \"NextState\",\n  \"Comment\": \"Executes the HelloWorld Lambda function\"\n}\n</code></pre> <p>Note</p> <ul> <li> <p>All states MUST have a \"Type\" field. This document refers to the values   of this field as a state's type, and to a state such as the one in the   example above as a Task State.</p> </li> <li> <p>Any state MAY have a \"Comment\" field, to hold a human-readable comment or   description.</p> </li> <li> <p>Most state types require additional fields as specified in this document.</p> </li> <li> <p>Any state except for Choice, Succeed, and Fail MAY have a field named \"End\"   whose value MUST be a boolean. The term \"Terminal State\" means a state with   with <code>{ \"End\": true }</code>, or a state with <code>{ \"Type\": \"Succeed\" }</code>, or a state with   <code>{ \"Type\": \"Fail\" }</code>.</p> </li> </ul>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#transitions","title":"Transitions","text":"<p>Transitions link states together, defining the control flow for the state machine. After executing a non-terminal state, the interpreter follows a transition to the next state. For most state types, transitions are unconditional and specified through the state's <code>\"Next\"</code> field.</p> <p>All non-terminal states MUST have a <code>\"Next\"</code> field, except for the Choice State. The value of the <code>\"Next\"</code> field MUST exactly and case-sensitively match the name of the another state.</p> <p>States can have multiple incoming transitions from other states.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#timestamps","title":"Timestamps","text":"<p>The Choice and Wait States deal with JSON field values which represent timestamps. These are strings which MUST conform to the RFC3339 profile of ISO 8601, with the further restrictions that an uppercase <code>\"T\"</code> character MUST be used to separate date and time, and an uppercase <code>\"Z\"</code> character MUST be present in the absence of a numeric time zone offset, for example <code>\"2016-03-14T01:59:00Z\"</code>.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#data","title":"Data","text":"<p>The interpreter passes data between states to perform calculations or to dynamically control the state machine's flow. All such data MUST be expressed in JSON.</p> <p>When a state machine is started, the caller can provide an initial JSON text as input, which is passed to the machine's start state as input. If no input is provided, the default is an empty JSON object, <code>{}</code>. As each state is executed, it receives a JSON text as input and can produce arbitrary output, which MUST be a JSON text. When two states are linked by a transition, the output from the first state is passed as input to the second state. The output from the machine's terminal state is treated as its output.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#the-context-object","title":"The Context Object","text":"<p>The interpreter can provide information to an executing state machine about the execution and other implementation details. This is delivered in the form of a JSON object called the \"Context Object\". This version of the States Language specification does not specify any contents of the Context Object.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#paths","title":"Paths","text":"<p>A Path is a string, beginning with \"$\", used to identify components with a JSON text. The syntax is that of JsonPath.</p> <p>When a Path begins with \"$$\", two dollar signs, this signals that it is intended to identify content within the Context Object. The first dollar sign is stripped, and the remaining text, which begins with a dollar sign, is interpreted as the JSONPath applying to the Context Object.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#reference-paths","title":"Reference Paths","text":"<p>A Reference Path is a Path with syntax limited in such a way that it can only identify a single node in a JSON structure: The operators \"@\", \",\", \":\", and \"?\" are not supported - all Reference Paths MUST be unambiguous references to a single value, array, or object (subtree).</p> <p>For example, if state input data contained the values:</p> <pre><code>{\n  \"foo\": 123,\n  \"bar\": [\"a\", \"b\", \"c\"],\n  \"car\": {\n      \"cdr\": true\n  }\n}\n</code></pre> <p>Then the following Reference Paths would return:</p> <pre><code>$.foo =&gt; 123\n$.bar =&gt; [\"a\", \"b\", \"c\"]\n$.car.cdr =&gt; true\n</code></pre> <p>Paths and Reference Paths are used by certain states, as specified later in this document, to control the flow of a state machine or to configure a state's settings or options.</p> <p>Here are some examples of acceptable Reference Path syntax:</p> <pre><code>$.store.book\n$.store\\.book\n$.\\stor\\e.boo\\k\n$.store.book.title\n$.foo.\\.bar\n$.foo\\@bar.baz\\[\\[.\\?pretty\n$.&amp;\u0416\u4e2d.\\uD800\\uDF46\n$.ledgers.branch[0].pending.count\n$.ledgers.branch[0]\n$.ledgers[0][22][315].foo\n$['store']['book']\n$['store'][0]['book']\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#payload-template","title":"Payload Template","text":"<p>A state machine interpreter dispatches data as input to tasks to do useful work, and receives output back from them. It is frequently desired to reshape input data to meet the format expectations of tasks, and similarly to reshape the output coming back. A JSON object structure called a Payload Template is provided for this purpose.</p> <p>In the Task, Map, Parallel, and Pass States, the Payload Template is the value of a field named \"Parameters\". In the Task, Map, and Parallel States, there is another Payload Template which is the value of a field named \"ResultSelector\".</p> <p>A Payload Template MUST be a JSON object; it has no required fields. The interpreter processes the Payload Template as described in this section; the result of that processing is called the payload.</p> <p>To illustrate by example, the Task State has a field named \"Parameters\" whose value is a Payload Template. Consider the following Task State:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"first\": 88,\n    \"second\": 99\n  }\n}\n</code></pre> <p>In this case, the payload is the object with \"first\" and \"second\" fields whose values are respectively 88 and 99. No processing needs to be performed and the payload is identical to the Payload Template.</p> <p>Values from the Payload Template's input and the Context Object can be inserted into the payload with a combination of a field-naming convention, Paths and Intrinsic Functions.</p> <p>If any field within the Payload Template (however deeply nested) has a name ending with the characters \".\\(\", its value is transformed according to rules below and the field is renamed to strip the \".\\)\" suffix.</p> <p>If the field value begins with only one \"$\", the value MUST be a Path. In this case, the Path is applied to the Payload Template's input and is the new field value.</p> <p>If the field value begins with \"$$\", the first dollar sign is stripped and the remainder MUST be a Path. In this case, the Path is applied to the Context Object and is the new field value.</p> <p>If the field value does not begin with \"$\", it MUST be an Intrinsic Function (see below). The interpreter invokes the Intrinsic Function and the result is the new field value.</p> <p>If the path is legal but cannot be applied successfully, the interpreter fails the machine execution with an Error Name of \"States.ParameterPathFailure\". If the Intrinsic Function fails during evaluation, the interpreter fails the machine execution with an Error Name of \"States.IntrinsicFailure\".</p> <p>A JSON object MUST NOT have duplicate field names after fields ending with the characters \".\\(\" are renamed to strip the \".\\)\" suffix.</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"flagged\": true,\n    \"parts\": {\n      \"first.$\": \"$.vals[0]\",\n      \"last3.$\": \"$.vals[-3:]\"\n    },\n    \"weekday.$\": \"$$.DayOfWeek\",\n    \"formattedOutput.$\": \"States.Format('Today is {}', $$.DayOfWeek)\"\n  }\n}\n</code></pre> <p>Suppose that the input to the P is as follows:</p> <pre><code>{\n  \"flagged\": 7,\n  \"vals\": [0, 10, 20, 30, 40, 50]\n}\n</code></pre> <p>Further, suppose that the Context Object is as follows:</p> <pre><code>{\n  \"DayOfWeek\": \"TUESDAY\"\n}\n</code></pre> <p>In this case, the effective input to the code identified in the \"Resource\" field would be as follows:</p> <pre><code>{\n  \"flagged\": true,\n  \"parts\": {\n    \"first\": 0,\n    \"last3\": [30, 40, 50]\n  },\n  \"weekday\": \"TUESDAY\",\n  \"formattedOutput\": \"Today is TUESDAY\"\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#intrinsic-functions","title":"Intrinsic Functions","text":"<p>The States Language provides a small number of \"Intrinsic Functions\", constructs which look like functions in programming languages and can be used to help Payload Templates process the data going to and from Task Resources. See Appendix B for a full list of Intrinsic Functions</p> <p>Here is an example of an Intrinsic Function named \"States.Format\" being used to prepare data:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Parameters\": {\n    \"greeting.$\": \"States.Format('Welcome to {} {}\\\\'s playlist.', $.firstName, $.lastName)\"\n  }\n}\n</code></pre> <p>Note</p> <ul> <li>An Intrinsic Function MUST be a string.</li> <li>The Intrinsic Function MUST begin with an Intrinsic Function name. An Intrinsic Function name MUST contain only the characters A through Z, a through z, 0 through 9, \".\", and \"_\".</li> <li>All Intrinsic Functions defined by this specification have names that begin with \"States.\". Other implementations may define their own Intrinsic Functions whose names MUST NOT begin with \"States.\".</li> <li>The Intrinsic Function name MUST be followed immediately by a list of zero or more arguments, enclosed by \"(\" and \")\", and separated by commas.</li> <li>Intrinsic Function arguments may be strings enclosed by apostrophe (<code>'</code>) characters, numbers, <code>null</code>, Paths, or nested Intrinsic Functions.</li> <li>The value of a string, number or <code>null</code> argument is the argument itself. The value of an argument which is a Path is the result of applying it to the input of the Payload Template. The value of an argument which is an Intrinsic Function is the result of the function invocation.\"</li> </ul> <p>Note that in the example above, the first argument of <code>States.Format</code> could have been a Path that yielded the formatting template string.</p> <ul> <li>The following characters are reserved for all Intrinsic Functions and MUST be escaped: ' { } \\</li> </ul> <p>If any of the reserved characters needs to appear as part of the value without serving as a reserved character, it MUST be escaped with a backslash.</p> <p>If the character \" needs to appear as part of the value without serving as an escape character, it MUST be escaped with a backslash.</p> <p>The literal string <code>\\'</code> represents <code>'</code>.   The literal string <code>\\{</code> represents <code>{</code>.   The literal string <code>\\}</code> represents <code>}</code>.   The literal string <code>\\\\</code> represents <code>\\</code>.</p> <p>In JSON, all backslashes contained in a string literal value must be escaped   with another backslash, therefore, the above will equate to:</p> <p>The escaped string <code>\\\\'</code> represents <code>'</code>.   The escaped string <code>\\\\{</code> represents <code>{</code>.   The escaped string <code>\\\\}</code> represents <code>}</code>.   The escaped string <code>\\\\\\\\</code> represents <code>\\</code>.</p> <p>If an open escape backslash <code>\\</code> is found in the Intrinsic Function, the   interpreter will throw a runtime error.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#input-and-output-processing","title":"Input and Output Processing","text":"<p>As described above, data is passed between states as JSON texts. However, a state may want to process only a subset of its input data, and may want that data structured differently from the way it appears in the input. Similarly, it may want to control the format and content of the data that it passes on as output.</p> <p>Fields named \"InputPath\", \"Parameters\", \"ResultSelector\", \"ResultPath\", and \"OutputPath\" exist to support this.</p> <p>Any state except for the Fail and Succeed States MAY have \"InputPath\" and \"OutputPath\".</p> <p>States which potentially generate results MAY have \"Parameters\", \"ResultSelector\" and \"ResultPath\": Task State, Parallel State, and Map State.</p> <p>Pass State MAY have \"Parameters\" and \"ResultPath\" to control its output value.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#using-inputpath-parameters-resultselector-resultpath-and-outputpath","title":"Using InputPath, Parameters, ResultSelector, ResultPath and OutputPath","text":"<p>In this discussion, \"raw input\" means the JSON text that is the input to a state. \"Result\" means the JSON text that a state generates, for example from external code invoked by a Task State, the combined result of the branches in a Parallel or Map State, or the Value of the \"Result\" field in a Pass State. \"Effective input\" means the input after the application of InputPath and Parameters, \"effective result\" means the result after processing it with ResultSelector and \"effective output\" means the final state output after processing the result with ResultSelector, ResultPath and OutputPath.</p> <ul> <li>The value of \"InputPath\" MUST be a Path, which is applied to a State's raw input   to select some or all of it; that selection is used by the state, for example   in passing to Resources in Task States and Choices selectors in Choice States.</li> <li>The value of \"Parameters\" MUST be a Payload Template which is a JSON object,   whose input is the result of applying the InputPath to the raw input. If the   \"Parameters\" field is provided, its payload, after the extraction and embedding,   becomes the effective input.</li> <li>The value of \"ResultSelector\" MUST be a Payload Template, whose input is the   result, and whose payload replaces and becomes the effective result.</li> <li>The value of \"ResultPath\" MUST be a Reference Path, which specifies the raw input's   combination with or replacement by the state's result.</li> <li>The value of \"ResultPath\" MUST NOT begin with \"$$\"; i.e. it may not be used to   insert content into the Context Object.</li> <li>The value of \"OutputPath\" MUST be a Path, which is applied to the state's output   after the application of ResultPath, producing the effective output which serves   as the raw input for the next state.</li> </ul> <p>Note that JsonPath can yield multiple values when applied to an input JSON text. For example, given the text:</p> <pre><code>{ \"a\": [1, 2, 3, 4] }\n</code></pre> <p>Then if the JsonPath <code>$.a[0,1]</code> is applied, the result will be two JSON texts, <code>1</code> and <code>2</code>. When this happens, to produce the effective input, the interpreter gathers the texts into an array, so in this example the state would see the input:</p> <pre><code>[ 1, 2 ]\n</code></pre> <p>The same rule applies to OutputPath processing; if the OutputPath result contains multiple values, the effective output is a JSON array containing all of them.</p> <p>The \"ResultPath\" field's value is a Reference Path that specifies where to place the result, relative to the raw input. If the raw input has a field at the location addressed by the ResultPath value then in the output that field is discarded and overwritten by the state's result. Otherwise, a new field is created in the state output, with intervening fields constructed as necessary. For example, given the raw input:</p> <pre><code>{\n  \"master\": {\n    \"detail\": [1, 2, 3]\n  }\n}\n</code></pre> <p>If the state's result is the number <code>6</code>, and the \"ResultPath\" is <code>$.master.detail</code>, then in the output the <code>detail</code> field would be overwritten:</p> <pre><code>{\n  \"master\": {\n    \"detail\": 6\n  }\n}\n</code></pre> <p>If instead a \"ResultPath\" of <code>$.master.result.sum</code> was used then the result would be combined with the raw input, producing a chain of new fields containing <code>result</code> and <code>sum</code>:</p> <pre><code>{\n  \"master\": {\n    \"detail\": [1, 2, 3],\n    \"result\": {\n      \"sum\": 6\n    }\n  }\n}\n</code></pre> <p>If the value of InputPath is <code>null</code>, that means that the raw input is discarded, and the effective input for the state is an empty JSON object, <code>{}</code>. Note that having a value of <code>null</code> is different from the \"InputPath\" field being absent.</p> <p>If the value of ResultPath is <code>null</code>, that means that the state's result is discarded and its raw input becomes its result.</p> <p>If the value of OutputPath is <code>null</code>, that means the input and result are discarded, and the effective output from the state is an empty JSON object, <code>{}</code>.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#defaults","title":"Defaults","text":"<p>Each of InputPath, Parameters, ResultSelector, ResultPath, and OutputPath are optional. The default value of InputPath is \"\\(\", so by default the effective input is just the raw input. The default value of ResultPath is \"\\)\", so by default a state's result overwrites and replaces the input. The default value of OutputPath is \"$\", so by default a state's effective output is the result of processing ResultPath.</p> <p>Parameters and ResultSelector have no default value. If absent, they have no effect on their input.</p> <p>Therefore, if none of InputPath, Parameters, ResultSelector, ResultPath, or OutputPath are supplied, a state consumes the raw input as provided and passes its result to the next state.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#inputoutput-processing-examples","title":"Input/Output Processing Examples","text":"<p>Consider the example given above, of a Lambda task that sums a pair of numbers. As presented, its input is: <code>{ \"val1\": 3, \"val2\": 4 }</code> and its output is: <code>7</code>.</p> <p>Suppose the input is little more complex:</p> <pre><code>{\n  \"title\": \"Numbers to add\",\n  \"numbers\": { \"val1\": 3, \"val2\": 4 }\n}\n</code></pre> <p>Then suppose we modify the state definition by adding:</p> <pre><code>\"InputPath\": \"$.numbers\",\n\"ResultPath\": \"$.sum\"\n</code></pre> <p>And finally,suppose we simplify Line 4 of the Lambda function to read as follows: <code>return JSON.stringify(total)</code>. This is probably a better form of the function, which should really only care about doing math and not care how its result is labeled.</p> <p>In this case, the output would be:</p> <pre><code>{\n  \"title\": \"Numbers to add\",\n  \"numbers\": { \"val1\": 3, \"val2\": 4 },\n  \"sum\": 7\n}\n</code></pre> <p>The interpreter might need to construct multiple levels of JSON object to achieve the desired effect. Suppose the input to some Task State is:</p> <pre><code>{ \"a\": 1 }\n</code></pre> <p>Suppose the output from the Task is \"Hi!\", and the value of the \"ResultPath\" field is \"$.b.greeting\". Then the output from the state would be:</p> <pre><code>{\n  \"a\": 1,\n  \"b\": {\n    \"greeting\": \"Hi!\"\n  }\n}\n</code></pre>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#runtime-errors","title":"Runtime Errors","text":"<p>Suppose a state's input is the string <code>\"foo\"</code>, and its \"ResultPath\" field has the value \"$.x\". Then ResultPath cannot apply and the interpreter fails the machine with an Error Name of \"States.ResultPathMatchFailure\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#errors","title":"Errors","text":"<p>Any state can encounter runtime errors. Errors can arise because of state machine definition issues (e.g. the \"ResultPath\" problem discussed immediately above), task failures (e.g. an exception thrown by a Lambda function) or because of transient issues, such as network partition events.</p> <p>When a state reports an error, the default course of action for the interpreter is to fail the whole state machine.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#error-representation","title":"Error representation","text":"<p>Errors are identified by case-sensitive strings, called Error Names. The States language defines a set of built-in strings naming well-known errors, all of which begin with the prefix \"States.\"; see Appendix A.</p> <p>States MAY report errors with other names, which MUST NOT begin with the prefix \"States.\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#retrying-after-error","title":"Retrying after error","text":"<p>Task States, Parallel States, and Map States MAY have a field named \"Retry\", whose value MUST be an array of objects, called Retriers.</p> <p>Each Retrier MUST contain a field named \"ErrorEquals\" whose value MUST be a non-empty array of Strings, which match Error Names.</p> <p>When a state reports an error, the interpreter scans through the Retriers and, when the Error Name appears in the value of a Retrier's \"ErrorEquals\" field, implements the retry policy described in that Retrier.</p> <p>An individual Retrier represents a certain number of retries, usually at increasing time intervals.</p> <p>A Retrier MAY contain a field named \"IntervalSeconds\", whose value MUST be a positive integer, representing the number of seconds before the first retry attempt (default value: 1); a field named \"MaxAttempts\" whose value MUST be a non-negative integer, representing the maximum number of retry attempts (default: 3); a field named \"MaxDelaySeconds\" whose value MUST be positive integer, representing the maximum interval in seconds to wait before the next retry attempt; a field named \"JitterStrategy\", whose value MUST be a string, representing the jitter strategy to use in the retry interval calculation; and a field named \"BackoffRate\", a number which is the multiplier that increases the retry interval on each attempt (default: 2.0). The value of BackoffRate MUST be greater than or equal to 1.0.</p> <p>Note that a \"MaxAttempts\" field whose value is 0 is legal, specifying that some error or errors should never be retried.</p> <p>The States Language does not constrain the value of the \"JitterStrategy\" field. The interpreter will use the specified value to select the jitter strategy to use when calculating the retry interval.</p> <p>Here is an example of a Retrier which will make 2 retry attempts after waits of <code>3</code> and <code>6</code> seconds:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 3,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0\n  }\n]\n</code></pre> <p>Here is the same example, but with a maximum limit of 4 seconds on the retry interval. In this case the Retrier will make the second attempt after 4 seconds, rather than 6 seconds:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 3,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0,\n    \"MaxDelaySeconds\": 4\n  }\n]\n</code></pre> <p>Here is an example of a Retrier which uses an interpreter-defined JitterStrategy called \"SAMPLE\". Instead of waiting for 2 seconds before the first retry attempt or 4 seconds before the second retry attempt, the interpreter will modify these values according to the jitter strategy:</p> <pre><code>\"Retry\" : [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"IntervalSeconds\": 2,\n    \"MaxAttempts\": 2,\n    \"BackoffRate\": 2.0,\n    \"JitterStrategy\": \"SAMPLE\"\n  }\n]\n</code></pre> <p>The reserved name \"States.ALL\" in a Retrier's \"ErrorEquals\" field is a wildcard and matches any Error Name. Such a value MUST appear alone in the \"ErrorEquals\" array and MUST appear in the last Retrier in the \"Retry\" array.</p> <p>Here is an example of a Retrier which will retry any error except for \"States.Timeout\", using the default retry parameters.</p> <pre><code>\"Retry\": [\n  {\n    \"ErrorEquals\": [ \"States.Timeout\" ],\n    \"MaxAttempts\": 0\n  },\n  {\n    \"ErrorEquals\": [ \"States.ALL\" ]\n  }\n]\n</code></pre> <p>If the error recurs more times than allowed for by the \"MaxAttempts\" field, retries cease and normal error handling resumes.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#complex-retry-scenarios","title":"Complex retry scenarios","text":"<p>A Retrier's parameters apply across all visits to that Retrier in the context of a single state execution. This is best illustrated by example; consider the following Task State:</p> <pre><code>\"X\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:X\",\n  \"Next\": \"Y\",\n  \"Retry\": [\n    {\n      \"ErrorEquals\": [ \"ErrorA\", \"ErrorB\" ],\n      \"IntervalSeconds\": 1,\n      \"BackoffRate\": 2,\n      \"MaxAttempts\": 2\n    },\n    {\n      \"ErrorEquals\": [ \"ErrorC\" ],\n      \"IntervalSeconds\": 5\n    }\n  ],\n  \"Catch\": [\n    {\n      \"ErrorEquals\": [ \"States.ALL\" ],\n      \"Next\": \"Z\"\n    }\n  ]\n}\n</code></pre> <p>Suppose that this task fails four successive times, throwing Error Names \"ErrorA\", \"ErrorB\", \"ErrorC\", and \"ErrorB\". The first two errors match the first retrier and cause waits of one and two seconds. The third error matches the second retrier and causes a wait of five seconds. The fourth error would match the first retrier but its \"MaxAttempts\" ceiling of two retries has already been reached, so that Retrier fails, and execution is redirected to the \"Z\" state via the \"Catch\" field.</p> <p>Note that once the interpreter transitions to another state in any way, all the Retrier parameters reset.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#fallback-states","title":"Fallback states","text":"<p>Task States, Parallel States, and Map States MAY have a field named \"Catch\", whose value MUST be an array of objects, called Catchers.</p> <p>Each Catcher MUST contain a field named \"ErrorEquals\", specified exactly as with the Retrier \"ErrorEquals\" field, and a field named \"Next\" whose value MUST be a string exactly matching a State Name.</p> <p>When a state reports an error and either there is no Retrier, or retries have failed to resolve the error, the interpreter scans through the Catchers in array order, and when the Error Name appears in the value of a Catcher's \"ErrorEquals\" field, transitions the machine to the state named in the value of the \"Next\" field.</p> <p>The reserved name \"States.ALL\" appearing in a Retrier's \"ErrorEquals\" field is a wildcard and matches any Error Name. Such a value MUST appear alone in the \"ErrorEquals\" array and MUST appear in the last Catcher in the \"Catch\" array.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#error-output","title":"Error output","text":"<p>When a state reports an error and it matches a Catcher, causing a transfer to another state, the state's result (and thus the input to the state identified in the Catcher's \"Next\" field) is a JSON object, called the Error Output. The Error Output MUST have a string-valued field named \"Error\", containing the Error Name. It SHOULD have a string-valued field named \"Cause\", containing human-readable text about the error.</p> <p>A Catcher MAY have an \"ResultPath\" field, which works exactly like a state's top-level \"ResultPath\", and may be used to inject the Error Output into the state's original input to create the input for the Catcher's \"Next\" state. The default value, if the \"ResultPath\" field is not provided, is \"$\", meaning that the output consists entirely of the Error Output.</p> <p>Here is an example of a Catcher that will transition to the state named \"RecoveryState\" when a Lambda function throws an unhandled Java Exception, and otherwise to the \"EndMachine\" state, which is presumably Terminal.</p> <p>Also in this example, if the first Catcher matches the Error Name, the input to \"RecoveryState\" will be the original state input, with the Error Output as the value of the top-level \"error-info\" field. For any other error, the input to \"EndMachine\" will just be the Error Output.</p> <pre><code>\"Catch\": [\n  {\n    \"ErrorEquals\": [ \"java.lang.Exception\" ],\n    \"ResultPath\": \"$.error-info\",\n    \"Next\": \"RecoveryState\"\n  },\n  {\n    \"ErrorEquals\": [ \"States.ALL\" ],\n    \"Next\": \"EndMachine\"\n  }\n]\n</code></pre> <p>Each Catcher can specific multiple errors to handle.</p> <p>When a state has both \"Retry\" and \"Catch\" fields, the interpreter uses any appropriate Retriers first and only applies the matching Catcher transition if the retry policy fails to resolve the error.</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#state-types","title":"State Types","text":"<p>As a reminder, the state type is given by the value of the \"Type\" field, which MUST appear in every State object.</p> <p></p> Pass StateTask StateChoice StateWait StateSucceed StateFail StateParallel StateMap State <p>The Pass State (identified by <code>\"Type\":\"Pass\"</code>) by default passes its input to its output, performing no work.</p> <p>A Pass State MAY have a field named \"Result\". If present, its value is treated as the output of a virtual task, and placed as prescribed by the \"ResultPath\" field, if any, to be passed on to the next state. If \"Result\" is not provided, the output is the input. Thus if neither \"Result\" nor \"ResultPath\" are provided, the Pass State copies its input through to its output.</p> <p>Here is an example of a Pass State that injects some fixed data into the state machine, probably for testing purposes.</p> <pre><code>\"No-op\": {\n  \"Type\": \"Pass\",\n  \"Result\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  },\n  \"ResultPath\": \"$.coords\",\n  \"Next\": \"End\"\n}\n</code></pre> InputOutput <pre><code>{\n  \"georefOf\": \"Home\"\n}\n</code></pre> <pre><code>{\n  \"georefOf\": \"Home\",\n  \"coords\": {\n    \"x-datum\": 0.381018,\n    \"y-datum\": 622.2269926397355\n  }\n}\n</code></pre> <p>The Task State (identified by <code>\"Type\":\"Task\"</code>) causes the interpreter to execute the work identified by the state's \"Resource\" field.</p> <p>Here is an example:</p> <pre><code>\"TaskState\": {\n  \"Comment\": \"Task State example\",\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:us-east-1:123456789012:task:HelloWorld\",\n  \"Next\": \"NextState\",\n  \"TimeoutSeconds\": 300,\n  \"HeartbeatSeconds\": 60\n}\n</code></pre> <p>A Task State MUST include a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI.</p> <p>A Task State MAY include a \"Parameters\" field, whose value MUST be a Payload Template. A Task State MAY include a \"ResultSelector\" field, whose value MUST be a Payload Template.</p> <p>Tasks can optionally specify timeouts. Timeouts (the \"TimeoutSeconds\" and \"HeartbeatSeconds\" fields) are specified in seconds and MUST be positive integers.</p> <p>Both the total and heartbeat timeouts can be provided indirectly. A Task State may have \"TimeoutSecondsPath\" and \"HeartbeatSecondsPath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are positive integers. A Task State MUST NOT include both \"TimeoutSeconds\" and \"TimeoutSecondsPath\" or both \"HeartbeatSeconds\" and \"HeartbeatSecondsPath\".</p> <p>If provided, the \"HeartbeatSeconds\" interval MUST be smaller than the \"TimeoutSeconds\" value.</p> <p>If not provided, the default value of \"TimeoutSeconds\" is 60.</p> <p>If the state runs longer than the specified timeout, or if more time than the specified heartbeat elapses between heartbeats from the task, then the interpreter fails the state with a States.Timeout Error Name.</p> <p>Note</p> <p>A Task State MAY include a \"Credentials\" field, whose value MUST be a JSON object whose value is defined by the interpreter. The States language does not constrain the value of the \"Credentials\" field. The interpreter will use the specified credentials to execute the work identified by the state's \"Resource\" field.</p> <p>A Choice State (identified by <code>\"Type\":\"Choice\"</code>) adds branching logic to a state machine.</p> <p>A Choice State MUST have a \"Choices\" field whose value is a non-empty array. Each element of the array MUST be a JSON object and is called a Choice Rule. A Choice Rule may be evaluated to return a boolean value. A Choice Rule at the top level, i.e. which is a member of the \"Choices\" array, MUST have a \"Next\" field, whose value MUST match a state name.</p> <p>The interpreter attempts pattern-matches against the top-level Choice Rules in array order and transitions to the state specified in the \"Next\" field on the first Choice Rule where there is an exact match between the input value and a member of the comparison-operator array.</p> <p>Here is an example of a Choice State.</p> <pre><code>\"DispatchEvent\": {\n  \"Type\" : \"Choice\",\n  \"Choices\": [\n    {\n      \"Not\": {\n        \"Variable\": \"$.type\",\n        \"StringEquals\": \"Private\"\n      },\n      \"Next\": \"Public\"\n    },\n    {\n      \"And\": [\n        {\n          \"Variable\": \"$.value\",\n          \"IsPresent\": true\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"IsNumeric\": true\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"NumericGreaterThanEquals\": 20\n        },\n        {\n          \"Variable\": \"$.value\",\n          \"NumericLessThan\": 30\n        }\n      ],\n      \"Next\": \"ValueInTwenties\"\n    },\n    {\n      \"Variable\": \"$.rating\",\n      \"NumericGreaterThanPath\": \"$.auditThreshold\",\n      \"Next\": \"StartAudit\"\n    }\n  ],\n  \"Default\": \"RecordEvent\"\n}\n</code></pre> <p>In this example, suppose the machine is started with an input value of:</p> <pre><code>{\n  \"type\": \"Private\",\n  \"value\": 22\n}\n</code></pre> <p>Then the interpreter will transition to the \"ValueInTwenties\" state, based on the \"value\" field.</p> <p>A Choice Rule MUST be either a Boolean Expression or a Data-test Expression.</p> Boolean ExpressionData-test Expression <p>A Boolean Expression is a JSON object which contains a field named \"And\", \"Or\", or \"Not\". If the field name is \"And\" or \"Or\", the value MUST be an non-empty object array of Choice Rules, which MUST NOT contain \"Next\" fields; the interpreter processes the array elements in order, performing the boolean evaluations in the expected fashion, ceasing array processing when the boolean value has been unambiguously determined.</p> <p>The value of a Boolean Expression containing a \"Not\" field MUST be a single Choice Rule, that MUST NOT contain a \"Next\" field; it returns the inverse of the boolean to which the Choice Rule evaluates.</p> <p>A Data-test Expression Choice Rule is an assertion about a field and its value which yields a boolean depending on the data. A Data-test Expression MUST contain a field named \"Variable\" whose value MUST be a Path.</p> <p>Each choice rule MUST contain exactly one field containing a comparison operator. The following comparison operators are supported:</p> Lists <ul> <li>StringEquals, StringEqualsPath</li> <li>StringLessThan, StringLessThanPath</li> <li>StringGreaterThan, StringGreaterThanPath</li> <li>StringLessThanEquals, StringLessThanEqualsPath</li> <li>StringGreaterThanEquals, StringGreaterThanEqualsPath</li> <li> <p>StringMatches</p> <p>Note: The value MUST be a String which MAY contain one or more \"\" characters. The expression yields true if the data value selected by the Variable Path matches the value, where \"\" in the value matches zero or more characters. Thus, <code>foo*.log</code> matches <code>foo23.log</code>, <code>*.log</code> matches <code>zebra.log</code>, and <code>foo*.*</code> matches <code>foobar.zebra</code>. No characters other than \"*\" have any special meaning during matching.</p> <p>If the character \"*\" needs to appear as part of the value without serving as a wildcard, it MUST be escaped with a backslash.</p> <p>If the character \" needs to appear as part of the value without serving as an escape character, it MUST be escaped with a backslash.</p> <p>The literal string <code>\\*</code> represents <code>*</code>. The literal string <code>\\\\</code> represents <code>\\</code>.</p> <p>In JSON, all backslashes contained in a string literal value must be escaped with another backslash, therefore, the above will equate to:</p> <p>The escaped string <code>\\\\*</code> represents <code>*</code>. The escaped string <code>\\\\\\\\</code> represents <code>\\</code>.</p> <p>If an open escape backslash  is found in the StringMatches string, the interpreter will throw a runtime error.</p> </li> <li> <p>NumericEquals, NumericEqualsPath</p> </li> <li>NumericLessThan, NumericLessThanPath</li> <li>NumericGreaterThan, NumericGreaterThanPath</li> <li>NumericLessThanEquals, NumericLessThanEqualsPath</li> <li>NumericGreaterThanEquals, NumericGreaterThanEqualsPath</li> <li>BooleanEquals, BooleanEqualsPath</li> <li>TimestampEquals, TimestampEqualsPath</li> <li>TimestampLessThan, TimestampLessThanPath</li> <li>TimestampGreaterThan, TimestampGreaterThanPath</li> <li>TimestampLessThanEquals, TimestampLessThanEqualsPath</li> <li>TimestampGreaterThanEquals, TimestampGreaterThanEqualsPath</li> <li> <p>IsNull</p> <p>Note: This means the value is the built-in JSON literal <code>null</code>.</p> </li> <li> <p>IsPresent</p> <p>Note: In this case, if the Variable-field Path fails to match anything in the input no exception is thrown and the Choice Rule just returns false.</p> </li> <li> <p>IsNumeric</p> </li> <li>IsString</li> <li>IsBoolean</li> <li>IsTimestamp</li> </ul> <p>For those operators that end with \"Path\", the value MUST be a Path, to be applied to the state's effective input to yield a value to be compared with the value yielded by the Variable path.</p> <p>For each operator which compares values, if the values are not both of the appropriate type (String, number, boolean, or Timestamp) the comparison will return false. Note that a field which is thought of as a timestamp could be matched by a string-typed comparator.</p> <p>The various String comparators compare strings character-by-character with no special treatments such as case-folding, white-space collapsing, or Unicode form normalization.</p> <p>Note that for interoperability, numeric comparisons should not be assumed to work with values outside the magnitude or precision representable using the IEEE 754-2008 \"binary64\" data type. In particular, integers outside of the range [-(2^53)+1, (2^53)-1] might fail to compare in the expected way.</p> <p>A Choice State MAY have a \"Default\" field, whose value MUST be a string whose value MUST match a State name; that state will execute if none of the Choice Rules match. The interpreter will raise a runtime States.NoChoiceMatched error if a Choice State fails to match a Choice Rule and no \"Default\" transition was specified.</p> <p>A Choice State MUST NOT be an End state.</p> <p>A Wait State (identified by <code>\"Type\":\"Wait\"</code>) causes the interpreter to delay the machine from continuing for a specified time. The time can be specified as a wait duration, specified in seconds, or an absolute expiry time, specified as an ISO-8601 extended offset date-time format string.</p> <p>For Examples:</p> Wait with secondWait until an absolute timeWait using a Reference Path <pre><code>\"wait_ten_seconds\" : {\n  \"Type\" : \"Wait\",\n  \"Seconds\" : 10,\n  \"Next\": \"NextState\"\n}\n</code></pre> <pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>Reworked to look up the timestamp time using a Reference Path to the data, which might look like <code>{ \"expirydate\": \"2016-03-14T01:59:00Z\" }</code>:</p> <pre><code>\"wait_until\" : {\n  \"Type\": \"Wait\",\n  \"TimestampPath\": \"$.expirydate\",\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>A Wait State MUST contain exactly one of \"Seconds\", \"SecondsPath\", \"Timestamp\", or \"TimestampPath\".</p> <p>The Succeed State (identified by <code>\"Type\":\"Succeed\"</code>) either terminates a state machine successfully, ends a branch of a Parallel State, or ends an iteration of a Map State. The output of a Succeed State is the same as its input, possibly modified by \"InputPath\" and/or \"OutputPath\".</p> <p>The Succeed State is a useful target for Choice-State branches that don't do anything except terminate the machine.</p> <p>Here is an example:</p> <pre><code>\"SuccessState\": {\n  \"Type\": \"Succeed\"\n}\n</code></pre> <p>Because Succeed States are terminal states, they have no \"Next\" field.</p> <p>The Fail State (identified by <code>\"Type\":\"Fail\"</code>) terminates the machine and marks it as a failure.</p> <p>Here is an example:</p> <pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Error\": \"ErrorA\",\n  \"Cause\": \"Kaiju attack\"\n}\n</code></pre> <p>A Fail State MAY have a field named \"Error\", whose value MUST be a string whose value provides an error name that can be used for error handling (Retry/Catch), operational, or diagnostic purposes. A Fail State MAY have a field named \"Cause\", whose value MUST be a string whose value provides a human-readable message.</p> <p>Both the \"Error\" and \"Cause\" can be provided indirectly. A Fail state MAY have \"ErrorPath\" and \"CausePath\" fields whose values MUST be Reference Paths or Intrinsic Functions which, when resolved, MUST be string values. A Fail state MUST NOT include both \"Error\" and \"ErrorPath\" or both \"Cause\" and \"CausePath\".</p> <p>Here is an example that sets the Error and Cause values dynamically from the state input rather than using static values:</p> <pre><code>\"FailState\": {\n  \"Type\": \"Fail\",\n  \"Comment\": \"my error comment\",\n  \"ErrorPath\": \"$.Error\",\n  \"CausePath\": \"$.Cause\"\n}\n</code></pre> <p>Because Fail States are terminal states, they have no \"Next\" field.</p> <p>The Parallel State (identified by <code>\"Type\":\"Parallel\"</code>) causes parallel execution of \"branches\".</p> <p>Here is an example:</p> <pre><code>\"LookupCustomerInfo\": {\n  \"Type\": \"Parallel\",\n  \"Branches\": [\n    {\n      \"StartAt\": \"LookupAddress\",\n      \"States\": {\n        \"LookupAddress\": {\n          \"Type\": \"Task\",\n          \"Resource\":\n            \"arn:aws:lambda:us-east-1:123456789012:function:AddressFinder\",\n          \"End\": true\n        }\n      }\n    },\n    {\n      \"StartAt\": \"LookupPhone\",\n      \"States\": {\n        \"LookupPhone\": {\n          \"Type\": \"Task\",\n          \"Resource\":\n            \"arn:aws:lambda:us-east-1:123456789012:function:PhoneFinder\",\n          \"End\": true\n        }\n      }\n    }\n  ],\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>A Parallel State causes the interpreter to execute each branch starting with the state named in its \"StartAt\" field, as concurrently as possible, and wait until each branch terminates (reaches a terminal state) before processing the Parallel State's \"Next\" field. In the above example, this means the interpreter waits for \"LookupAddress\" and \"LookupPhoneNumber\" to both finish before transitioning to \"NextState\".</p> <p>A Parallel State MUST contain a field named \"Branches\" which is an array whose elements MUST be objects. Each object MUST contain fields named \"States\" and \"StartAt\" whose meanings are exactly like those in the top level of a State Machine.</p> <p>A state in a Parallel State branch \"States\" field MUST NOT have a \"Next\" field that targets a field outside of that \"States\" field. A state MUST NOT have a \"Next\" field which matches a state name inside a Parallel State branch's \"States\" field unless it is also inside the same \"States\" field.</p> <p>Put another way, states in a branch's \"States\" field can transition only to each other, and no state outside of that \"States\" field can transition into it.</p> <p>If any branch fails, due to an unhandled error or by transitioning to a Fail State, the entire Parallel State is considered to have failed and all the branches are terminated. If the error is not handled by the Parallel State, the interpreter should terminate the machine execution with an error.</p> <p>Unlike a Fail State, a Succeed State within a Parallel merely terminates its own branch. A Succeed State passes its input through as its output, possibly modified by \"InputPath\" and \"OutputPath\".</p> <p>The Parallel State passes its input (potentially as filtered by the \"InputPath\" field) as the input to each branch's \"StartAt\" state. It generates a result which is an array with one element for each branch containing the output from that branch. The elements of the output array correspond to the branches in the same order that they appear in the \"Branches\" array. There is no requirement that all elements be of the same type.</p> <p>The output array can be inserted into the input data using the state's \"ResultPath\" field in the usual way.</p> Example of ResultPath <pre><code>\"FunWithMath\": {\n  \"Type\": \"Parallel\",\n  \"Branches\": [\n    {\n      \"StartAt\": \"Add\",\n      \"States\": {\n        \"Add\": {\n          \"Type\": \"Task\",\n          \"Resource\": \"arn:aws:states:::task:Add\",\n          \"End\": true\n        }\n      }\n    },\n    {\n      \"StartAt\": \"Subtract\",\n      \"States\": {\n        \"Subtract\": {\n          \"Type\": \"Task\",\n          \"Resource\": \"arn:aws:states:::task:Subtract\",\n          \"End\": true\n        }\n      }\n    }\n  ],\n  \"Next\": \"NextState\"\n}\n</code></pre> <p>If the \"FunWithMath\" state was given the JSON array <code>[3, 2]</code> as input, then both the \"Add\" and \"Subtract\" states would receive that array as input. The output of \"Add\" would be <code>5</code>, that of \"Subtract\" would be <code>1</code>, and the output of the Parallel State would be a JSON array:</p> <pre><code>[ 5, 1 ]\n</code></pre> <p>The Map State (identified by <code>\"Type\": \"Map\"</code>) causes the interpreter to process all the elements of an array, potentially in parallel, with the processing of each element independent of the others. This document uses the term \"iteration\" to describe each such nested execution.</p> <p>The Parallel State applies multiple different state-machine branches to the same input, while the Map State applies a single state machine to multiple input elements.</p> <p>There are several fields which may be used to control the execution. To summarize:</p> <ul> <li>The \"ItemProcessor\" (or \"Iterator\" which is now deprecated) field's value is an object that defines a state machine which will process each item or batch of items of the array.</li> <li>The \"ItemsPath\" field's value is a Reference Path identifying where in the effective input the array field is found.</li> <li>The \"ItemReader\" field's value is an object that specifies where to read the items instead of from the effective input.</li> <li>The \"ItemSelector\" (or \"Parameters\" which is now deprecated in Map) field's value is an object that overrides each single element of the item array.</li> <li>The \"ItemBatcher\" field's value is an object that specifies how to batch the items for the ItemProcessor.</li> <li>The \"ResultWriter\" field's value is an object that specifies where to write the results instead of to the Map state's result.</li> <li>The \"MaxConcurrency\" field's value is an integer that provides an upper bound on how many invocations of the Iterator may run in parallel.</li> <li>The \"ToleratedFailurePercentage\" field's value is an integer that provides an upper bound on the percentage of items that may fail.</li> <li>The \"ToleratedFailureCount\" field's value is an integer that provides an upper bound on how many items may fail.</li> </ul> <p>Consider the following example input data:</p> <pre><code>{\n  \"ship-date\": \"2016-03-14T01:59:00Z\",\n  \"detail\": {\n    \"delivery-partner\": \"UQS\",\n    \"shipped\": [\n      { \"prod\": \"R31\", \"dest-code\": 9511, \"quantity\": 1344 },\n      { \"prod\": \"S39\", \"dest-code\": 9511, \"quantity\": 40 },\n      { \"prod\": \"R31\", \"dest-code\": 9833, \"quantity\": 12 },\n      { \"prod\": \"R40\", \"dest-code\": 9860, \"quantity\": 887 },\n      { \"prod\": \"R40\", \"dest-code\": 9511, \"quantity\": 1220 }\n    ]\n  }\n}\n</code></pre> <p>Suppose it is desired to apply a single Lambda function, \"ship-val\", to each of the elements of the \"shipped\" array. Here is an example of an appropriate Map State.</p> <pre><code>\"Validate-All\": {\n  \"Type\": \"Map\",\n  \"InputPath\": \"$.detail\",\n  \"ItemsPath\": \"$.shipped\",\n  \"MaxConcurrency\": 0,\n  \"ItemProcessor\": {\n    \"StartAt\": \"Validate\",\n    \"States\": {\n      \"Validate\": {\n        \"Type\": \"Task\",\n        \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ship-val\",\n        \"End\": true\n      }\n    }\n  },\n  \"ResultPath\": \"$.detail.shipped\",\n  \"End\": true\n}\n</code></pre> <p>In the example above, the \"ship-val\" Lambda function will be executed once for each element of the \"shipped\" field. The input to one iteration will be:</p> <pre><code>{\n  \"prod\": \"R31\",\n  \"dest-code\": 9511,\n  \"quantity\": 1344\n}\n</code></pre> <p>Suppose that the \"ship-val\" function also needs access to the shipment's courier, which would be the same in each iteration. The \"ItemSelector\" field may be used to construct the raw input for each iteration:</p> <pre><code>\"Validate-All\": {\n  \"Type\": \"Map\",\n  \"InputPath\": \"$.detail\",\n  \"ItemsPath\": \"$.shipped\",\n  \"MaxConcurrency\": 0,\n  \"ItemSelector\": {\n    \"parcel.$\": \"$$.Map.Item.Value\",\n    \"courier.$\": \"$.delivery-partner\"\n  },\n  \"ItemProcessor\": {\n    \"StartAt\": \"Validate\",\n    \"States\": {\n      \"Validate\": {\n        \"Type\": \"Task\",\n        \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:ship-val\",\n        \"End\": true\n      }\n    }\n  },\n  \"ResultPath\": \"$.detail.shipped\",\n  \"End\": true\n}\n</code></pre> <p>The \"ship-val\" Lambda function will be executed once for each element of the array selected by \"ItemsPath\". In the example above, the raw input to one iteration, as specified by \"ItemSelector\", will be:</p> <pre><code>{\n  \"parcel\": {\n    \"prod\": \"R31\",\n    \"dest-code\": 9511,\n    \"quantity\": 1344\n   },\n   \"courier\": \"UQS\"\n}\n</code></pre> <p>In the examples above, the ResultPath results in the output being the same as the input, with the \"detail.shipped\" field being overwritten by an array in which each element is the output of the \"ship-val\" Lambda function as applied to the corresponding input element.</p> Note Map State input/output processing <p>The \"InputPath\" field operates as usual, selecting part of the raw input - in the example, the value of the \"detail\" field - to serve as the effective input.</p> Reading Items <p>A Map State MAY have an \"ItemReader\" field, whose value MUST be a JSON object and is called the ItemReader Configuration. The ItemReader Configuration causes the interpreter to read items from the resource identified by the ItemReader Configuration's \"Resource\" field.</p> <p>The ItemReader Configuration MUST have a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI. The ItemReader Configuration MAY have a \"Parameters\" field, whose value MUST be a Payload Template. The ItemReader Configuration MAY have a \"ReaderConfig\" field whose value is a JSON object which MAY have a \"MaxItems\" field which MUST be a positive integer. The interpreter MAY define additional \"ReaderConfig\" fields.</p> <p>The ItemReader Configuration causes the interpreter to read items from the task identified by the ItemReader's \"Resource\" field. The interpreter will limit the number of items to the maximum number of items specified by the \"ReaderConfig\"'s \"MaxItems\" field. The \"MaxItems\" can be provided indirectly. A \"ReaderConfig\" field MAY have \"MaxItemsPath\" field which MUST be a Reference Path which, when resolved, MUST select a field whose value is a positive integer. A \"ReaderConfig\" field MUST NOT include both \"MaxItems\" and \"MaxItemsPath\".</p> <p>The default result for \"ItemReader\" is \"$\", which is to say the whole effective input.</p> Selecting Items <p>A Map State MAY have an \"ItemsPath\" field, whose value MUST be a Reference Path. The Reference Path is applied to the ItemReader result and MUST identify a field whose value is a JSON array.</p> <p>The default value of \"ItemsPath\" is \"$\", which is to say the whole ItemReader result. So, if a Map State has neither an \"InputPath\" nor an \"ItemReader\" nor an \"ItemsPath\" field, it is assuming that the raw input to the state will be a JSON array.</p> <p>A Map State MAY have an \"ItemSelector\" field, whose value MUST be a Payload Template. The interpreter uses the \"ItemSelector\" field to override each single element of the item array. The result of the \"ItemSelector\" field is called the selected item. The \"ItemSelector\" field performs the same function as the \"Parameters\" field, which is now deprecated in the Map State.</p> <p>The default of \"ItemSelector\" is a single element of the array field identified by the \"ItemsPath\" value.</p> Batching Items <p>A Map State MAY have an \"ItemBatcher\" field, whose value MUST be a JSON object and is called the ItemBatcher Configuration. The ItemBatcher Configuration causes the interpreter to batch selected items into sub-arrays before passing them to each invocation. The interpreter will limit each sub-array to the maximum number of items specified by the \"MaxItemsPerBatch\" field, and to the maximum size in bytes specified by the \"MaxInputBytesPerBatch\" field.</p> <p>The ItemBatcher Configuration MAY have a \"BatchInput\" field, whose value MUST be a Payload Template. An ItemBatcher Configuration MAY have a \"MaxItemsPerBatch\" field, whose value MUST be a positive integer. An ItemBatcher Configuration MAY have a \"MaxInputBytesPerBatch\" field, whose value MUST be a positive integer.</p> <p>The default of \"ItemBatcher\" is the selected item. Put another way, the interpreter will not batch items if no \"ItemBatcher\" field is provided.</p> <p>Both the \"MaxItemsPerBatch\" and \"MaxInputBytesPerBatch\" can be provided indirectly. A Map State may have \"MaxItemsPerBatchPath\" and \"MaxInputBytesPerBatchPath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are positive integers. A Map State MUST NOT include both \"MaxItemsPerBatch\" and \"MaxItemsPerBatchPath\" or both \"MaxInputBytesPerBatch\" and \"MaxInputBytesPerBatchPath\".</p> <p>An ItemBatcher Configuration MUST contain at least one of \"MaxItemsPerBatch\", \"MaxItemsPerBatchPath\", \"MaxInputBytesPerBatch\", or \"MaxInputBytesPerBatchPath\".</p> Processing Items <p>The input to each invocation, by default, is a single element of the   array field identified by the \"ItemsPath\" value, but may be overridden   using the \"ItemSelector\" and/or \"ItemBatcher\" fields. If present, the   interpreter uses the \"ItemSelector\" field to override each single element   of the array field identified by the \"ItemsPath\" value to produce an   array of selected items. If present, the interpreter then uses the   \"ItemBatcher\" field to specify how to batch the selected items array   to produce an array of batched selected items.</p> <p>For each item, within the Map State's \"ItemSelector\" (or deprecated \"Parameters\") field, the Context Object will have an object field named \"Map\" which contains an object field named \"Item\" which in turn contains an integer field named \"Index\" whose value is the (zero-based) array index being processed and a field named \"Value\", whose value is the array element being processed.</p> Writing Results <p>A Map State MAY have a \"ResultWriter\" field, whose value MUST be a JSON object and is called the ResultWriter Configuration. The ResultWriter Configuration causes the interpreter to write results to the resource identified by the ResultWriter's \"Resource\" field.</p> <p>The ResultWriter Configuration MUST have a \"Resource\" field, whose value MUST be a URI that uniquely identifies the specific task to execute. The States language does not constrain the URI scheme nor any other part of the URI. The ResultWriter Configuration MAY have a \"Parameters\" field, whose value MUST be a Payload Template.</p> <p>If a \"ResultWriter\" field is provided, a Map State's result is a JSON object containing fields defined by the interpreter. If a \"ResultWriter\" field is not specified, the result is a JSON array, whose value is either an array containing one element for each element of the ItemsPath input array, in the same order (if an \"ItemBatcher\" field is not specified), or an array containing one element for each batched sub-array of the ItemsPath input array, in the same order (if an \"ItemBatcher\" field is specified).</p> Map State concurrency <p>A Map State MAY have a non-negative integer \"MaxConcurrency\" field. Its default value is zero, which places no limit on invocation parallelism and requests the interpreter to execute the iterations as concurrently as possible.</p> <p>If \"MaxConcurrency\" has a non-zero value, the interpreter will not allow the number of concurrent iterations to exceed that value.</p> <p>A MaxConcurrency value of 1 is special, having the effect that interpreter will invoke the ItemProcessor once for each array element in the order of their appearance in the input, and will not start an iteration until the previous iteration has completed execution.</p> <p>The \"MaxConcurrency\" can be provided indirectly. A Map State may have \"MaxConcurrencyPath\" field which MUST be a Reference Path which, when resolved, MUST select a field whose value is a non-negative integer. A Map State MUST NOT include both \"MaxConcurrency\" and \"MaxConcurrencyPath\".</p> Map State ItemProcessor definition <p>A Map State MUST contain an object field named \"ItemProcessor\" (or deprecated \"Iterator\") which MUST contain fields named \"States\" and \"StartAt\", whose meanings are exactly like those in the top level of a State Machine. The \"ItemProcessor\" field performs the same function as the \"Iterator\" field, which is now deprecated in the Map State.</p> <p>The \"ItemProcessor\" field MAY contain a field named \"ProcessorConfig\", whose value MUST be a JSON object whose value is defined by the interpreter. The interpreter will execute the ItemProcessor according to the ProcessorConfig.</p> <p>A state in the \"States\" field of an \"ItemProcessor\" field MUST NOT have a \"Next\" field that targets a field outside of that \"States\" field. A state MUST NOT have a \"Next\" field which matches a state name inside an \"ItemProcessor\" field's \"States\" field unless it is also inside the same \"States\" field.</p> <p>Put another way, states in an ItemProcessor's \"States\" field can transition only to each other, and no state outside of that \"States\" field can transition into it.</p> <p>If any iteration fails, due to an unhandled error or by transitioning to a Fail state, the entire Map State is considered to have failed and all the iterations are terminated. If the error is not handled by the Map State, the interpreter should terminate the machine execution with an error.</p> <p>Unlike a Fail State, a Succeed State within a Map merely terminates its own iteration. A Succeed State passes its input through as its output, possibly modified by \"InputPath\" and \"OutputPath\".</p> Map State Failure Tolerance <p>A Map State MAY have a \"ToleratedFailurePercentage\" field whose value MUST be a number between zero and 100. Its default value is zero, which means the Map State will fail if any (i.e. more than 0%) of its items fail. A \"ToleratedFailurePercentage\" value of 100 means the interpreter will continue starting iterations even if all items fail.</p> <p>A Map State MAY have a non-negative integer \"ToleratedFailureCount\" field. Its default value is zero, which means the Map State will fail if at least 1 of its items fail. If a \"ToleratedFailurePercentage\" field and a \"ToleratedFailureCount\" field are both specified, the Map State will fail if either threshold is breached.</p> <p>Both the \"ToleratedFailureCount\" and \"ToleratedFailurePercentage\" can be provided indirectly. A Map State may have \"ToleratedFailureCountPath\" and \"ToleratedFailurePercentagePath\" fields which MUST be Reference Paths which, when resolved, MUST select fields whose values are non-negative integers. A Map State MUST NOT include both \"ToleratedFailureCount\" and \"ToleratedFailureCountPath\" or both \"ToleratedFailurePercentage\" and \"ToleratedFailurePercentagePath\".</p>"},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendices","title":"Appendices","text":""},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendix-a-predefined-error-codes","title":"Appendix A: Predefined Error Codes","text":"Code Description States.ALL A wildcard which matches any Error Name. States.HeartbeatTimeout A Task State failed to heartbeat for a time longer than the \"HeartbeatSeconds\" value. States.Timeout A Task State either ran longer than the \"TimeoutSeconds\" value, or failed to heartbeat for a time longer than the \"HeartbeatSeconds\" value. States.TaskFailed A Task State failed during the execution. States.Permissions A Task State failed because it had insufficient privileges to execute the specified code. States.ResultPathMatchFailure A state's \"ResultPath\" field cannot be applied to the input the state received. States.ParameterPathFailure Within a state's \"Parameters\" field, the attempt to replace a field whose name ends in \".$\" using a Path failed. States.BranchFailed A branch of a Parallel State failed. States.NoChoiceMatched A Choice State failed to find a match for the condition field extracted from its input. States.IntrinsicFailure Within a Payload Template, the attempt to invoke an Intrinsic Function failed. States.ExceedToleratedFailureThreshold A Map state failed because the number of failed items exceeded the configured tolerated failure threshold. States.ItemReaderFailed A Map state failed to read all items as specified by the \"ItemReader\" field. States.ResultWriterFailed A Map state failed to write all results as specified by the \"ResultWriter\" field."},{"location":"services/aws/step_functions/stfn-state-machine-language/#appendix-b-list-of-intrinsic-functions","title":"Appendix B: List of Intrinsic Functions","text":"States.Format <p>This Intrinsic Function takes one or more arguments. The Value of the first MUST be a string, which MAY include zero or more instances of the character sequence <code>{}</code>. There MUST be as many remaining arguments in the Intrinsic Function as there are occurrences of <code>{}</code>. The interpreter returns the first-argument string with each <code>{}</code> replaced by the Value of the positionally-corresponding argument in the Intrinsic Function.</p> <p>If necessary, the <code>{</code> and <code>}</code> characters can be escaped respectively as <code>\\\\{</code> and <code>\\\\}</code>.</p> <p>If the argument is a Path, applying it to the input MUST yield a value that is a string, a boolean, a number, or <code>null</code>. In each case, the Value is the natural string representation; string values are not accompanied by enclosing <code>\"</code> characters. The Value MUST NOT be a JSON array or object.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.Format('Your name is {}, we are in the year {}', $.name, 2020)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"name\": \"Foo\",\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": \"Your name is Foo, we are in the year 2020\"\n}\n</code></pre> States.StringToJson <p>This Intrinsic Function takes a single argument whose Value MUST be a string. The interpreter applies a JSON parser to the Value and returns its parsed JSON form.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.StringToJson($.someString)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someString\": \"{\\\"number\\\": 20}\",\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": {\n    \"number\": 20\n  }\n}\n</code></pre> States.JsonToString <p>This Intrinsic Function takes a single argument which MUST be a Path. The interpreter returns a string which is a JSON text representing the data identified by the Path.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.JsonToString($.someJson)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someJson\": {\n    \"name\": \"Foo\",\n    \"year\": 2020\n  },\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": \"{\\\"name\\\":\\\"Foo\\\",\\\"year\\\":2020}\"\n}\n</code></pre> States.Array <p>This Intrinsic Function takes zero or more arguments. The interpreter returns a JSON array containing the Values of the arguments, in the order provided.</p> <p>For example, given the following Payload Template:</p> <pre><code>{\n  \"Parameters\": {\n    \"foo.$\": \"States.Array('Foo', 2020, $.someJson, null)\"\n  }\n}\n</code></pre> InputOutput <p>Suppose the input to the Payload Template is as follows:</p> <pre><code>{\n  \"someJson\": {\n    \"random\": \"abcdefg\"\n  },\n  \"zebra\": \"stripe\"\n}\n</code></pre> <p>After processing the Payload Template, the new payload becomes:</p> <pre><code>{\n  \"foo\": [\n    \"Foo\",\n    2020,\n    {\n      \"random\": \"abcdefg\"\n    },\n    null\n  ]\n}\n</code></pre> States.ArrayPartition <p>Use the <code>States.ArrayPartition</code> intrinsic function to partition a large array. You can also use this intrinsic to slice the data and then send the payload in smaller chunks.</p> <p>This intrinsic function takes two arguments. The first argument is an array, while the second argument defines the chunk size. The interpreter chunks the input array into multiple arrays of the size specified by chunk size. The length of the last array chunk may be less than the length of the previous array chunks if the number of remaining items in the array is smaller than the chunk size.</p> <p>Input validation</p> <ul> <li>You must specify an array as the input value for the function's first argument.</li> <li>You must specify a non-zero, positive integer for the second argument representing   the chunk size value.</li> <li>The input array can't exceed Step Functions' payload size limit of 256 KB.</li> </ul> <p>For example, given the following input array:</p> <pre><code>{\"inputArray\": [1,2,3,4,5,6,7,8,9] }\n</code></pre> <p>You could use the States.ArrayPartition function to divide the array into chunks of four values:</p> <pre><code>\"inputArray.$\": \"States.ArrayPartition($.inputArray,4)\"\n</code></pre> <p>Which would return the following array chunks:</p> <pre><code>{\"inputArray\": [ [1,2,3,4], [5,6,7,8], [9]] }\n</code></pre> <p>In the previous example, the States.ArrayPartition function outputs three arrays. The first two arrays each contain four values, as defined by the chunk size. A third array contains the remaining value and is smaller than the defined chunk size.</p> States.ArrayContains <p>Use the <code>States.ArrayContains</code> intrinsic function to determine if a specific value is present in an array. For example, you can use this function to detect if there was an error in a <code>Map</code> state iteration.</p> <p>This intrinsic function takes two arguments. The first argument is an array, while the second argument is the value to be searched for within the array.</p> <p>Input validation:</p> <ul> <li>You must specify an array as the input value for function's first argument.</li> <li>You must specify a valid JSON object as the second argument.</li> <li>The input array can't exceed Step Functions' payload size limit of 256 KB.</li> </ul> <p>For example, given the following input array:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9], \"lookingFor\": 5 }\n</code></pre> <p>You could use the States.ArrayContains function to find the lookingFor value within the inputArray:</p> <pre><code>\"contains.$\": \"States.ArrayContains($.inputArray, $.lookingFor)\"\n</code></pre> <p>Because the value stored in <code>lookingFor</code> is included in the inputArray, <code>States.ArrayContains</code> returns the following result:</p> <pre><code>{\"contains\": true }\n</code></pre> States.ArrayRange <p>Use the <code>States.ArrayRange</code> intrinsic function to create a new array containing a specific range of elements. The new array can contain up to 1000 elements.</p> <p>This function takes three arguments. The first argument is the first element of the new array, the second argument is the final element of the new array, and the third argument is the increment value between the elements in the new array.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for all of the arguments.</li> <li>You must specify a non-zero value for the third argument.</li> <li>The newly generated array can't contain more than 1000 items.</li> </ul> <p>For example, the following use of the <code>States.ArrayRange</code> function will create an array with a first value of 1, a final value of 9, and values in between the first and final values increase by two for each item:</p> <pre><code>\"array.$\": \"States.ArrayRange(1, 9, 2)\"\n</code></pre> <p>Which would return the following array:</p> <pre><code>{\"array\": [1,3,5,7,9] }\n</code></pre> States.ArrayGetItem <p>This intrinsic function returns a specified index's value. This function takes two arguments. The first argument is an array of values and the second argument is the array index of the value to return.</p> <p>For example, use the following <code>inputArray</code> and <code>index</code> values:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9], \"index\": 5 }\n</code></pre> <p>From these values, you can use the <code>States.ArrayGetItem</code> function to return the value in the <code>index</code> position 5 within the array:</p> <pre><code>\"item.$\": \"States.ArrayGetItem($.inputArray, $.index)\"\n</code></pre> <p>In this example, <code>States.ArrayGetItem</code> would return the following result:</p> <pre><code>{ \"item\": 6 }\n</code></pre> States.ArrayLength <p>The <code>States.ArrayLength</code> intrinsic function returns the length of an array. It has one argument, the array to return the length of.</p> <p>For example, given the following input array:</p> <pre><code>{ \"inputArray\": [1,2,3,4,5,6,7,8,9] }\n</code></pre> <p>You can use <code>States.ArrayLength</code> to return the length of <code>inputArray</code>:</p> <pre><code>\"length.$\": \"States.ArrayLength($.inputArray)\"\n</code></pre> <p>In this example, <code>States.ArrayLength</code> would return the following JSON object that represents the array length:</p> <pre><code>{ \"length\": 9 }\n</code></pre> States.ArrayUnique <p>The <code>States.ArrayUnique</code> intrinsic function removes duplicate values from an array and returns an array containing only unique elements. This function takes an array, which can be unsorted, as its sole argument.</p> <p>For example, the following <code>inputArray</code> contains a series of duplicate values:</p> <pre><code>{\"inputArray\": [1,2,3,3,3,3,3,3,4] }\n</code></pre> <p>You could use the <code>States.ArrayUnique</code> function as and specify the array you want to remove duplicate values from:</p> <pre><code>\"array.$\": \"States.ArrayUnique($.inputArray)\"\n</code></pre> <p>The <code>States.ArrayUnique</code> function would return the following array containing only unique elements, removing all duplicate values:</p> <pre><code>{\"array\": [1,2,3,4] }\n</code></pre> States.Base64Encode <p>Use the <code>States.Base64Encode</code> intrinsic function to encode data based on MIME Base64 encoding scheme. You can use this function to pass data to other AWS services without using an AWS Lambda function.</p> <p>This function takes a data string of up to 10,000 characters to encode as its only argument.</p> <p>For example, consider the following <code>input</code> string:</p> <pre><code>{\"input\": \"Data to encode\" }\n</code></pre> <p>You can use the <code>States.Base64Encode</code> function to encode the <code>input</code> string as a MIME Base64 string:</p> <pre><code>\"base64.$\": \"States.Base64Encode($.input)\"\n</code></pre> <p>The <code>States.Base64Encode</code> function returns the following encoded data in response:</p> <pre><code>{\"base64\": \"RGF0YSB0byBlbmNvZGU=\" }\n</code></pre> States.Base64Decode <p>Use the <code>States.Base64Decode</code> intrinsic function to decode data based on MIME Base64 decoding scheme. You can use this function to pass data to other AWS services without using a Lambda function.</p> <p>This function takes a Base64 encoded data string of up to 10,000 characters to decode as its only argument.</p> <p>For example, given the following input:</p> <pre><code>{\"base64\": \"RGF0YSB0byBlbmNvZGU=\" }\n</code></pre> <p>You can use the <code>States.Base64Decode</code> function to decode the base64 string to a human-readable string:</p> <pre><code>\"data.$\": \"States.Base64Decode($.base64)\"\n</code></pre> <p>The <code>States.Base64Decode</code> function would return the following decoded data in response:</p> <pre><code>{\"data\": \"Decoded data\" }\n</code></pre> States.Hash <p>Use the <code>States.Hash</code> intrinsic function to calculate the hash value of a given input. You can use this function to pass data to other AWS services without using a Lambda function.</p> <p>This function takes two arguments. The first argument is the data you want to calculate the hash value of. The second argument is the hashing algorithm to use to perform the hash calculation. The data you provide must be an object string containing 10,000 characters or less.</p> <p>The hashing algorithm you specify can be any of the following algorithms:</p> <ul> <li>MD5</li> <li>SHA-1</li> <li>SHA-256</li> <li>SHA-384</li> <li>SHA-512</li> </ul> <p>For example, you can use this function to calculate the hash value of the <code>Data</code> string using the specified <code>Algorithm</code>:</p> <pre><code>{ \"Data\": \"input data\", \"Algorithm\": \"SHA-1\" }\n</code></pre> <p>You can use the <code>States.Hash</code> function to calculate the hash value:</p> <pre><code>\"output.$\": \"States.Hash($.Data, $.Algorithm)\"\n</code></pre> <p>The <code>States.Hash</code> function returns the following hash value in response:</p> <pre><code>{\"output\": \"aaff4a450a104cd177d28d18d7485e8cae074b7\" }\n</code></pre> States.JsonMerge <p>Use the <code>States.JsonMerge</code> intrinsic function to merge two JSON objects into a single object. This function takes three arguments. The first two arguments are the JSON objects that you want to merge.The third argument is a boolean value of <code>false</code>. This boolean value determines if the deep merging mode is enabled.</p> <p>Currently, Step Functions only supports the shallow merging mode; therefore, you must specify the boolean value as <code>false</code>. In the shallow mode, if the same key exists in both JSON objects, the latter object's key overrides the same key in the first object. Additionally, objects nested within a JSON object aren't merged when you use shallow merging.</p> <p>For example, you can use the <code>States.JsonMerge</code> function to merge the following JSON arrays that share the key <code>a</code>.</p> <pre><code>{ \"json1\": { \"a\": {\"a1\": 1, \"a2\": 2}, \"b\": 2, }, \"json2\": { \"a\": {\"a3\": 1, \"a4\": 2}, \"c\": 3 } }\n</code></pre> <p>You can specify the json1 and json2 arrays as inputs in the <code>States.JasonMerge</code> function to merge them together:</p> <pre><code>\"output.$\": \"States.JsonMerge($.json1, $.json2, false)\"\n</code></pre> <p>The <code>States.JsonMerge</code> returns the following merged JSON object as result. In the merged JSON object <code>output</code>, the <code>json2</code> object's key a replaces the <code>json1</code> object's key <code>a</code>. Also, the nested object in <code>json1</code> object's key <code>a</code> is discarded because shallow mode doesn't support merging nested objects.</p> <pre><code>{ \"output\": { \"a\": {\"a3\": 1, \"a4\": 2}, \"b\": 2, \"c\": 3 } }\n</code></pre> States.MathRandom <p>Use the <code>States.MathRandom</code> intrinsic function to return a random number between the specified start and end number. For example, you can use this function to distribute a specific task between two or more resources.</p> <p>This function takes three arguments. The first argument is the start number, the second argument is the end number, and the last argument controls the seed value. The seed value argument is optional.</p> <p>If you use this function with the same seed value, it returns an identical number.</p> <p>Warning</p> <p>Because the <code>States.MathRandom</code> function doesn't return cryptographically secure random numbers, we recommend that you don't use it for security sensitive applications.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for the start number and end number arguments.</li> </ul> <p>For example, to generate a random number from between one and 999, you can use the following input values:</p> <pre><code>{ \"start\": 1, \"end\": 999 }\n</code></pre> <p>To generate the random number, provide the <code>start</code> and <code>end</code> values to the <code>States.MathRandom</code> function:</p> <pre><code>\"random.$\": \"States.MathRandom($.start, $.end)\"\n</code></pre> <p>The <code>States.MathRandom</code> function returns the following random number as a response:</p> <pre><code>{\"random\": 456 }\n</code></pre> States.MathAdd <p>Use the <code>States.MathAdd</code> intrinsic function to return the sum of two numbers. For example, you can use this function to increment values inside a loop without invoking a Lambda function.</p> <p>Input validation:</p> <ul> <li>You must specify integer values for all the arguments.</li> </ul> <p>For example, you can use the following values to subtract one from 111:</p> <pre><code>{ \"value1\": 111, \"step\": -1 }\n</code></pre> <p>Then, use the <code>States.MathAdd</code> function defining <code>value1</code> as the starting value, and <code>step</code> as the value to increment <code>value1</code> by:</p> <pre><code>\"value1.$\": \"States.MathAdd($.value1, $.step)\"\n</code></pre> <p>The <code>States.MathAdd</code> function would return the following number in response:</p> <pre><code>{\"value1\": 110 }\n</code></pre> States.StringSplit <p>Use the <code>States.StringSplit</code> intrinsic function to split a string into an array of values. This function takes two arguments.The first argument is a string and the second argument is the delimiting character that the function will use to divide the string.</p> <p>For example, you can use <code>States.StringSplit</code> to divide the following <code>inputString</code>, which contains a series of comma separated values:</p> <pre><code>{ \"inputString\": \"1,2,3,4,5\", \"splitter\": \",\" }\n</code></pre> <p>Use the <code>States.StringSplit</code> function and define <code>inputString</code> as the first argument, and the delimiting character <code>splitter</code> as the second argument:</p> <pre><code>\"array.$\": \"States.StringSplit($.inputString, $.splitter)\"\n</code></pre> <p>The States.StringSplit function returns the following string array as result:</p> <pre><code>{\"array\": [\"1\",\"2\",\"3\",\"4\",\"5\"] }\n</code></pre> States.UUID <p>Use the <code>States.UUID</code> intrinsic function to return a version 4 universally unique identifier (v4 UUID) generated using random numbers. For example, you can use this function to call other AWS services or resources that need a UUID parameter or insert items in a DynamoDB table.</p> <p>The <code>States.UUID</code> function is called with no arguments specified:</p> <pre><code>\"uuid.$\": \"States.UUID()\"\n</code></pre> <p>The function returns a randomly generated UUID, as in the following example:</p> <pre><code>{\"uuid\": \"ca4c1140-dcc1-40cd-ad05-7b4aa23df4a8\" }\n</code></pre> <ol> <li> <p>Copyright \u00a9 2016 Amazon.com Inc. or Affiliates.</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this specification and associated documentation files (the \"specification\"), to use, copy, publish, and/or distribute, the Specification) subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies of the Specification.</p> <p>You may not modify, merge, sublicense, and/or sell copies of the Specification.</p> <p>THE SPECIFICATION IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SPECIFICATION OR THE USE OR OTHER DEALINGS IN THE SPECIFICATION.</p> <p>Any sample code included in the Specification, unless otherwise specified, is licensed under the Apache License, Version 2.0.\u00a0\u21a9</p> </li> </ol>"},{"location":"services/azure/","title":"Microsoft Azure","text":"<p>Designed by Microsoft in 2010, Microsoft Azure is one of the widely used cloud computing platforms. Azure provides a wide variety of services such as cloud storage, compute services, network services, cognitive services, databases, analytics, and IoT.</p> <p>It makes building, deploying, and managing applications very easy. All the Microsoft Azure fundamentals are also described for a better understanding of readers.</p>"},{"location":"services/azure/az-eventhubs/","title":"Azure EventHubs","text":""},{"location":"services/azure/az-eventhubs/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/az-eventhubs/#connection-code","title":"Connection Code","text":"Python: AsyncPython: Batches AsyncPython: Checkpoint <pre><code>import logging\nimport asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\n\nlogger = logging.getLogger(\"azure.eventhub\")\nlogging.basicConfig(level=logging.INFO)\n\nasync def on_event(partition_context, event):\n    logger.info(\"Received event from partition {}\".format(partition_context.partition_id))\n    await partition_context.update_checkpoint(event)\n\nasync def receive():\n    client = EventHubConsumerClient.from_connection_string(connection_str, consumer_group, eventhub_name=eventhub_name)\n    async with client:\n        await client.receive(\n            on_event=on_event,\n            starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n        )\n        # receive events from specified partition:\n        # await client.receive(on_event=on_event, partition_id='0')\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(receive())\n</code></pre> <pre><code>import logging\nimport asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\n\nlogger = logging.getLogger(\"azure.eventhub\")\nlogging.basicConfig(level=logging.INFO)\n\nasync def on_event_batch(partition_context, events):\n    logger.info(\"Received event from partition {}\".format(partition_context.partition_id))\n    await partition_context.update_checkpoint()\n\nasync def receive_batch():\n    client = EventHubConsumerClient.from_connection_string(connection_str, consumer_group, eventhub_name=eventhub_name)\n    async with client:\n        await client.receive_batch(\n            on_event_batch=on_event_batch,\n            starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n        )\n        # receive events from specified partition:\n        # await client.receive_batch(on_event_batch=on_event_batch, partition_id='0')\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(receive_batch())\n</code></pre> <pre><code>import asyncio\nfrom azure.eventhub.aio import EventHubConsumerClient\nfrom azure.eventhub.extensions.checkpointstoreblobaio import BlobCheckpointStore\n\nconnection_str = '&lt;&lt; CONNECTION STRING FOR THE EVENT HUBS NAMESPACE &gt;&gt;'\nconsumer_group = '&lt;&lt; CONSUMER GROUP &gt;&gt;'\neventhub_name = '&lt;&lt; NAME OF THE EVENT HUB &gt;&gt;'\nstorage_connection_str = '&lt;&lt; CONNECTION STRING FOR THE STORAGE &gt;&gt;'\ncontainer_name = '&lt;&lt;NAME OF THE BLOB CONTAINER&gt;&gt;'\n\nasync def on_event(partition_context, event):\n    # do something\n    await partition_context.update_checkpoint(event)  # Or update_checkpoint every N events for better performance.\n\nasync def receive(client):\n    await client.receive(\n        on_event=on_event,\n        starting_position=\"-1\",  # \"-1\" is from the beginning of the partition.\n    )\n\nasync def main():\n    checkpoint_store = BlobCheckpointStore.from_connection_string(storage_connection_str, container_name)\n    client = EventHubConsumerClient.from_connection_string(\n        connection_str,\n        consumer_group,\n        eventhub_name=eventhub_name,\n        checkpoint_store=checkpoint_store,  # For load balancing and checkpoint. Leave None for no load balancing\n    )\n    async with client:\n        await receive(client)\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main())\n</code></pre>"},{"location":"services/azure/az-eventhubs/#producer","title":"Producer","text":"<pre><code>import asyncio\nimport aiohttp\nimport json\nimport logging\nfrom azure.eventhub import EventHubClient, EventData\n\nasync def get_weather(city, state, api_key):\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city},{state}&amp;appid={api_key}\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            return await resp.json()\n\nasync def send_to_event_hub(event_hub_client, weather_data):\n    event = EventData(json.dumps(weather_data).encode(\"utf-8\"))\n    await event_hub_client.send(event)\n\nasync def main(api_key, event_hub_client):\n    # List of cities in the USA\n    cities = [\n        {\"city\": \"New York\", \"state\": \"NY\"},\n        {\"city\": \"Los Angeles\", \"state\": \"CA\"},\n        {\"city\": \"Chicago\", \"state\": \"IL\"},\n        {\"city\": \"Houston\", \"state\": \"TX\"},\n        {\"city\": \"Phoenix\", \"state\": \"AZ\"},\n        {\"city\": \"Philadelphia\", \"state\": \"PA\"},\n        {\"city\": \"San Antonio\", \"state\": \"TX\"},\n        {\"city\": \"San Diego\", \"state\": \"CA\"},\n        {\"city\": \"Dallas\", \"state\": \"TX\"},\n        {\"city\": \"San Jose\", \"state\": \"CA\"}\n    ]\n\n    tasks = []\n    for city_data in cities:\n        task = asyncio.create_task(get_weather(city_data[\"city\"], city_data[\"state\"], api_key))\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks)\n    for weather_data in results:\n        await send_to_event_hub(event_hub_client, weather_data)\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# Event Hub Configuration\nevent_hub_namespace = \"&lt;Your Azure Event Hub Namespace&gt;\"\nevent_hub_name = \"eh_sample_01\"\nevent_hub_key = \"&lt;Your Azure Event Hub Key&gt;\"\nevent_hub_endpoint = f\"amqps://{event_hub_namespace}.servicebus.windows.net/{event_hub_name}\"\n\napi_key = \"&lt;Your OpenWeatherMap API Key&gt;\"\ninterval = 5 * 60 # 5 minutes in seconds\nevent_hub_client = EventHubClient.from_connection_string(event_hub_endpoint, event_hub_key)\n\nwhile True:\n    asyncio.run(main(api_key, event_hub_client))\n    await asyncio.sleep(interval)\n</code></pre>"},{"location":"services/azure/az-eventhubs/#connect-with-kafka","title":"Connect with Kafka","text":"<p>Read More on Connector Document</p> <pre><code># Source: https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/tutorials/spark#running-spark\nEH_NAME_SPACE = \"eventhubs-name-space\"\nEH_NAME = \"eventhubs-name\"\nEH_SASL = (\n    f'org.apache.kafka.common.security.plain.PlainLoginModule required'\n    f'username=\"$ConnectionString\" '\n    f'password=\"Endpoint=sb://{EH_NAME_SPACE}.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=****\";'\n)\n(\n    df.write\n        .format(\"kafka\")\n        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n        .option(\"kafka.sasl.jaas.config\", EH_SASL)\n        .option(\"kafka.batch.size\", 5000)\n        .option(\"kafka.bootstrap.servers\", f\"{EH_NAME_SPACE}.servicebus.windows.net:9093\")\n        .option(\"kafka.request.timeout.ms\", 120000)\n        .option(\"topic\", EH_NAME)\n        .option(\"checkpointLocation\", \"/mnt/telemetry/cp.txt\")\n        .save()\n)\n</code></pre>"},{"location":"services/azure/az-eventhubs/#read-mores","title":"Read Mores","text":"<ul> <li> Azure Event Hubs Features</li> <li> PyPI: <code>azure-eventhub</code></li> <li> Set startingPosition in Event Hub on Databricks</li> <li>How to format a Pyspark connection string for Azure Eventhub with Kafka</li> </ul>"},{"location":"services/azure/az-keyvaults/","title":"Azure Key Vaults","text":""},{"location":"services/azure/az-keyvaults/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/az-keyvaults/#connection-code","title":"Connection Code","text":"<p>The Python connection code that use to interact with the Azure Key Vaults service.</p> <pre><code>pip install azure-keyvault-secrets==4.8.0\n</code></pre> DefaultMSI <pre><code>from azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient, KeyVaultSecret\n\ndef get_kv_secret(name: str) -&gt; str:\n    credential = DefaultAzureCredential()\n    secret_client = SecretClient(\n        vault_url=f\"https://{os.environ['keyvault']}.vault.azure.net\",\n        credential=credential,\n        logging_enable=False,\n    )\n    secret: KeyVaultSecret = secret_client.get_secret(secret_name)\n    return secret.value\n</code></pre> <pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient, KeyVaultSecret\n\ndef get_kv_secret(name: str) -&gt; str:\n    credential = ManagedIdentityCredential()\n    secret_client = SecretClient(\n        vault_url=f\"https://{os.environ['keyvault']}.vault.azure.net\",\n        credential=credential,\n        logging_enable=False,\n    )\n    secret: KeyVaultSecret = secret_client.get_secret(secret_name)\n    return secret.value\n</code></pre>"},{"location":"services/azure/az-keyvaults/#read-mores","title":"Read Mores","text":""},{"location":"services/azure/az-oauth/","title":"Azure OAuth","text":"Access Scenarios"},{"location":"services/azure/az-oauth/#authentication-vs-authorization","title":"Authentication vs Authorization","text":"<p>Authentication and Authorization are the two terms used in the context of OAuth and API Security. They are used in conjunction with each other and both sound similar, but they refer to entirely different security processes.</p> <ul> <li>Authentication answers the question: Who are you?</li> <li>Authorization answers the question: What are you allowed to do?</li> </ul>"},{"location":"services/azure/az-oauth/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/az-oauth/#1-create-service-principle","title":"1) Create Service Principle","text":"<ul> <li>Go to App Registration  Click New registration</li> <li>On Authentication  Click Add a platform    Pass <code>http://localhost</code> to this field</li> <li>On Certificates &amp; secrets  Click Client secrets  Copy Client ID and Client Secret ID from this creation process</li> </ul>"},{"location":"services/azure/az-oauth/#2-get-authorization-code","title":"2) Get Authorization Code","text":"<pre><code>GET {tenant-id}/oauth2/v2.0/authorize HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}&amp;\nredirect_uri={redirect-uri}&amp;\nresponse_type=code&amp;\nresponse_mode=query&amp;\nscope=offline_access {scopes}&amp;\naccess_type=offline\n</code></pre>"},{"location":"services/azure/az-oauth/#3-request-access-and-refresh-tokens","title":"3) Request Access and Refresh tokens","text":"<pre><code>POST {tenant-id}/oauth2/v2.0/token HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\ncode={authorization-code}&amp;\nclient_id={client-id}&amp;\nclient_secret={client-secret}&amp;\nredirect_uri={redirect-uri}&amp;\nscope=offline_access {scopes}&amp;\ngrant_type=authorization_code\n</code></pre>"},{"location":"services/azure/az-oauth/#31-re-generate-access-token","title":"3.1) Re-generate Access Token","text":"<pre><code>POST {tenant-id}/oauth2/v2.0/token HTTP/1.1\nHost: login.microsoftonline.com\nContent-Type: application/x-www-form-urlencoded\n\nrefresh_token={refresh-token}&amp;\nclient_id={client-id}&amp;\nclient_secret={client-secret}&amp;\nscope=offline_access {scopes}&amp;\ngrant_type=refresh_token\n</code></pre>"},{"location":"services/azure/az-oauth/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft - Get access and refresh tokens</li> <li> Microsoft - identity platform and OAuth 2.0 authorization code flow</li> <li> Microsoft Graph - Permissions Reference</li> </ul>"},{"location":"services/azure/az-servicebus/","title":"Azure Service Bus","text":""},{"location":"services/azure/az-servicebus/#connection-code","title":"Connection Code","text":"<pre><code>pip install azure-servicebus==7.12.2\n</code></pre>"},{"location":"services/azure/az-servicebus/#read-mores","title":"Read Mores","text":"<ul> <li>Azure Service Bus client library for Python - version 7.12.3</li> </ul>"},{"location":"services/azure/az-storage-account/","title":"Azure Storage Account","text":""},{"location":"services/azure/az-storage-account/#fqdn","title":"FQDN","text":"Type FQDN Blob <code>https://{storage-account-name}.blob.core.windows.net/</code> File <code>https://{storage-account-name}.file.core.windows.net/</code> Queue <code>https://{storage-account-name}.queue.core.windows.net/</code> Table <code>https://{storage-account-name}.table.core.windows.net/</code> DataLake <code>https://{storage-account-name}.dfs.core.windows.net/</code> Static Web <code>https://{storage-account-name}.z23.web.core.windows.net/</code>"},{"location":"services/azure/az-storage-account/#services","title":"Services","text":""},{"location":"services/azure/az-storage-account/#storage-datalake","title":"Storage DataLake","text":""},{"location":"services/azure/az-storage-account/#storage-queue","title":"Storage Queue","text":""},{"location":"services/azure/az-storage-account/#examples","title":"Examples","text":"<ul> <li> Azure SDK Python - Storage Queue Sample</li> <li> Azure Storage Queue Samples</li> </ul>"},{"location":"services/azure/az-vnet/","title":"Azure VNet","text":"<p>Azure Virtual Network (VNet) is a representation of your own network in the cloud. It is a logical isolation of the Azure cloud dedicated to your subscription.</p>"},{"location":"services/azure/az-vnet/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/az-vnet/#components","title":"Components","text":""},{"location":"services/azure/az-vnet/#subnets","title":"Subnets","text":"<p>Each Virtual Network can be divided into sub parts, these sub parts are called Subnets.</p> <p>A subnet can further be divided into:</p> <ul> <li>Private Subnet \u2014 A network in which there is no internet access.</li> <li>Public Subnet \u2014 A network in which there is internet access.</li> </ul> <p></p>"},{"location":"services/azure/az-vnet/#network-security-groups","title":"Network Security Groups","text":"<p>A network security group is a set of rules or policies for incoming and outgoing requests. A network security group is generally attached at the subnet level. The advantage of applying network security groups to Subnets is that we can control the data flow. This is important in terms of network security. With an NSG, we can define the IP range for the source of incoming requests.</p> <p>But first, let me show you how the final architecture for a Virtual Network looks like:</p> <p></p>"},{"location":"services/azure/az-vnet/#create-vnet","title":"Create VNet","text":"<ul> <li>On Azure Portal &gt; Go to Virtual Network &gt; Click Create</li> </ul> <p>Note</p> <p>This is how Virtual Network works:</p> <ul> <li>First you create a virtual network.</li> <li>Then, in this virtual network you create subnets.</li> <li>You associate each subnet with the respective Virtual Machines or Cloud Instances.</li> <li>Attach the relevant Network Security Group to each subnet.</li> <li>Configure the properties in the NSGs and you are set!</li> </ul>"},{"location":"services/azure/az-vnet/#read-mores","title":"Read Mores","text":"<ul> <li> Azure Virtual Network For Beginners \u2014 Securing Your Applications Using VPC</li> <li> Beginner\u2019s Guide to Azure Virtual Networks</li> </ul>"},{"location":"services/azure/batch/","title":"Azure Batch","text":"<p>Azure Batch is a robust service that provides parallel batch processing to execute intensive workloads of varying size of Azure Compute. It creates a pool of compute nodes (Virtual Machines) to tackle heavy loads.</p> Azure Batch Architecture"},{"location":"services/azure/batch/#typical-workload-applications","title":"Typical Workload Applications","text":"<p>The data processing remains same as mentioned in Azure batch processing with the only difference being the tasks act as the intermediate between I/O container and workload application present in the compute node.</p> <p>Azure Batch has the capability to push the file from Cloud storage to the node before starting the task, and after completion of the task, push it back to Cloud storage.</p>"},{"location":"services/azure/batch/#cloud-aware-application","title":"Cloud-Aware Application","text":"<p>Consider this use case based on a sample project in GitHub (Ref1)*. In this instance, the application is aware of the existence of Cloud and can interact with it.</p> Cloud-Aware Application <p>The block diagram above is an extension of the detailed diagram represented in the Azure batch processing section above. The overall execution process is the same, except one special attribute that the workload application is aware of the Cloud. It is capable to execute read and write operations on Cloud storage.</p>"},{"location":"services/azure/batch/#legacy-application","title":"Legacy Application","text":"<p>This includes standalone applications that do not include Cloud as part of the overall architecture or applications which organizations implemented before Cloud became mainstream.</p> Legacy Application <p>Let\u2019s review an example of an application which is not aware of the cloud. Consider this sample project (Ref 2)where an application takes MP4 files from the filesystem and converts them into an AVI format using a FFmpeg tool (which is not Cloud aware) and then saves them to the filesystem.</p>"},{"location":"services/azure/batch/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/#create-azure-batch-pool","title":"Create Azure Batch Pool","text":"<pre><code>{\n  \"properties\": {\n    \"vmSize\": \"STANDARD_D2a_V4\",\n    \"deploymentConfiguration\": {\n      \"virtualMachineConfiguration\": {\n        \"imageReference\": {\n          \"publisher\": \"canonical\",\n          \"offer\": \"0001-com-ubuntu-server-jammy\",\n          \"sku\": \"22_04-lts\",\n          \"version\": \"latest\"\n        },\n        \"nodeAgentSKUId\": \"batch.node.ubuntu 22.04\"\n      }\n    },\n    \"scaleSettings\": {\n      \"autoScale\": {\n        \"evaluationInterval\": \"PT5M\",\n        \"formula\": \"samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 5);\\r\\ncappedPoolSize = 1;\\r\\nAvgActiveTask = samples&lt; 70 ? max(0,$ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\\r\\nAvgRunningTask = samples&lt; 70 ? max(0,$RunningTasks.GetSample(1)) : avg($RunningTasks.GetSample(1 * TimeInterval_Minute, 10 * TimeInterval_Minute));\\r\\n$TargetDedicatedNodes = 0;\\r\\nActiveTask = AvgActiveTask &gt; 0 ? 1 : 0;\\r\\nRunningTask = AvgRunningTask &gt; 0 ? 1 : 0;\\r\\n$TargetLowPriorityNodes = min(cappedPoolSize,max(ActiveTask,RunningTask));\\r\\n// Set node deallocation mode - keep nodes active only until tasks finish\\r\\n$NodeDeallocationOption = taskcompletion;\"\n      }\n    },\n    \"startTask\": {\n      \"commandLine\": \"/bin/bash -c \\\"echo 'Set Python 3.10' &amp;&amp; sudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp; echo '########## Add PPA Repository ##########' &amp;&amp; sudo apt update &amp;&amp; sudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp; echo '########## Install Python V3.8 ##########' &amp;&amp; sudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp; sudo apt -y install python3.8-dev &amp;&amp; sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp; sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp; sudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp; python3 --version &amp;&amp; sudo apt -y install python3-pip &amp;&amp; sudo apt -y install python3.8-distutils &amp;&amp; python3 -m pip install --upgrade pip &amp;&amp; echo '########## Setting Others Configuration ##########' &amp;&amp; sudo curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp; sudo curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp; sudo ACCEPT_EULA=Y apt install -y msodbcsql17 &amp;&amp; sudo ACCEPT_EULA=Y apt install -y mssql-tools &amp;&amp; echo 'export PATH=\\\\\\\"$PATH:/opt/mssql-tools/bin\\\\\\\"' &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc &amp;&amp; sudo apt -y install unixodbc-dev &amp;&amp; echo '########## Start Install Python Library ##########' &amp;&amp; pip3 install azure-core==1.17.0 &amp;&amp; pip3 install azure-storage-blob==12.8.1 &amp;&amp; pip3 install networkx==2.5 &amp;&amp; pip3 install numpy==1.19.5 &amp;&amp; pip3 install pandas==1.1.3 &amp;&amp; pip3 install pyarrow==1.0.1 &amp;&amp; pip3 install pyodbc==4.0.35 &amp;&amp; pip3 install pythainlp==2.3.0 &amp;&amp; pip3 install rapidfuzz==1.3.3 &amp;&amp; pip3 install scikit-learn==0.24.1 &amp;&amp; pip3 install scipy==1.6.0 &amp;&amp; pip3 install torch==1.7.1 &amp;&amp; pip3 install tqdm==4.58.0 &amp;&amp; pip3 install azure-keyvault-secrets==4.3.0 &amp;&amp; pip3 install azure-identity==1.6.1 &amp;&amp; pip3 install cffi==1.14.6 &amp;&amp; pip install azure-storage-file-datalake==12.4.0 &amp;&amp; pip install duckdb==0.2.9 &amp;&amp; pip install Office365-REST-Python-Client==2.3.8 &amp;&amp; pip install openpyxl==3.0.9 &amp;&amp; pip install xlsxwriter &amp;&amp; pip install xlrd==1.2.0 &amp;&amp; pip install pytz==2021.1\\\"\",\n      \"userIdentity\": {\n        \"autoUser\": {\n          \"scope\": \"Pool\",\n          \"elevationLevel\": \"Admin\"\n        }\n      },\n      \"maxTaskRetryCount\": 1,\n      \"waitForSuccess\": true\n    },\n    \"taskSlotsPerNode\": 2,\n    \"taskSchedulingPolicy\": {\n      \"nodeFillType\": \"pack\"\n    }\n  },\n  \"identity\": {\n    \"type\": \"UserAssigned\",\n    \"userAssignedIdentities\": {\n      \"/subscriptions/{tenant-id}/resourceGroups/{resource-group-name}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{managed-id-name}\": {}\n    }\n  }\n}\n</code></pre>"},{"location":"services/azure/batch/#python-tutorial","title":"Python Tutorial","text":"<ul> <li> Azure Content - Azure Batch Python Tutorial</li> </ul>"},{"location":"services/azure/batch/#read-mores","title":"Read Mores","text":"<ul> <li> Running heavy workloads using Azure Batch Processing</li> <li> Azure Batch cloud scale containers for HPC</li> </ul>"},{"location":"services/azure/batch/az-ba-auto-scalable/","title":"Auto Scalable","text":"<p>Autoscale is not (currently) intended as a sub 1M response to changes but rather to adjust the size of your pool gradually as you run a workload.</p> <p>Quote</p> <p>Batch uses your formula to determine the target number of compute nodes in the pool for the next interval of processing.</p> <p>Since we only evaluate the formula every ~15m it is not like your pool is going to immediately respond to new task pressure if left to its own devices. On the other hand if you are running a long-lived pool, and you want to respond to changes over the course of a day (for example day/night discrepancies in task load) then autoscale is a good fit .</p>"},{"location":"services/azure/batch/az-ba-auto-scalable/#examples","title":"Examples","text":"<p>Simaple re-scale dedicate node (<code>$TargetDedicated</code>) that base on running and active tasks that exists on the Pool,</p> <pre><code>$NodeDeallocationOption = taskcompletion;\n\nminNodes = 1;\nmaxNodes = 10;\n\nactiveTasks = $ActiveTasks.GetSample(1);\nrunningTasks = $RunningTasks.GetSample(1);\n\ntotalTasks = activeTasks + runningTasks;\n\nnodes = min(max(minNodes, totalTasks), maxNodes);\n$TargetDedicated = nodes;\n</code></pre> <p>Note</p> <p>If you want to scale low-priority node, you have change variable to <code>$TargetLowPriorityNodes</code></p>"},{"location":"services/azure/batch/az-ba-auto-scalable/#low-priority-nodes","title":"Low-Priority Nodes","text":"<p>Set autoscale evaluation interval to <code>15m 00s</code>.</p> Using Working Hour and WeekdayUse Running Task Sample <pre><code>// Fix dedicate target node to zero value\n$TargetDedicatedNodes = 0;\n\n// Get pending tasks for the past 15 minutes.\nsamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\n\n// Catch current value of low-priority node.\nCurr_TargetLowPriorityNodes = $TargetLowPriorityNodes;\n\n// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 19) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 20;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the average of last sample point between 1 and 2 minutes.\nAvgActiveTask = samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\n\n// Fix capacity of the pool sizes for low-priority node.\nCapped_TargetLowPriorityNodes = 1;\n\n// Set calculation low-priority target node value by the minimum value of\n// this capacity and average task value.\nCal_TargetLowPriorityNodes = min(Capped_TargetLowPriorityNodes, AvgActiveTask);\n\n// Set low-priority target node.\n$TargetLowPriorityNodes = $IsWorkingWeekdayHour ? max(Cal_TargetLowPriorityNodes, Curr_TargetLowPriorityNodes) : 0;\n\n// Set node de-allocation mode - keep nodes active only until tasks finish.\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <pre><code>// Fix dedicate target node to zero value\n$TargetDedicatedNodes = 0;\n\n// Get pending tasks for the past 5 minutes.\nSamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 5);\n\n// Fix capacity of the pool sizes\nCapped_PoolSize = 1;\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the average of last sample point between 1 and 2 minutes.\n// (for running task we use last sample point between 1 and 10 minutes).\nAvgActiveTask = Samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : avg($ActiveTasks.GetSample(1 * TimeInterval_Minute, 2 * TimeInterval_Minute));\nAvgRunningTask = Samples &lt; 70 ? max(0, $RunningTasks.GetSample(1)) : avg($RunningTasks.GetSample(1 * TimeInterval_Minute, 10 * TimeInterval_Minute));\n\nActiveTask = AvgActiveTask &gt; 0 ? 1 : 0;\nRunningTask = AvgRunningTask &gt; 0 ? 1 : 0;\n\n// Set low-priority target node by the minimum value\n$TargetLowPriorityNodes = min(Capped_PoolSize, max(ActiveTask, RunningTask));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre>"},{"location":"services/azure/batch/az-ba-auto-scalable/#dedicated-nodes","title":"Dedicated Nodes","text":"Using Active and Running Task SampleUsing Working Hour and WeekdayUsing Pending Task SampleUsing Max Task Per NodeUsing Working Hour and Weekday with Minimum <pre><code>// Get the average of last sample point between 1 and 2 minutes.\nAvgActiveTask = $ActiveTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\nAvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n\n// Combine the average of active and running tasks together.\nTotalTasksVector = AvgActiveTask + AvgRunningTask;\n\nvmsRequiredVector = TotalTasksVector / 4;\nvmsRequired = avg(vmsRequiredVector);\n\n// Set dedicated target node by the minimum value from the vsm required\n// value and capacity of 60 nodes.\n$TargetDedicated = min(max(vmsRequired, 1), 60);\n</code></pre> <p>Warning</p> <p>We strongly recommend you avoid of using <code>GetSample(1)</code> in your autoscale formulas. \\ This is because <code>GetSample(1)</code> is just saying \"give me the last sample you have, no matter how long ago you got it\"  \\ Since you will use these samples to grow/shrink your pool (and your pool costs you money) we recommend that you base the formula on more than 1 samples worth of data. Instead, we suggest you do some trending type analysis and grow your pool based on that.</p> <p>In addition to the best practices comments that mentioned above, you have hit 2 different bugs:</p> <ul> <li> <p>The last 1-2 samples of <code>$RunningTasks</code> are almost always <code>0</code>, so <code>.GetSamples(1)</code>   on <code>RunningTasks</code> often will return <code>0</code> even if there are some running tasks.   We will investigate this and work on a fix, but in the meantime adhering to the   best practice of avoiding <code>GetSamples(1)</code> will help you avoid this issue</p> </li> <li> <p>This one is more painful -- right now there is a bug where even when <code>MultipleTasksPerVM</code>   is not set to the default of 1, <code>$RunningTasks</code> will only report 1 running task   (even though there may be up to <code>N</code>, where <code>N == MaxTasksPerVM</code>).</p> </li> </ul> <p>We're already tracking this bug in our backlog and will get to it ASAP.</p> <p>In the meantime, you can probably edit your formula to think of <code>$RunningTasks</code>   as <code>RunningVMs</code> instead -- which should be able to get you close to what you   want.</p> <p>See the following formula for an example of what I mean:</p> <pre><code>AvgActiveTask = $ActiveTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\nAvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n\nvmsRequiredVector = AvgActiveTask / 4 + AvgRunningTask;\nvmsRequired = avg(vmsRequiredVector);\n\n// Set dedicated target node by the minimum value from the vsm required\n// value and capacity of 60 nodes.\n$TargetDedicated = min(max(vmsRequired, 1), 60);\n</code></pre> <p>Note</p> <p>You can use the <code>.GetSample(Interval Lookback Start, Interval Lookback End)</code> API to get a vector of samples, for example:</p> <pre><code>AvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second);\n</code></pre> <p>Might return:</p> <pre><code>AvgRunningTask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1];\n</code></pre> <p>Or, if you would like more certainty, you can force the evaluation to fail if there are less than a certain percentage of samples (here percentage means that if in a given time interval there were supposed to be <code>60</code> samples, but actually due to networking failures or other issues we were only able to gather 30 samples, the percentage would be 50%).</p> <p>This is how you specify a percentage (note the <code>60</code> as the 3rd parameter):</p> <pre><code>AvgRunningTask = $RunningTasks.GetSample(60 * TimeInterval_Second, 120 * TimeInterval_Second, 60);\n</code></pre> <p>When specifying a time range, always start with the time range starting at least 1m ago, since it takes about 1m for samples to propagate through the system, so samples in the range <code>(0 * TimeInterval_Second, 60 * TimeInterval_Second)</code> will often not be available. Again you can use the percentage API to force a particular sample percentage.</p> <p>Read More: Autoscale Formula Improvements Needed</p> <pre><code>// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 17) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 18;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// Set dedicated target node to 20 if datetime in range else 10.\n$TargetDedicated = $IsWorkingWeekdayHour ? 20 : 10;\n</code></pre> <pre><code>// Get pending tasks for the past 15 minutes.\nSamples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\n\n// If we have less than 70% data points, we use the last sample point,\n// otherwise we use the maximum of last sample point and the history average.\n$Tasks = Samples &lt; 70 ? max(0, $ActiveTasks.GetSample(1)) : max($ActiveTasks.GetSample(1), avg($ActiveTasks.GetSample(TimeInterval_Minute * 15)));\n\n// If number of pending tasks is not 0, set target node to pending tasks,\n// otherwise half of current dedicated target node.\n$TargetVMs = $Tasks &gt; 0 ? $Tasks : max(0, $TargetDedicated / 2);\n\n// The pool size is capped at 20, if target node value is more than that,\n// set it to 20. This value should be adjusted according to your use case.\n$TargetDedicated = max(0, min($TargetVMs, 20));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <p>Another example that adjusts the pool size based on the number of tasks, this formula also takes into account the <code>MaxTasksPerComputeNode</code> value that has been set for the pool. This is particularly useful in situations where parallel task execution on compute nodes is desired.</p> <pre><code>// Determine whether 70% of the samples have been recorded in the past 15 minutes.\n// If not, use last sample.\nSamples = $ActiveTasks.GetSamplePercent(TimeInterval_Minute * 15);\n$Tasks = Samples &lt; 70 ? max(0,$ActiveTasks.GetSample(1)) : max( $ActiveTasks.GetSample(1),avg($ActiveTasks.GetSample(TimeInterval_Minute * 15)));\n\n// Set the number of nodes to add to one-fourth the number of active tasks\n// (the MaxTasksPerComputeNode property on this pool is set to 4, adjust\n// this number for your use case)\n$Cores = $TargetDedicated * 4;\n$ExtraVMs = (($Tasks - $Cores) + 3) / 4;\n$TargetVMs = ($TargetDedicated + $ExtraVMs);\n\n// Attempt to grow the number of compute nodes to match the number of active\n// tasks, with a maximum of 3\n$TargetDedicated = max(0, min($TargetVMs, 3));\n\n// Set node de-allocation mode - keep nodes active only until tasks finish\n$NodeDeallocationOption = taskcompletion;\n</code></pre> <pre><code>// Set maximum node limit to 10.\n$MaxComputeNodeLimit = 10;\n\n// Get current time with timezone that add 7 hours.\n$CurTime = time() + 7 * TimeInterval_Hour;\n\n// Set working hours (8 - 17) and weekday (Mon - Fri) flag.\n$WorkHours = $CurTime.hour &gt;= 8 &amp;&amp; $CurTime.hour &lt; 18;\n$IsWeekday = $CurTime.weekday &gt;= 1 &amp;&amp; $CurTime.weekday &lt;= 5;\n$IsWorkingWeekdayHour = $WorkHours &amp;&amp; $IsWeekday;\n\n// Set minimum capacity of target node to 1 if datetime in range else 0\n$MinCapacity = $IsWorkingWeekdayHour ? 1 : 0;\n\n// Get the last sample point of active and running tasks.\n$LastSampledActiveTasks = $ActiveTasks.GetSample(1);\n$LastSampledRunningTasks = $RunningTasks.GetSample(1);\n$RunningAndWaiting = max($LastSampledActiveTasks, 1) + max($LastSampledRunningTasks, 1);\n\n$NeedCompute = $RunningAndWaiting &gt;= 1;\n\n$NodesToAllocate = $RunningAndWaiting &gt; $MaxComputeNodeLimit ? $MaxComputeNodeLimit : $RunningAndWaiting;\n$TargetDedicated = $NeedCompute ? $NodesToAllocate : $MinCapacity;\n</code></pre>"},{"location":"services/azure/batch/az-ba-auto-scalable/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft Azure Batch Automatic Scaling</li> <li> Azure Batch Pool Auto Scale Formulas</li> <li> Azure Content: Batch Automatic Scaling</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/","title":"Run PySpark","text":"<p>You simply wrap your Spark code in a Docker container, and schedule 100 of those containers on a pool of 20 Azure Batch nodes, and let them execute the 100 jobs in parallel. You get to use Low Priority VMs out of the box, and you only pay for what you need. No long-running Spark or Hadoop clusters.</p> <p>In order to run this example, we need to set up:</p> <ul> <li>Azure Storage Accounts to read and write your data</li> <li>Azure Container Registry to store the Docker container</li> <li>Azure Batch Accounts to run your batch jobs in.</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/#getting-started","title":"Getting Started","text":"<p>Chances are you already have these running in Azure. If not, it\u2019s simple click and install on the Azure Portal. For each of these 3 services, you need to get the Keys and add them to a <code>config.py</code>:</p> config.py<pre><code>STORAGE_ACCOUNT_NAME = \"&lt;storage-account-name&gt;\"\nSTORAGE_ACCOUNT_KEY = \"****\"\n\nBATCH_ACCOUNT_NAME = \"&lt;batch-account-name&gt;\"\nBATCH_ACCOUNT_KEY = \"****\"\nBATCH_ACCOUNT_URL = \"https://&lt;batch-account-name&gt;.westeurope.batch.azure.com\"\n\nACR_LOGINSERVER = \"&lt;registry-name&gt;.azurecr.io\"\nACR_USERNAME = \"&lt;registry-name&gt;\"\nACR_PASSWORD = \"****\"\n</code></pre>"},{"location":"services/azure/batch/az-ba-run-pyspark/#create-the-spark-job-script","title":"Create the Spark job script","text":"<p>For this demo, we need a simple spark job. Any job will do, really, The only thing you need to be aware of, is that it has to be able to read from Blob Storage.</p> requirements.txt<pre><code>azure\nazure-storage\nazure-storage-blob\npyspark==2.4.0\n</code></pre> <pre><code>import argparse\nimport config\nfrom pyspark.sql import SparkSession\n\ndef get_azure_spark_connection(storage_account_name, storage_account_key):\n    spark = (\n        SparkSession.builder\n            .config('spark.jars.packages', 'org.apache.hadoop:hadoop-azure:2.7.3')\n            .config('spark.hadoop.fs.azure', \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n            .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)\n            .appName(\"AzureSparkDemo\")\n            .getOrCreate()\n    )\n\n    (\n        spark.sparkContext._jsc\n            .hadoopConfiguration()\n            .set(\"fs.wasbs.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n    )\n    return spark\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--input\", help=\"input file to parse\", type=str)\n    parser.add_argument(\"-o\", \"--output\", help=\"result file to write\", type=str)\n    args = parser.parse_args()\n    spark = get_azure_spark_connection(\n        storage_account_name=config.STORAGE_ACCOUNT_NAME,\n        storage_account_key=config.STORAGE_ACCOUNT_KEY\n    )\n    df = (\n        spark.read.option(\"header\", \"true\")\n            .option(\"delimiter\", \",\")\n            .option(\"inferSchema\", \"true\")\n            .csv(args.input)\n    )\n    df.registerTempTable(\"airlines\")\n    result = spark.sql(\"\"\"\n      SELECT Year,\n          Month,\n          DayOfMonth,\n          avg(ArrDelay) as avg_ArrDelay,\n          avg(DepDelay) as avg_DepDelay\n      FROM airlines\n      GROUP BY Year, Month, DayOfMonth\n    \"\"\")\n    (\n        result\n            .repartition(1)\n            .write\n            .mode(\"overwrite\")\n            .parquet(args.output)\n    )\n</code></pre>"},{"location":"services/azure/batch/az-ba-run-pyspark/#dockerize","title":"Dockerize","text":"<p>We wrap this job in a Docker container and push it to the Azure Container Registry (ACR).</p> <p>A simple <code>Dockerfile</code> can be as follows:</p> Dockerfile<pre><code>FROM python:3.6\n\n# Install OpenJDK 8, and monitoring tooling\nRUN \\\n  apt-get update &amp;&amp; \\\n  apt-get install -y openjdk-8-jdk htop bmon &amp;&amp; \\\n  rm -rf /var/lib/apt/lists/*\n\nRUN pip install --upgrade setuptools\nCOPY requirements.txt /\nRUN pip install -r /requirements.txt\n\nENV PATH=$PATH:/src\nENV PYTHONPATH /src\n\nADD ./ /src\nWORKDIR /src/\n</code></pre> <p>It starts from a <code>python3.6</code> base image, installs <code>openjdk</code> and some monitoring tooling, and then all the python dependencies as listed in <code>requirements.txt</code>. Finally, it copies your code to a <code>src</code> folder.</p> <p>You can build it with the command:</p> <pre><code>docker build -t dataminded/spark_on_azure_batch_demo .\n</code></pre> <p>Now you could already run this script locally as follows:</p> <pre><code>docker run dataminded/spark_on_azure_batch_demo python /src/airline_analytics.py \\\n  --input wasbs://demo@datamindeddata.blob.core.windows.net/raw/airlines/2007.csv.bz2 \\\n  --output wasbs://demo@datamindeddata.blob.core.windows.net/aggregated/airlines/2007.parquet\n</code></pre> <p>Next, you need to log in to the Azure Container Registry, with <code>&lt;registry-name&gt;</code> replaced by your own registry of course. Enter the admin username you see on the Azure portal:</p> <pre><code>$ docker login &lt;registry-name&gt;.azurecr.io\n$ docker tag dataminded/spark_on_azure_batch_demo:latest &lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo:latest\n$ docker push &lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo:latest\n</code></pre> <p>Note</p> <p>That the first time you push this Docker container, it will be huge, about 1GB: it has an ubuntu image, a bunch of spark libraries, a JDK, etc. The next push you do of this container, will only push your latest code changes.</p>"},{"location":"services/azure/batch/az-ba-run-pyspark/#run-on-azure-batch","title":"Run on Azure Batch","text":"<p>We\u2019ve written a sample Spark application, with the right Azure libraries. We\u2019ve dockerized it and uploaded it to Azure Container Registry. Now we\u2019re finally ready to run our code on Azure Batch. This is going to be quite a long python script, but it does a lot of things as well:</p> <ul> <li> <p>It connects to your container registry and uses those docker images</p> </li> <li> <p>Create a Pool if it doesn't exist yet. Here, you can configure which kind of   VMs and how many of them you want in your pool. And more importantly, you can   specify that it are Low Prio VMs, which are cheap.</p> </li> <li> <p>Create a Job within the Pool</p> </li> <li> <p>Create a separate task to process each year of data. In a real-life situation,   you would have a task for each day of data.</p> </li> </ul> <pre><code>import datetime\nimport sys\nimport time\nimport azure.batch.batch_auth as batch_auth\nimport azure.batch.batch_service_client as batch\nimport azure.batch.models as batchmodels\nimport config\n\nsys.path.append('docs')\nsys.path.append('')\n\nIMAGE_NAME = '&lt;registry-name&gt;.azurecr.io/&lt;registry-name&gt;/spark_on_azure_batch_demo'\nIMAGE_VERSION = 'latest'\n\n\ndef create_pool(\n        batch_service_client,\n        container_registry,\n        image_name,\n        pool_id,\n        pool_vm_size,\n        pool_node_count,\n        skip_if_exists = True\n):\n  print(f'Creating pool [{pool_id}]...')\n\n  container_conf = batch.models.ContainerConfiguration(\n    container_image_names=[image_name],\n    container_registries=[container_registry]\n  )\n\n  image_ref_to_use = batch.models.ImageReference(\n    publisher='microsoft-azure-batch',\n    offer='ubuntu-server-container',\n    sku='16-04-lts',\n    version='latest',\n  )\n\n  new_pool = batch.models.PoolAddParameter(\n    id=pool_id,\n    virtual_machine_configuration=batch.models.VirtualMachineConfiguration(\n      image_reference=image_ref_to_use,\n      container_configuration=container_conf,\n      node_agent_sku_id='batch.node.ubuntu 16.04',\n    ),\n    vm_size=pool_vm_size,\n    target_low_priority_nodes=pool_node_count)\n\n  if not skip_if_exists or not batch_service_client.pool.exists(pool_id):\n    batch_service_client.pool.add(new_pool)\n\n\ndef create_job(batch_service_client, job_id, pool_id):\n  print('Creating job [{}]...'.format(job_id))\n\n  job = batch.models.JobAddParameter(\n    id=job_id,\n    pool_info=batch.models.PoolInformation(pool_id=pool_id))\n\n  batch_service_client.job.add(job)\n\n\ndef add_task(\n    batch_service_client,\n    image_name,\n    image_version,\n    job_id,\n    command,\n    name\n):\n  user = batchmodels.UserIdentity(\n      auto_user=batchmodels.AutoUserSpecification(\n        elevation_level=batchmodels.ElevationLevel.admin,\n        scope=batchmodels.AutoUserScope.task,\n      )\n  )\n\n  task_id = name\n  task_container_settings = batch.models.TaskContainerSettings(\n    image_name=image_name + ':' + image_version,\n    container_run_options='--rm -p 4040:4040')\n  task = batch.models.TaskAddParameter(\n    id=task_id,\n    command_line=command,\n    container_settings=task_container_settings,\n    user_identity=user\n  )\n  print(\"running \" + command)\n\n  batch_service_client.task.add(job_id, task)\n\n\ndef wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n  timeout_expiration = datetime.datetime.now() + timeout\n\n  print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n        .format(timeout), end='')\n\n  while datetime.datetime.now() &lt; timeout_expiration:\n    print('.', end='')\n    sys.stdout.flush()\n    tasks = batch_service_client.task.list(job_id)\n\n    incomplete_tasks = [task for task in tasks if\n                        task.state != batchmodels.TaskState.completed]\n    if not incomplete_tasks:\n      print()\n      return True\n    else:\n      time.sleep(1)\n\n  print()\n  raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n                     \"timeout period of \" + str(timeout))\n\n\ndef daterange(start_date, end_date):\n  for n in range(int((end_date - start_date).days)):\n    yield start_date + datetime.timedelta(n)\n\n\nif __name__ == '__main__':\n\n  start_time = datetime.datetime.now().replace(microsecond=0)\n  print('Start batch job {}'.format(start_time))\n  print()\n\n  image_name = IMAGE_NAME\n  image_version = IMAGE_VERSION\n\n  # Create the blob client, for use in obtaining references to\n  # blob storage containers and uploading files to containers.\n\n  credentials = batch_auth.SharedKeyCredentials(config.BATCH_ACCOUNT_NAME,\n                                                config.BATCH_ACCOUNT_KEY)\n\n  batch_client = batch.BatchServiceClient(credentials,\n                                          base_url=config.BATCH_ACCOUNT_URL)\n\n  job_id: str = f'Job-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}'\n  pool_id: str = 'Airlines'\n\n  try:\n    container_registry = batch.models.ContainerRegistry(\n      registry_server=config.ACR_LOGINSERVER,\n      user_name=config.ACR_USERNAME,\n      password=config.ACR_PASSWORD)\n\n    create_pool(\n      batch_service_client=batch_client,\n      pool_id=pool_id,\n      container_registry=container_registry,\n      image_name=image_name,\n      pool_node_count=3,\n      pool_vm_size='Standard_E2s_v3',\n      skip_if_exists=True)\n\n    # Create the job that will run the tasks.\n    create_job(batch_client, job_id, pool_id)\n\n    for year in range(2001, 2009):\n      command = (\n        f\"python /src/airline_analytics.py \"\n        f\"--input wasbs://demo@datamindeddata.blob.core.windows.net/raw/airlines/{year}.csv.bz2\"\n        f\"--output wasbs://demo@datamindeddata.blob.core.windows.net/aggregated/airlines/{year}.parquet\"\n      )\n      add_task(\n        batch_service_client=batch_client,\n        image_name=image_name,\n        image_version=image_version,\n        job_id=job_id,\n        command=command,\n        name=f'airlines{year}'\n      )\n\n    # Pause execution until tasks reach Completed state.\n    wait_for_tasks_to_complete(\n      batch_client,\n      job_id,\n      datetime.timedelta(hours=2),\n    )\n    print(\n      \"Success! All tasks reached the 'Completed' state within the specified timeout period.\")\n\n  except batchmodels.BatchErrorException as err:\n    print(err)\n    raise\n\n  # Print out some timing info\n  end_time = datetime.datetime.now().replace(microsecond=0)\n  print()\n  print('Sample end: {}'.format(end_time))\n  print('Elapsed time: {}'.format(end_time - start_time))\n  print()\n</code></pre> <p>And finally, you will see your jobs run on Azure Batch:</p> <p></p> <p></p> <p>Warning</p> <p>That all nodes in a pool are completely separate. This is not a one big Spark cluster concept.</p>"},{"location":"services/azure/batch/az-ba-run-pyspark/#next-steps","title":"Next steps","text":"<p>This solves 90% of the needs of a project we\u2019re working on. However, there are always next steps:</p> <ul> <li>Security:</li> </ul> <p>Account keys are everywhere here. Maybe not in the git repo. But you are going   to fill them in and put them in a docker image. Which is a big No No. Better   would be to work with some sort of Role Based Access Control and a Vault,   where this job is allowed to access the vault for certain keys, and it is allowed   to read and write certain data from Blob storage</p> <ul> <li>Monitoring:</li> </ul> <p>The htop monitoring is nice, and you sure do look like a hardcore hacker with that CLI dashboard. But you would just like to see the Spark UI, really. Just like any normal human being.</p> <ul> <li>Scaling:</li> </ul> <p>Not all jobs fit on a single node. Although you can get up to 64 cores and 432GB (for not even $1 per hour in Low Prio), there might be use cases where you need more. Although, honestly, we all talk about big data a lot, but 99% of jobs that I\u2019ve seen, really don\u2019t need 432GB of RAM. But anyway, in those cases, AZTK is the way to go if you want to stick with Azure Batch.</p> <ul> <li>Auto scaling:</li> </ul> <p>Even better than scaling, we would also like to auto-scale our pool. Now it\u2019s just a fixed size, which you have to turn off manually. Don\u2019t forget, or your \u201cIt\u2019s cheap!\u201d claim won\u2019t survive for long. Ideally, you want to autoscale to dozens or even hundreds of nodes, if you have a large workload. And then you want them to automatically be turned off when they are idle for a couple of minutes.</p> <ul> <li>Kubernetes: \\   We are big believers of Kubernetes as the next platform for big data analytics, and we\u2019re looking into whether it\u2019s stable enough to run Spark workloads. Since Spark 2.4.0, which got released yesterday, it also supports pyspark. The nice thing with this setup, is that you are already halfway there. You\u2019ve wrapped your spark jobs in a docker container, and you put the container in a private registry. Now it\u2019s just a matter of choosing where to launch it.</li> </ul>"},{"location":"services/azure/batch/az-ba-run-pyspark/#references","title":"References","text":"<ul> <li>Medium: Run Spark on Azure Batch using Azure Container Registry</li> <li>GitHub: Spark on Azure Batch Demo</li> </ul>"},{"location":"services/azure/batch/az-ba-start-task/","title":"Start Task","text":""},{"location":"services/azure/batch/az-ba-start-task/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-start-task/#command-line","title":"Command Line","text":"<p>Below CMD will remove newline charactor that come from Windows first and then it will execute the start task file with <code>bash</code>.</p> <pre><code>/bin/bash -c \"sed -i 's/\\r$//' start_task.sh &amp;&amp; bash ./start_task.sh\"\n</code></pre>"},{"location":"services/azure/batch/az-ba-start-task/#start-task-file","title":"Start Task File","text":"Python 3.8Python 3.11 start_task.sh<pre><code>#!/bin/bash\n\necho 'Update Ubuntu'\nsudo apt update\necho 'Import Python 3.8 PPA on Ubuntu'\nsudo add-apt-repository ppa:deadsnakes/ppa -y\nsudo apt update\nsudo apt -y install python3.8\nsudo update-alternatives --set python3 /usr/bin/python3.8\npython3 --version\n</code></pre> start_task.sh<pre><code>sudo export DEBIAN_FRONTEND=noninteractive\nsudo apt install software-properties-common\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update -y\nsudo apt -y install python3.11 python3-pip\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1\npython --version\necho $(date -u) '########## INSTALL python3.11-full DONE ##########'\nsudo systemctl restart walinuxagent.service\nsudo systemctl restart networkd-dispatcher.service\nsudo systemctl restart unattended-upgrades.service\necho \"########## INSTALL python3-pip DONE ##########\"\nsudo python -m pip install --upgrade pip\n</code></pre>"},{"location":"services/azure/batch/az-ba-start-task/#examples","title":"Examples","text":"ETL Python 3.8ETL Python 3.8 with UVODBC SQL ServerDockerGCloud start_task.sh<pre><code>#!/bin/bash\n\necho 'Set Python 3.10' &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp;\necho '########## Add PPA Repository ##########' &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\necho '########## Install Python V3.8 ##########' &amp;&amp;\nsudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp;\npython3 --version &amp;&amp;\nsudo apt -y install python3-pip &amp;&amp;\nsudo apt -y install python3.8-distutils &amp;&amp;\npython3 -m pip install --upgrade pip &amp;&amp;\necho '########## Start Install Python Library ##########' &amp;&amp;\npip3 install backports.zoneinfo &amp;&amp;\npip3 install azure-storage-file-datalake==12.4.0 &amp;&amp;\npip3 install azure-keyvault-secrets==4.3.0 &amp;&amp;\npip3 install azure-identity==1.6.1 &amp;&amp;\npip3 install cffi==1.16.0 &amp;&amp;\npip install google-cloud-bigquery==3.13.0 &amp;&amp;\npip install pandas_gbq==0.17.0 &amp;&amp;\npip install pandas==2.0.3 &amp;&amp;\npip install pyarrow==14.0.1 &amp;&amp;\npip install -U pytz\n</code></pre> start_task.sh<pre><code>#!/bin/bash\n\necho 'Set Python 3.10' &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.10 || echo 'Skipped: Set Python 3.10' &amp;&amp;\necho '########## Add PPA Repository ##########' &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\necho '########## Install Python V3.8 ##########' &amp;&amp;\nsudo apt -y install python3.8 || echo 'Skipped: Install Python 3.8' &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 &amp;&amp;\nsudo update-alternatives --set python3 /usr/bin/python3.8 &amp;&amp;\npython3 --version &amp;&amp;\nsudo apt -y install python3-pip &amp;&amp;\nsudo apt -y install python3.8-distutils &amp;&amp;\npython3 -m pip install --upgrade pip &amp;&amp;\necho '########## Start Install Python Library ##########' &amp;&amp;\necho $(date -u) &amp;&amp;\nsudo pip install --upgrade pip &amp;&amp;\nsudo pip install uv &amp;&amp;\nsudo uv pip install --system backports.zoneinfo &amp;&amp;\nsudo uv pip install --system azure-storage-file-datalake==12.4.0 &amp;&amp;\nsudo uv pip install --system azure-keyvault-secrets==4.3.0 &amp;&amp;\nsudo uv pip install --system azure-identity==1.6.1 &amp;&amp;\nsudo uv pip install --system cffi==1.16.0 &amp;&amp;\nsudo uv pip install --system google-cloud-bigquery==3.13.0 &amp;&amp;\nsudo uv pip install --system pandas_gbq==0.17.0 &amp;&amp;\nsudo uv pip install --system pandas==2.0.3 &amp;&amp;\nsudo uv pip install --system pyarrow==14.0.1 &amp;&amp;\nsudo uv pip install --system -U pytz\necho $(date -u) \"End of Python Library installation\"\n</code></pre> start_task.sh<pre><code>#!/bin/bash\n\necho '########## Installing Python 3.11 on Ubuntu 22.04 by using the PPA repository ##########' &amp;&amp;\nsudo apt install python3-apt --fix-missing &amp;&amp;\nsudo apt update &amp;&amp;\nsudo add-apt-repository ppa:deadsnakes/ppa || echo 'Skipped: Add Repository' &amp;&amp;\nsudo apt -y install python3.11 &amp;&amp;\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 2 &amp;&amp;\nsudo update-alternatives --config python3 &amp;&amp;\nsudo apt install python-is-python3 &amp;&amp;\npython --version &amp;&amp;\necho '########## Install Extras for Python 3.11 ##########' &amp;&amp;\nsudo apt-get install python3.11-full -y &amp;&amp;\nsudo apt-get install python3-pip -y &amp;&amp;\nsudo python3 -m pip install --upgrade pip &amp;&amp;\necho '########## Setting ODBC Libs ##########' &amp;&amp;\nsudo curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp;\nsudo curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp;\nsudo apt-get update &amp;&amp;\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17 &amp;&amp;\necho '--- Done: msodbcsql17' &amp;&amp;\nsudo ACCEPT_EULA=Y apt-get install -y mssql-tools &amp;&amp;\necho '--- Done: mssql-tools' &amp;&amp;\necho 'export PATH=\\\"$PATH:/opt/mssql-tools/bin\\\"' &gt;&gt; ~/.bashrc &amp;&amp;\nsource ~/.bashrc\n</code></pre> Note install_msodbcsql17.sh<pre><code>if ! [[ \"16.04 18.04 20.04 22.04\" == *\"$(lsb_release -rs)\"* ]];\nthen\n    echo \"Ubuntu $(lsb_release -rs) is not currently supported.\";\n    exit;\nfi\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo tee /etc/apt/trusted.gpg.d/microsoft.asc\ncurl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get install -y msodbcsql17\n# Optional: for bcp and sqlcmd\nsudo ACCEPT_EULA=Y apt-get install -y mssql-tools\necho 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n# Optional: for unixODBC development headers\nsudo apt-get install -y unixodbc-dev\n</code></pre> <p>Reference from How to Install Microsoft ODBC Driver for SQL Server on Ubuntu</p> <pre><code>sudo apt-get update\nsudo apt -y install curl apt-transport-https ca-certificates software-properties-common\nsudo mkdir -p /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod 755 /etc/apt/keyrings\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\necho \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg ] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" \\\n    | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nsudo usermod -aG docker $USER\nsudo chmod 666 /var/run/docker.sock\ndocker ps\n</code></pre> <pre><code>sudo mkdir -p /etc/apt/keyrings\nsudo curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg\nsudo chmod 755 /etc/apt/keyrings\nsudo chmod a+r /etc/apt/keyrings/cloud.google.gpg\necho \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" \\\n    | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\nsudo apt-get update\nsudo apt-get -y install google-cloud-cli\n</code></pre>"},{"location":"services/azure/batch/az-ba-start-task/#advance","title":"Advance","text":""},{"location":"services/azure/batch/az-ba-start-task/#wrapped-start-task","title":"Wrapped Start Task","text":"<pre><code>/bin/bash -c \"sed -i 's/\\r$//' start_task.sh &amp;&amp; bash ./start_task.sh\"\n</code></pre> <pre><code>#!/bin/bash\n\ncnt_startup=$( cat /root/cnt_startup.txt ) || echo \"0\" &gt; /root/cnt_startup.txt\ncnt_startup=$( cat /root/cnt_startup.txt )\n((cnt_startup++))\necho \"########## Start up count = [$cnt_startup] ##########\"\necho \"$cnt_startup\" &gt; /root/cnt_startup.txt\n\necho '########## Install Azure CLI for calling Azure function ##########'\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n\naz login --identity\nFUNC_APP=$( az keyvault secret show --name \"FUNCAPP\" --vault-name \"$KEYVAULT_NM\" --query \"value\" )\n# NOTE: Convert secret to percent encoding\nFUNC_APP=$( python3 -c \"import urllib.parse, sys; print(urllib.parse.quote(sys.argv[1]))\" \"$FUNCAPP\" )\n\n/bin/bash -c \"sed -i 's/\\r$//' start_task.sh &amp;&amp; bash ./start_task.sh\"\nif [ $? -eq 0 ]\nthen\n    echo \"########## Successfully start up task. ##########\"\nelse\n    echo \"########## Startup task fail, calling Azure function. ########## \"\n    # NOTE: Creating Azure function to check API Azure batch\n    curl --location \"$FUNC_URL?code=$FUNC_APP\" \\\n        --header 'Content-Type: application/json' \\\n        --data \"{\\\"pool_id\\\": \\\"$AZ_BATCH_POOL_ID\\\", \\\"node_id\\\": \\\"$AZ_BATCH_NODE_ID\\\", \\\"reboot_cnt\\\": $cnt_startup}\"\n    exit 1\nfi\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/","title":"Connect to Azure Services","text":""},{"location":"services/azure/batch/az-ba-to-az/#authentication","title":"Authentication","text":""},{"location":"services/azure/batch/az-ba-to-az/#using-user-assigned-managed-identity","title":"Using User-Assigned Managed Identity","text":"<p>Warning</p> <p>The system-assigned managed identity created in a Batch account is only used for retrieving customer-managed keys from the Key Vault. This identity is not available on Batch pools.</p>"},{"location":"services/azure/batch/az-ba-to-az/#1-create-user-assigned-managed-identity","title":"1) Create User-Assigned Managed Identity","text":"<ul> <li>In the Azure Portal  Go to Managed Identities  Click Create</li> <li>Add the managed identity information  Select Review + create</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#2-enable-azure-batch-account","title":"2) Enable Azure Batch Account","text":"<ul> <li>Go to Azure Batch Accounts  Click Pools  Select your Batch Pool name</li> <li>Go to Identity  Nav User assigned  Click Add</li> <li>Select your managed identity that was created from above  Click Add</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#3-credential-code","title":"3) Credential Code","text":"<p>Before getting managed identity, you should install the Azure client library;</p> <pre><code>pip install -U azure-identity\n</code></pre> <pre><code>from azure.identity import ManagedIdentityCredential\n\nmsi_credential = ManagedIdentityCredential()\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#using-certificate","title":"Using Certificate","text":"<p>The computational jobs running on Batch will need to use a certificate to prove their identity to Azure AD, so they can assume the identity of the App you registered.</p> <p>Danger</p> <p>the Azure Batch Account Certificates feature will be retired on February 29, 2024.</p>"},{"location":"services/azure/batch/az-ba-to-az/#1-generate-certificate","title":"1) Generate Certificate","text":"<p>Firstly we need to create a certificate which can be used for authentication. To do that we're going to generate a Certificate Signing Request (CSR) using <code>openssl</code>.</p> <pre><code>$ openssl req \\\n  -newkey rsa:4096 -nodes -keyout \"service-principal.key\" \\\n  -out \"service-principal.csr\"\n</code></pre> <p>We can now sign that Certificate Signing Request (CSR), in this example we're going to self-sign this certificate using the Key we just generated; however it's also possible to do this using a Certificate Authority. In order to do that we're again going to use <code>openssl</code></p> <pre><code>$ openssl x509 \\\n  -signkey \"service-principal.key\" \\\n  -in \"service-principal.csr\" \\\n  -req -days 365 \\\n  -out \"service-principal.crt\"\n</code></pre> <p>This <code>service-principal.crt</code> file you can upload to the App Registration and note the resulting thumbprint. Then, we have an App Registration with a related certificate. Finally, we can generate a <code>.pfx</code> file which can be used to authenticate with Azure:</p> <pre><code>$ openssl pkcs12 -export -out \"service-principal.pfx\" \\\n  -inkey \"service-principal.key\" \\\n  -in \"service-principal.crt\"\n</code></pre> <p>So we will use this <code>service-principal.pfx</code> file, providing the thumbprint we got when we uploaded the certificate to the App Registration for Azure Batch Account.</p> <p>Note</p> <p>We will actually need the thumbprint converted from its hexadecimal representation to base64. We can use sed to replace the colons and remove <code>SHA1 Fingerprint=</code> substring, <code>xxd</code> to convert to bytes, and <code>base64</code> to encode.</p> <pre><code>$ echo $(openssl x509 -in \"service-principal.csr\" -fingerprint -noout) \\\n  | sed 's/SHA1 Fingerprint=//g' \\\n  | sed 's/://g' \\\n  | xxd -r -ps \\\n  | base64\n</code></pre> <p>The <code>service-principal.csr</code> file contains the public key value of the self-signed certificate we generated. We will need to grab that value skipping the first and the last lines.</p> <pre><code>$ tail -n+2 service-principal.csr | head -n-1\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#2-assign-certificate-to-service-principle","title":"2) Assign Certificate to Service Principle","text":"<p>Sometimes called a public key, a certificate is the recommended credential type because they're considered more secure than client secrets.</p> <ul> <li>Go to Azure App registrations  Select <code>Certificates &amp; secrets</code>  Click <code>Certificates</code></li> <li>Upload the certificate that was created from above step. Select the file you   want to upload. It must be one of the following file types: <code>.cer</code>, <code>.pem</code>, <code>.crt</code>  Select <code>Add</code>.</li> </ul> <p>Warning</p> <p>This accepts the following file formats: <code>cer</code>, <code>pem</code> and <code>crt</code>.</p>"},{"location":"services/azure/batch/az-ba-to-az/#3-assign-certificate-to-batch-account","title":"3) Assign Certificate to Batch Account","text":"<p>Assigning the certificate to the account lets Batch assign it to the pools and then to the nodes.</p> <ul> <li>In the <code>Azure portal</code>, in <code>Batch accounts</code>, select your batch account.</li> <li>Select <code>Certificates</code>, select <code>Add</code>.</li> <li>Upload the <code>.pfx</code> file you generated and supply the password</li> <li>Pass the certificate thumbprint.</li> <li>Select <code>Create</code></li> </ul> <p>Now when you create a Batch pool, you can navigate to <code>Certificates</code> within the pool and assign the certificate that you created in your Batch account to that pool. When you do so, ensure you select <code>LocalMachine</code> for the store location. The certificate is loaded on all Batch nodes in the pool.</p> <p>In that setup, the certificates attached to the pool will be available in the folder defined by an environmental variable <code>AZ_BATCH_CERTIFICATES_DIR</code>.</p> <pre><code>${AZ_BATCH_CERTIFICATES_DIR}/sha1-${THUMBPRINT}.pfx\n${AZ_BATCH_CERTIFICATES_DIR}/sha1-${THUMBPRINT}.pfx.pw\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#4-credential-code","title":"4) Credential Code","text":"<p>Before getting managed identity, you should install the Azure client library;</p> <pre><code>pip install -U azure-identity\n</code></pre> <p>If you are using the Azure SDK for python, unfortunately the pfx format is not compatible with the SDK, so we need to convert it:</p> <pre><code>CERT_THUMBPRINT=&lt;your-cert-thumbprint&gt;;\nCERT_IN=\"${AZ_BATCH_CERTIFICATES_DIR}/sha1-${CERT_THUMBPRINT}.pfx\";\nCERT_OUT=\"${AZ_BATCH_CERTIFICATES_DIR}/cert.pem\";\nCERT_PWD=\"${CERT_IN}.pw\";\n</code></pre> <pre><code>$ openssl pkcs12 -in ${CERT_IN} -out ${CERT_OUT} -nokeys -nodes -password file:${CERT_PWD};\n$ openssl pkcs12 -in ${CERT_IN} -nocerts -nodes -password file:${CERT_PWD} \\\n  | openssl rsa -out ${AZ_BATCH_CERTIFICATES_DIR}/cert.key;\n$ cat ${AZ_BATCH_CERTIFICATES_DIR}/cert.key &gt;&gt; CERT_OUT;\n</code></pre> <p>With these steps, we have converted the <code>.pfx</code> certificate file to a <code>.pem</code> style certificate file, which is usable with Python:</p> <pre><code>import os\nfrom azure.identity import CertificateCredential\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ncertificate_credential = CertificateCredential(\n    tenant_id=os.environ[\"AZURE_TENANT_ID\"],\n    client_id=os.environ[\"CLIENT_ID\"],\n    certificate_path=f\"{CERT_PATH}/cert.pem\"\n)\n</code></pre> <p>Full Python scripts:</p> <pre><code>import os\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ndef gen_pem_cert(cert_thumbprint: str):\n    # Start pem certificate generation\n    os.system(\n        (\n            f\"openssl pkcs12 -in {CERT_PATH}/sha1-{cert_thumbprint}.pfx \"\n            f\"-out {CERT_PATH}/cert.pem -nokeys -nodes \"\n            f\"-password file:{CERT_PATH}/sha1-{cert_thumbprint}.pfx.pw \"\n            f\"2&gt;/dev/null\"\n        )\n    )\n    # Start RSA Key generation\n    os.system(\n        (\n            f\"openssl pkcs12 -in {CERT_PATH}/sha1-{cert_thumbprint}.pfx \"\n            f\"-nocerts -nodes \"\n            f\"-password file:{CERT_PATH}/sha1-{cert_thumbprint}.pfx.pw \"\n            f\"| openssl rsa -out {CERT_PATH}/cert.key \"\n            f\"2&gt;/dev/null\"\n        )\n    )\n    # Combine key with certificate\n    os.system(\n        f\"cat {CERT_PATH}/cert.key &gt;&gt; {CERT_PATH}/cert.pem\"\n    )\n\ndef rm_pem_cert():\n    # Start removing Certificate\n    os.system(f\"rm {CERT_PATH}/cert.key\")\n    os.system(f\"rm {CERT_PATH}/cert.pem\")\n</code></pre> <pre><code>import os\nfrom azure.identity import CertificateCredential\n\nCERT_PATH: str = os.environ.get('AZ_BATCH_CERTIFICATES_DIR')\n\ncredential = CertificateCredential(\n    tenant_id=tenant_id,\n    client_id=client_id,\n    certificate_path=f\"{CERT_PATH}/cert.pem\"\n)\n</code></pre> <p>Tip</p> <p>Wait for at least 15 minutes for role to propagate and then try to access after assign IAM Role to target service.</p>"},{"location":"services/azure/batch/az-ba-to-az/#key-vault","title":"Key Vault","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite","title":"Prerequisite","text":"<ul> <li>Go to Azure Key Vaults  Select your key vault name, <code>kv-demo</code></li> <li>On Access control (IAM)  Click Add    Assign Key Vault Secrets User to your user-assigned managed identity</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code","title":"Connection Code","text":"<p>Before develop code, you should install Azure Key Vault client library;</p> <pre><code>pip install azure-keyvault-secrets\n</code></pre> <p>Implement connection code to the Azure Batch Node that use to get any secrets from Azure Key Vault.</p> <pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\n\ndef secret_client(keyvault_name: str):\n    \"\"\"Return Secret Client from Managed Identity Authentication.\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return SecretClient(\n        vault_url=f\"https://{keyvault_name}.vault.azure.net\",\n        credential=msi_credential\n    )\n</code></pre> Reference Links <ul> <li>https://arsenvlad.medium.com/certificate-based-auth-with-azure-service-principals-from-linux-command-line-a440c4599cae</li> <li>https://msendpointmgr.com/2023/03/11/certificate-based-authentication-aad/</li> <li>https://learn.microsoft.com/en-us/azure/batch/managed-identity-pools</li> <li>https://medium.com/datamindedbe/how-to-access-key-vaults-from-azure-batch-jobs-34388b1adf46</li> <li>https://learn.microsoft.com/en-us/azure/batch/batch-customer-managed-key</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#synapse","title":"Synapse","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite_1","title":"Prerequisite","text":"<ul> <li> <p>Go to Azure Synapse SQL Pool  Create user from external     provider and grant permission of this user such as Read access,</p> <pre><code>CREATE USER [&lt;user-assigned-name&gt;] FROM EXTERNAL PROVIDER;\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code_1","title":"Connection Code","text":"<p>Before develop code, you should install DataFrame API library;</p> <pre><code>pip install arrow-odbc polars\n</code></pre> <p>Note</p> <p>I recommend <code>arrow-odbc</code> and <code>polars</code> for loadding performance.</p> <p>Read More about Polars to Syanpse</p> <pre><code>import os\nimport polars as pl\nfrom arrow_odbc import read_arrow_batches_from_odbc\n\nreader = read_arrow_batches_from_odbc(\n    query=\"SELECT * FROM &lt;schema-name&gt;.&lt;table-name&gt;\",\n    connection_string=(\n        f\"Driver={{ODBC Driver 17 for SQL Server}};\"\n        f\"Server={os.getenv('MSSQL_HOST')};\"\n        f\"Port=1433;\"\n        f\"Database={os.getenv('MSSQL_DB')};\"\n        f\"Authentication=ActiveDirectoryMsi;\"\n    ),\n    max_bytes_per_batch=536_870_912,  # Default: 2 ** 29 (536_870_912)\n    batch_size=1_000_000,  # Default: 65_535\n)\nreader.fetch_concurrently()\nfor batch in reader:\n    df: pl.DataFrame = pl.from_arrow(batch)\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-az/#datalake","title":"DataLake","text":""},{"location":"services/azure/batch/az-ba-to-az/#prerequisite_2","title":"Prerequisite","text":"<ul> <li>Go to Azure Storage Accounts  Select your storage account name</li> <li>On Access control (IAM)  Click Add    Assign Storage Blob Data Contributor to your user-assigned managed identity</li> </ul>"},{"location":"services/azure/batch/az-ba-to-az/#connection-code_2","title":"Connection Code","text":"<p>Before develop code, you should install Azure Datalake Storage client library;</p> <pre><code>pip install azure-storage-file-datalake cffi\n</code></pre> <pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.storage.filedatalake import DataLakeServiceClient\n\n\ndef lake_client(storage_account_name) -&gt; DataLakeServiceClient:\n    \"\"\"Generate ADLS Client from input credential\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return DataLakeServiceClient(\n        account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n        credential=msi_credential\n    )\n</code></pre> More Codes <pre><code>import io\nimport pathlib\n\nimport pyarrow.parquet as pq\nimport pandas as pd\nfrom azure.core.exceptions import ResourceNotFoundError\nfrom azure.identity import ManagedIdentityCredential\nfrom azure.storage.filedatalake import (\n    DataLakeServiceClient,\n    DataLakeFileClient,\n)\n\n\ndef lake_client(storage_account_name) -&gt; DataLakeServiceClient:\n    \"\"\"Generate ADLS Client from input credential\"\"\"\n    msi_credential = ManagedIdentityCredential()\n    return DataLakeServiceClient(\n        account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n        credential=msi_credential\n    )\n\n\ndef exists(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; bool:\n    \"\"\"Return True if filepath on target container exists.\"\"\"\n    try:\n        (\n            client\n                .get_file_system_client(file_system=container)\n                .get_file_client(filepath)\n                .get_file_properties()\n        )\n        return True\n    except ResourceNotFoundError:\n        return False\n\ndef download(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; pathlib.Path:\n    file_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_file_client(filepath)\n    )\n    output_file = pathlib.Path(filepath)\n    output_file.parent.mkdir(exist_ok=True, parents=True)\n    with output_file.open(mode='wb') as local_file:\n        file_client.download_file().readinto(local_file)\n    return output_file\n\n\ndef upload(\n    client: DataLakeServiceClient,\n    container: str,\n    dirpath: str,\n    file: str,\n    df: pd.DataFrame,\n) -&gt; DataLakeFileClient:\n    dir_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_directory_client(directory=dirpath)\n    )\n    file_client = dir_client.create_file(file)\n\n    # If Upload Parquet file\n    io_file = df.to_parquet()\n\n    # Or, file_client.append_data(data=df, offset=0, length=len(df))\n    file_client.upload_data(data=io_file, overwrite=True)\n    file_client.flush_data(len(io_file))\n    return file_client\n\n\ndef to_pyarrow(\n    client: DataLakeServiceClient,\n    container: str,\n    filepath: str,\n) -&gt; pq.Table:\n    file_client: DataLakeFileClient = (\n        client\n            .get_file_system_client(file_system=container)\n            .get_file_client(filepath)\n    )\n    data = file_client.download_file(0)\n    with io.BytesIO() as b:\n        data.readinto(b)\n        table = pq.read_table(b)\n    return table\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/","title":"Connection BigQuery","text":""},{"location":"services/azure/batch/az-ba-to-google/#authentication","title":"Authentication","text":""},{"location":"services/azure/batch/az-ba-to-google/#using-service-account","title":"Using Service Account","text":""},{"location":"services/azure/batch/az-ba-to-google/#1-create-service-account","title":"1) Create Service Account","text":"<ul> <li>In the IAM &amp; Admin  Go to Service Accounts  Click CREATE SERVICE ACCOUNT</li> <li>On Permission  Assign BigQuery Job User and   BigQuery Data Editor to this service account</li> </ul>"},{"location":"services/azure/batch/az-ba-to-google/#2-keep-credential-to-key-vault","title":"2) Keep Credential to Key Vault","text":"<ul> <li> <p>When a service account was created, it will generate Json credential file that     has detail like:</p> <pre><code>{\n  \"type\": \"service_account\",\n  \"project_id\": \"&lt;project-id&gt;\",\n  \"private_key_id\": \"&lt;private-key-id&gt;\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n???\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"&lt;service-name&gt;@&lt;project-id&gt;.iam.gserviceaccount.com\",\n  \"client_id\": \"&lt;client-id&gt;\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/&lt;service-name&gt;%40&lt;project-id&gt;.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-to-google/#3-connection-code","title":"3) Connection Code","text":"<p>On Bath Pool, it should install Python package:</p> <pre><code>$ pip install google-auth\n</code></pre> <pre><code>import json\nfrom google.oauth2.service_account import Credentials\n\njson_info = json.loads(secret_client.get_secret(\"GOOGLE-JSON-STR\").value)\ncredentials = Credentials.from_service_account_info(json_info),\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/#using-oauth-token","title":"Using OAuth Token","text":"<p>User credentials are typically obtained via OAuth2.0</p> <pre><code>from google.oauth2.credentials import Credentials\n\ncredentials = Credentials(\n    '&lt;access-token&gt;',\n    # NOTE: If you obtain a refresh token\n    refresh_token='&lt;refresh_token&gt;',\n    token_uri='&lt;token_uri&gt;',\n    client_id='&lt;client_id&gt;',\n    client_secret='&lt;client_secret&gt;',\n)\n</code></pre>"},{"location":"services/azure/batch/az-ba-to-google/#bigquery","title":"BigQuery","text":"<p>On Bath Pool, it should install Python package:</p> <pre><code>$ pip install google-cloud-bigquery pandas_gbq\n</code></pre> <p>Note</p> <p>I recommend <code>padas_gbq</code> because it does not implement complex code.</p> <p>Before this connection code, you should implement connection for Azure Key Vault first for getting above secret json.</p> <pre><code>import json\nimport pandas as pd\nimport pandas_gbq as pg\nfrom google.oauth2.service_account import Credentials\n\njson_info = json.loads(secret_client.get_secret(\"GOOGLE-JSON-STR\").value)\npg.to_gbq(\n    pd.read_parquet(\"/dummy-file.parquet\"),\n    destination_table=\"&lt;dataset&gt;.&lt;table-name&gt;\",\n    if_exists='replace',\n    project_id=\"&lt;project-id&gt;\",\n    credentials=Credentials.from_service_account_info(json_info),\n)\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/","title":"Dockerize inside Node","text":"<p>This feature is running a Docker image before start provision the Azure Batch Node.</p>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-pool-with-dockerize-start-task","title":"Create Pool with Dockerize start task","text":"<p>Create Ubuntu pool and set start task command line:</p> <pre><code>/bin/bash -c\n\n\"sudo apt-get update &amp;&amp;\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common &amp;&amp;\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - &amp;&amp;\nsudo apt-key fingerprint 0EBFCD88 &amp;&amp;\nsudo add-apt-repository \\\"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\\\" &amp;&amp;\nsudo apt-get update &amp;&amp;\nsudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-compose-plugin &amp;&amp;\nsudo usermod -aG docker $USER &amp;&amp;\nsudo systemctl restart docker &amp;&amp;\nsudo apt-get install dos2unix\n\"\n</code></pre> <p>Info</p> <p>We add <code>$USER</code> to docker group because we want to execute <code>docker</code> command without <code>sudo</code>.</p>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-docker-image-file","title":"Create Docker image file","text":"Dockerfile<pre><code>FROM ubuntu:16.04\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y cmake build-essential gcc g++ git wget libgl1-mesa-glx\n\nRUN echo \"ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true\" | debconf-set-selections\nRUN apt-get install -y --no-install-recommends msttcorefonts\n\nRUN wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\n    /bin/bash Miniconda3-latest-Linux-x86_64.sh -f -b -p /opt/conda &amp;&amp; \\\n    export PATH=\"/opt/conda/bin:$PATH\"\n\nENV PATH /opt/conda/bin:$PATH\n\nRUN conda install -y numpy scipy scikit-learn pandas matplotlib\n\nRUN pip install azure azure-storage\n\nRUN apt-get autoremove -y &amp;&amp; apt-get clean &amp;&amp; \\\n    conda clean -i -l -t -y &amp;&amp; \\\n    rm -rf /usr/local/src/*\n\nCOPY . .\n\nENV AZURE_BLOB_KEY=\"[AZURE_BLOB_KEY]\"\n\nENTRYPOINT [ \"python\", \"train.py\" ]\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#create-runner-script","title":"Create runner script","text":"runner.sh<pre><code>#!/bin/bash\n\necho \"Script Name: $0 with process id: $$\";\necho \"Start run docker with image: $1\";\necho \"Receive environment file name: $2\";\n\necho ${AZ_BATCH_CERTIFICATES_DIR};\nmkdir certs/;\ncp ${AZ_BATCH_CERTIFICATES_DIR}/* certs/;\nls certs/;\n\nACR_PWD=$(python3 runner.py 2&gt;&amp;1 &gt;/dev/null);\n\n# Build Docker Container\ndocker --version;\ndocker build -t $1:latest . --no-cache;\ndocker login dataplatdev.azurecr.io -u dataplatdev -p $ACR_PWD;\ndocker pull dataplatdev.azurecr.io/poc/python-test:0.0.8 &gt;/dev/null 2&gt;&amp;1;\ndocker images;\necho\necho \"Delete Old images ...\";\ndocker rmi -f $(docker image ls -f \"dangling=true\" -q);\ndocker images;\necho\n# Run Docker Container\nOLD=$(docker ps --all --quiet --filter=name=\"$1\");\nif [[ -n \"$OLD\" ]]; then docker stop $OLD &amp;&amp; docker rm $OLD; fi;\n# docker run --name $1 --env-file \"./$2\" -v \"$(pwd)\\output:/output\" $1:latest;\ndocker run --name $1 -v \"$(pwd)\\output:/output\" $1:latest;\ndocker ps -a;\nCONTAINER_RC=$(docker inspect --format '{{.State.ExitCode}}' $1);\nexit $CONTAINER_RC;\n</code></pre>"},{"location":"services/azure/batch/az-ba-with-docker-inside/#references","title":"References","text":"<ul> <li>Container ML on Azure Batch</li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/","title":"Dockerize","text":"<p>Azure Batch can be a great tool for instant batch processing as it creates and manages a pool of compute nodes (virtual machines), installs the applications you want to run, and schedules jobs to run on the nodes.</p>"},{"location":"services/azure/batch/az-ba-with-docker/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/batch/az-ba-with-docker/#azure-container-registry","title":"Azure Container Registry","text":"<ul> <li>Go to Container Registries  Create container registry with prefix name <code>cr</code></li> <li>Add the information of this registry, for example with name: <code>cr-ba-python-dev</code>  Click Create for registry creation</li> <li>After registry creation, Go to <code>cr-ba-python-dev</code> registry    On Access Keys</li> <li>Click Enable on Admin user option</li> <li>Save these values, Login server, Username, and Password</li> <li>Go to your local terminal for prepare docker file</li> <li> <p>Create your <code>Dockerfile</code> file</p> Dockerfile<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY main.py ./\n\nRUN mkdir -p ./output\n\nCMD [\"python\", \"./main.py\"]\n</code></pre> <p>The main Python file that run on this Docker</p> ./main.py<pre><code>print(\"This is a Main file for testing Dockerize.\")\nwith open('/output/docker_test.txt', 'w') as f:\n    f.write(\"Write Output file on Dockerize.\")\n</code></pre> </li> <li> <p>Test this Docker image able to run on the Local</p> <pre><code>docker build -t \"python-ba\" . --no-cache\ndocker run --name python-btch -v \"${pwd}\\output:/output\" python-btch\n</code></pre> </li> <li> <p>Push your Docker image to Azure Container Registries</p> <pre><code>docker login cr-ba-python-dev.azurecr.io\ndocker tag python-btch:latest cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test\ndocker push cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test\n</code></pre> </li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/#azure-batch-accounts","title":"Azure Batch Accounts","text":"<ul> <li>Go to your Azure Batch Accounts  Click Pools  Add new pool that Supports Container</li> <li>Click Enable to Custom on Container configuration option</li> <li>Go to Container registries  Add <code>cr-ba-python-dev</code> registry from ACR values</li> <li>Create Pool with name is <code>btch-pool-cntn</code></li> <li>Go to Jobs  Create new job in <code>btch-pool-cntn</code> pool with name <code>btch-job-cntn</code></li> <li> <p>Go to Tasks  Create new task in this job</p> <ul> <li>Go to Image name  Add <code>cr-ba-python-dev.azurecr.io/btch/python-btch:0.0.1-test</code></li> <li> <p>Go to Container run options and add below command</p> <pre><code>--rm --workdir /app\n</code></pre> </li> </ul> </li> <li> <p>Create the Automate Script file</p> <p>Package image version from local to Azure Container Registries</p> <pre><code>@echo off\nset \"version=%~1\"\nif defined version (\n    echo Start package docker image version: %version% ...\n    call docker build -t python-test:latest . --no-cache\n    call docker tag python-test:latest cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n    call docker push cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n    call docker rmi cr-ba-python-dev.azurecr.io/poc/python-test:%version%\n\n    for /f \"tokens=1-3\" %%c IN ('docker image ls ^| Findstr /r \"^cr-ba-python-dev.azurecr.io* ^&lt;none&gt;\"')\n    do (\n        echo Start remove image: `%%c:%%d` with ID: %%e\n        if \"%%d\" equ \"&lt;none&gt;\" (\n            echo Delete image with id ...\n            call docker rmi %%e &gt; nul 2&gt;&amp;1\n        ) else (\n            echo Delete image with name:tag ...\n            call docker rmi \"%%c:%%d\" &gt; nul 2&gt;&amp;1\n        )\n    )\n)\n</code></pre> <p>Additional, run this Task with JSON</p> <pre><code>{\n  \"id\": \"container-job-10\",\n  \"commandLine\": \"\",\n  \"containerSettings\": {\n      \"containerRunOptions\": \"--rm --workdir /app\",\n      \"imageName\": \"cr-ba-python-dev.azurecr.io/poc/python-test:0.0.8\",\n      \"workingDirectory\": \"taskWorkingDirectory\"\n  },\n  \"userIdentity\": {\n      \"autoUser\": {\n          \"scope\": \"pool\",\n          \"elevationLevel\": \"admin\"\n      }\n  }\n}\n</code></pre> </li> </ul> <p>Note</p> <p>About mounting volumn to Azure Batch Node,</p> <p><code>--mount type=bind,source=/datadisks/disk1,target=/data</code></p> <p><code>-v {&lt;volume_id&gt;}:{&lt;path&gt;}</code></p> <p>Warning</p> <p>Azure Data Factory does not support for run Azure Batch with a Docker container in the Custom Activity currently, Read More</p>"},{"location":"services/azure/batch/az-ba-with-docker/#run-with-mount-volume","title":"Run with Mount Volume","text":"<ul> <li> Azure Samples: Compute Automation Configurations - Prepare VM disks</li> <li> Can I use Docker volumes in Container based Azure Batch Pool</li> </ul>"},{"location":"services/azure/batch/az-ba-with-docker/#read-mores","title":"Read Mores","text":"<ul> <li> Use Container for Azure Batch Service</li> </ul>"},{"location":"services/azure/data_factory/","title":"Azure Data Factory","text":""},{"location":"services/azure/data_factory/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/data_factory/#syntax","title":"Syntax","text":""},{"location":"services/azure/data_factory/#coalesce","title":"Coalesce","text":"ExistsNot Exists P_EXTRACT<pre><code>{\n    \"parent\": {\n      \"child\": {\n        \"key\": \"value\"\n      }\n    }\n}\n</code></pre> <pre><code>@coalesce(parameters.P_EXTRACT.parent.child?.key, 'default')\n</code></pre> <p>Output:</p> <pre><code>value\n</code></pre> P_EXTRACT<pre><code>{\n    \"parent\": {\n      \"child\": {\n        \"foo\": \"bar\"\n      }\n    }\n}\n</code></pre> <pre><code>@coalesce(parameters.P_EXTRACT.parent.child?.key, 'default')\n</code></pre> <p>Output:</p> <pre><code>default\n</code></pre>"},{"location":"services/azure/data_factory/#read-mores","title":"Read Mores","text":""},{"location":"services/azure/data_factory/adf-ir-sharing/","title":"IR Sharing","text":""},{"location":"services/azure/data_factory/adf-ir-sharing/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft: Data Factory - Create Shared Self Hosted IR</li> </ul>"},{"location":"services/azure/data_factory/adf-link-services/","title":"Link Services","text":""},{"location":"services/azure/data_factory/adf-link-services/#azure-services","title":"Azure Services","text":"KeyVaultFunctionBatchDatabricksDataLakeBlob StorageSQLDatabaseSynapse <pre><code>{\n    \"name\": \"KeyVault\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"description\": \"All of Key Vaults\",\n        \"annotations\": [],\n        \"type\": \"AzureKeyVault\",\n        \"typeProperties\": {\n            \"baseUrl\": \"https://&lt;key-vault-name&gt;.vault.azure.net/\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureFunction\",\n    \"properties\": {\n        \"parameters\": {\n            \"FuctionAppUrl\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureFunction\",\n        \"typeProperties\": {\n            \"functionAppUrl\": \"@{linkedService().FuctionAppUrl}\",\n            \"functionKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"authentication\": \"Anonymous\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureBatch\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"AzureBatch\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"BatchURI\": {\n                \"type\": \"String\"\n            },\n            \"Pool\": {\n                \"type\": \"String\"\n            },\n            \"Account\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"batchUri\": \"@linkedService().BatchURI\",\n            \"poolName\": \"@linkedService().Pool\",\n            \"accountName\": \"@linkedService().Account\",\n            \"linkedServiceName\": {\n                \"referenceName\": \"&lt;azure-blob-storage-link-service-name&gt;\",\n                \"type\": \"LinkedServiceReference\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAzureDatabrick\",\n    \"properties\": {\n        \"description\": \"Databrick Connection used for common dynamic\",\n        \"parameters\": {\n            \"ClusterURL\": {\n                \"type\": \"String\"\n            },\n            \"ClusterID\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureDatabricks\",\n        \"typeProperties\": {\n            \"domain\": \"@concat('https://', linkedService().ClusterURL, '/')\",\n            \"accessToken\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            },\n            \"existingClusterId\": \"@linkedService().ClusterID\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicDataLake\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"description\": \"Azure Data Lake Gen 2\",\n        \"annotations\": [],\n        \"type\": \"AzureBlobFS\",\n        \"typeProperties\": {\n            \"url\": \"https://&lt;storage-account-name&gt;.dfs.core.windows.net\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicBlobStorage\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"annotations\": [],\n        \"type\": \"AzureBlobStorage\",\n        \"typeProperties\": {\n            \"connectionString\": \"DefaultEndpointsProtocol=https;AccountName=&lt;storage-account-name&gt;;EndpointSuffix=core.windows.net;\",\n            \"accountKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"&lt;secret-name&gt;\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"AzureSqlAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"AzureSqlDatabase\",\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"connectionString\": \"Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSynapse\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureSqlDW\",\n        \"typeProperties\": {\n            \"connectionString\": \"Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@{linkedService().Secret}\",\n                    \"type\": \"Expression\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#storage-system","title":"Storage System","text":"MySQLSQLServerPostgresMongoDBBigQuery <pre><code>{\n    \"name\": \"MySqlAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"MySql\",\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"connectionString\": \"Server=@{linkedService().Server};Port=3306;Database=@{linkedService().Database};User=@{linkedService().Username};SSLMode=1;UseSystemTrustStore=0\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSqlServer\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"SqlServer\",\n        \"typeProperties\": {\n            \"connectionString\": \"Data Source=@{linkedService().Server};Initial Catalog=@{linkedService().Database};Integrated Security=False;User ID=@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@{linkedService().Secret}\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicPostgres\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"PostgreSql\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"String\"\n            },\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Username\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"connectionString\": \"Server=@{linkedService().Server};Database=@{linkedService().Database};Port=5432;UID=@{linkedService().Username};\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@{linkedService().Secret}\"\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"MongoDBAuto\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"MongoDbV2\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Database\": {\n                \"type\": \"String\"\n            },\n            \"Secret\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"connectionString\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            },\n            \"database\": \"@linkedService().Database\"\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <p>The Connection String that keep in Azure Key Vaults:</p> <pre><code>mongodb://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:27017\n</code></pre> <pre><code>{\n    \"name\": \"DynamicBigQuery\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"GoogleBigQuery\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"Project\": {\n                \"type\": \"String\"\n            },\n            \"ClientID\": {\n                \"type\": \"String\"\n            },\n            \"ClientSecret\": {\n                \"type\": \"String\"\n            },\n            \"RefreshToken\": {\n                \"type\": \"String\"\n            }\n        },\n        \"typeProperties\": {\n            \"project\": \"@replace(linkedService().Project, '''', '')\",\n            \"requestGoogleDriveScope\": false,\n            \"authenticationType\": \"UserAuthentication\",\n            \"clientId\": \"@replace(linkedService().ClientID, '''', '')\",\n            \"clientSecret\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"KVdataplat\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@replace(linkedService().ClientSecret, '''', '')\"\n            },\n            \"refreshToken\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@replace(linkedService().RefreshToken, '''', '')\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#file-system","title":"File System","text":"File SystemSFTPGCSS3 <pre><code>{\n    \"name\": \"DynamicFileSystem\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"string\",\n                \"defaultValue\": \"\\\\\\\\path\\\\sub-path\\\\folder\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"FileServer\",\n        \"typeProperties\": {\n            \"host\": \"@{linkedService().Server}\",\n            \"userId\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicSFTP\",\n    \"properties\": {\n        \"parameters\": {\n            \"Server\": {\n                \"type\": \"string\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"Sftp\",\n        \"typeProperties\": {\n            \"host\": \"@{linkedService().Server}\",\n            \"port\": 22,\n            \"skipHostKeyValidation\": true,\n            \"authenticationType\": \"Basic\",\n            \"userName\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().Secret\",\n                    \"type\": \"Expression\"\n                }\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicGCS\",\n    \"properties\": {\n        \"type\": \"GoogleCloudStorage\",\n        \"annotations\": [],\n        \"parameters\": {\n            \"ServiceURL\": {\n                \"type\": \"string\"\n            },\n            \"AccessKey\": {\n                \"type\": \"string\"\n            },\n            \"AccessSecret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"typeProperties\": {\n            \"serviceUrl\": \"@linkedService().ServiceURL\",\n            \"accessKeyId\": \"@linkedService().AccessKey\",\n            \"secretAccessKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().AccessSecret\"\n            }\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicAmazonS3\",\n    \"properties\": {\n        \"type\": \"AmazonS3\",\n        \"parameters\": {\n            \"AccessSecret\": {\n                \"type\": \"string\"\n            },\n            \"AccessKey\": {\n                \"type\": \"string\"\n            },\n            \"S3URL\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"serviceUrl\": \"@{linkedService().S3URL}\",\n            \"accessKeyId\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().AccessKey\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"secretAccessKey\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"KVdataplat\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": {\n                    \"value\": \"@linkedService().AccessSecret\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"authenticationType\": \"AccessKey\"\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    },\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\"\n}\n</code></pre>"},{"location":"services/azure/data_factory/adf-link-services/#http","title":"HTTP","text":"AnonymousWindows <pre><code>{\n    \"name\": \"DynamicHTTPAnonymous\",\n    \"type\": \"Microsoft.DataFactory/factories/linkedservices\",\n    \"properties\": {\n        \"type\": \"HttpServer\",\n        \"parameters\": {\n            \"BaseURL\": {\n                \"type\": \"string\",\n                \"defaultValue\": \"https://dev.domain.com/api/\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"url\": \"@{linkedService().BaseURL}\",\n            \"enableServerCertificateValidation\": true,\n            \"authenticationType\": \"Anonymous\"\n        }\n    }\n}\n</code></pre> <pre><code>{\n    \"name\": \"DynamicHTTPWindows\",\n    \"properties\": {\n        \"type\": \"HttpServer\",\n        \"parameters\": {\n            \"BaseURL\": {\n                \"type\": \"string\"\n            },\n            \"Username\": {\n                \"type\": \"string\"\n            },\n            \"Secret\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"typeProperties\": {\n            \"url\": \"@{linkedService().BaseURL}\",\n            \"enableServerCertificateValidation\": false,\n            \"authenticationType\": \"Windows\",\n            \"userName\": \"@{linkedService().Username}\",\n            \"password\": {\n                \"type\": \"AzureKeyVaultSecret\",\n                \"store\": {\n                    \"referenceName\": \"&lt;key-vault-link-service-name&gt;\",\n                    \"type\": \"LinkedServiceReference\"\n                },\n                \"secretName\": \"@linkedService().Secret\"\n            }\n        },\n        \"connectVia\": {\n            \"referenceName\": \"&lt;integration-runtime-name&gt;\",\n            \"type\": \"IntegrationRuntimeReference\"\n        }\n    }\n}\n</code></pre> <p>Note</p> <p>The Windows username should has domain name before username:</p> <pre><code>domain\\\\username\n</code></pre> <p>Note</p> <p>If you want to use <code>AutoResolveIntegrationRuntime</code>, you can delete key <code>connectVia</code> from above json structure.</p>"},{"location":"services/azure/databricks/","title":"Azure Databricks","text":""},{"location":"services/azure/databricks/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/databricks/#connection-code","title":"Connection Code","text":""},{"location":"services/azure/databricks/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft - Azure Databricks Getting Started</li> </ul>"},{"location":"services/azure/databricks/adb-init-script/","title":"Init Script","text":"<p>Init Script (Initialization Script) is a shell script that runs during startup of each cluster node before the Apache Spark driver or executor JVM starts.</p> <p>Danger</p> <p>Legacy Global and Legacy Cluster-Named init scripts run before other init scripts. These init scripts might be present in workspaces created before February 21, 2023.</p>"},{"location":"services/azure/databricks/adb-init-script/#prerequisite","title":"Prerequisite","text":"<p>We want to initialize some program before a cluster started like:</p> init_script.sh<pre><code>#!/bin/bash\n\necho \"Start running init script: adb-default\"\necho \"Running on the driver? $DB_IS_DRIVER\"\necho \"Driver IP: $DB_DRIVER_IP\"\n\ntimedatectl set-timezone Asia/Bangkok\n</code></pre>"},{"location":"services/azure/databricks/adb-init-script/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/databricks/adb-init-script/#cluster-scoped-init-scripts","title":"Cluster-Scoped init scripts","text":"<p>To use the UI to configure a cluster to run an init script, complete the following steps:</p> <ul> <li>On the Cluster Configuration Page  Click the Advanced Options toggle</li> <li>At the bottom of the page  click the Init Scripts tab</li> <li>In the Destination drop-down  Select the Workspace type</li> <li>Specify a path to the init script like <code>SYS/init_script.sh</code>    Click Add.</li> </ul> <p>Note</p> <p>Each user has a Home directory configured under the <code>/Users</code> directory in the workspace. If a user with the name <code>user@databricks.com</code> stored an init script called <code>my-init.sh</code> in their home directory, the configured path would be <code>/Users/user@databricks.com/my-init.sh</code>.</p>"},{"location":"services/azure/databricks/adb-init-script/#cluster-scoped-with-shared-cluster","title":"Cluster-Scoped with Shared Cluster","text":"<p>For shared access mode, you must add init scripts to the <code>allowlist</code>. See Allowlist libraries and init scripts on shared compute.</p> <ul> <li>In your Azure Databricks Workspace  Click Catalog</li> <li>Click Gear Icon to open the metastore details and permissions UI    Select Allowed JARs/Init Scripts  Click Add</li> </ul> <p>Warning</p> <p>Init scripts use the identity of the cluster owner.</p>"},{"location":"services/azure/databricks/adb-init-script/#global-init-scripts","title":"Global init scripts","text":"<ul> <li>Go to the Admin Settings  Click <code>Global Init Scripts</code></li> <li>Click Add  Name the script and enter it by typing,   pasting, or dragging a text file into the Script field.</li> </ul>"},{"location":"services/azure/databricks/adb-init-script/#read-mores","title":"Read Mores","text":"<ul> <li> Databricks - Init Scripts</li> <li> Global Init Script in Azure databricks</li> </ul>"},{"location":"services/azure/databricks/adb-mount-storage/","title":"Mount Storage","text":""},{"location":"services/azure/databricks/adb-mount-storage/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/databricks/adb-mount-storage/#list-of-mounts","title":"List of Mounts","text":"<pre><code>display(dbutils.fs.mounts())\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#mounting","title":"Mounting","text":""},{"location":"services/azure/databricks/adb-mount-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>adls_account: str = \"&lt;storage-account-name&gt;\"\nadls_container: str = \"&lt;container-name&gt;\"\nmount_point: str = \"/mnt/&lt;mount-path&gt;\"\n\naccess_key = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adls-account-key\")\n\nif not any(\n    mount.mountPoint == mount_point\n    for mount in dbutils.fs.mounts()\n):\n    dbutils.fs.mount(\n        source=f\"wasbs://{adls_container}@{adls_account}.blob.core.windows.net\",\n        mount_point=mount_point,\n        extra_configs={\n            \"fs.azure.account.key.&lt;storage-account-name&gt;.blob.core.windows.net\",\n            access_key,\n        },\n    )\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#azure-datalake-storage","title":"Azure DataLake Storage","text":"<pre><code>adls_account: str = \"&lt;storage-account-name&gt;\"\nadls_container: str = \"&lt;container-name&gt;\"\nadls_dir: str = \"&lt;dir-path&gt;\"\nmount_point: str = \"/mnt/&lt;mount-path&gt;\"\n\nclient_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-client-id\")\nclient_secret_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-client-secrete-id\")\ntenant_id = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"adb-tenant-id\")\n\nendpoint: str = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n\nif adls_dir:\n    source: str = f\"abfss://{adls_container}@{adls_account}.dfs.core.windows.net/{adls_dir}\"\nelse:\n    source: str = f\"abfss://{adls_container}@{adls_account}.dfs.core.windows.net\"\n\n# Connecting using Service Principal secrets and OAuth\nconfigs: Dict[str, str] = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": client_id,\n    \"fs.azure.account.oauth2.client.secret\": client_secret_id,\n    \"fs.azure.account.oauth2.client.endpoint\": endpoint\n}\n\n# Mount ADLS Storage to DBFS only if the directory is not already mounted\nif not any(\n    mount.mountPoint == mount_point\n    for mount in dbutils.fs.mounts()\n):\n    dbutils.fs.mount(\n        source=source,\n        mount_point=mount_point,\n        extra_configs=configs\n    )\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#unmount","title":"Unmount","text":"<pre><code>mount_point: str = \"/mnt/&lt;mount-path&gt;\"\n\nif any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n    dbutils.fs.unmount(mount_point)\n</code></pre>"},{"location":"services/azure/databricks/adb-mount-storage/#read-mores","title":"Read Mores","text":"<ul> <li>Mount ADLS Gen2 to Databricks File System using Service Principle</li> <li>Storage - Azure Storage</li> </ul>"},{"location":"services/azure/databricks/adb-secrets/","title":"Secrets","text":"<p>Azure Databricks allow you to map Azure Key Vault with secrete module.</p>"},{"location":"services/azure/databricks/adb-secrets/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/databricks/adb-secrets/#prerequisite","title":"Prerequisite","text":"<ul> <li>Go to <code>https://&lt;databricks-instance&gt;.azuredatabricks.net#secrets/createScope</code></li> <li>Create Scope with <code>All workspace users</code> manage principle</li> </ul> Azure Databricks Secrets Scope"},{"location":"services/azure/databricks/adb-secrets/#list-of-secrets","title":"List of Secrets","text":"<p>List of all secrets scopes:</p> <pre><code>display(dbutils.secrets.listScopes())\n</code></pre> <p>List of secrets with a scope name:</p> <pre><code>display(dbutils.secrets.list(scope='&lt;scope-name&gt;'))\n</code></pre>"},{"location":"services/azure/databricks/adb-secrets/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft - Secret scopes</li> </ul>"},{"location":"services/azure/databricks/adb-to-aws/","title":"Connect to AWS Services","text":""},{"location":"services/azure/databricks/adb-to-aws/#authentication","title":"Authentication","text":""},{"location":"services/azure/databricks/adb-to-aws/#using-aws-access-token","title":"Using AWS Access Token","text":"<ul> <li>Go to IAM</li> </ul>"},{"location":"services/azure/databricks/adb-to-aws/#kinesis","title":"Kinesis","text":"<p>Warning</p> <p>In Databricks Runtime 11.3 LTS and Above, the <code>Trigger.Once</code> setting is deprecated. Databricks recommends you use <code>Trigger.AvailableNow</code> for all incremental batch processing workloads.<sup>1</sup></p>"},{"location":"services/azure/databricks/adb-to-aws/#1-iam-policy","title":"1) IAM Policy","text":"<p>By default, the Kinesis connector resorts to  Amazon\u2019s default credential provider chain</p> ReadPut <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:Get*\",\n                \"kinesis:DescribeStreamSummary\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kinesis:us-east-1:111122223333:stream/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:ListStreams\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"kinesis:PutRecord\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kinesis:us-east-1:111122223333:stream/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"services/azure/databricks/adb-to-aws/#2-connection-code","title":"2) Connection Code","text":"ReadStreamWriteStream <pre><code>df = (\n    spark.readStream\n        .format(\"kinesis\")\n        .option(\"streamName\", \"&lt;aws-kinesis-stream-name&gt;\")\n        .option(\"initialPosition\", \"latest\")\n        .option(\"format\", \"json\")\n        .option(\"awsAccessKey\", \"&lt;aws-access-key&gt;\")\n        .option(\"awsSecretKey\", \"&lt;aws-access-secret-key\")\n        .option(\"region\", \"&lt;aws-region&gt;\")\n        .option(\"inferSchema\", \"true\")\n        .load()\n)\n</code></pre> <p>Note</p> <p>initialPosition:</p> <ul> <li><code>latest</code>: Read from the latest position that data ingest.</li> <li><code>trim_horizon</code> or <code>earliest</code>:  Read all data that keep in shard.</li> <li><code>at_timestamp</code>: Specify time value such as   <code>{\"at_timestamp\": \"06/25/2020 10:23:45 PDT\", \"format\": \"MM/dd/yyyy HH:mm:ss ZZZ\"}</code></li> </ul> <p>Read more about <code>StartingPosition</code></p> <pre><code>(\n    df\n        .writeStream\n        .format(\"kinesis\")\n        .outputMode(\"update\")\n        .option(\"streamName\", \"&lt;aws-kinesis-stream-name&gt;\")\n        .option(\"region\", \"&lt;aws-region&gt;\")\n        .option(\"awsAccessKeyId\", \"&lt;aws-access-key&gt;\")\n        .option(\"awsSecretKey\", \"aws-access-secret-key\")\n        .option(\"checkpointLocation\", \"/path/to/checkpoint\")\n        .start()\n        .awaitTermination()\n)\n</code></pre> <p>Note</p> <p>Kinesis returns records with the following schema:</p> <pre><code>from pyspark.sql.types import TimestampType, StringType, StructType, StructField, BinaryType\n\nschema: StructType = StructType(\n    [\n        StructField(\"partitionKey\", StringType(), True),\n        StructField(\"data\", BinaryType(), False),\n        StructField(\"stream\", StringType(), False),\n        StructField(\"shardId\", StringType(), False),\n        StructField(\"sequenceNumber\", StringType(), False),\n        StructField(\"approximateArrivalTimestamp\", TimestampType(), False),\n    ],\n)\n</code></pre> <p>References:</p> <ul> <li> Apache Spark\u2019s Structured Streaming with Amazon Kinesis on Databricks</li> <li> Connect to Amazon Kinesis</li> <li> Amazon Kinesis Data Streams Connector for Spark Structured Streaming</li> </ul> <ol> <li> <p> Configure Structured Streaming trigger intervals \u21a9</p> </li> </ol>"},{"location":"services/azure/databricks/adb-to-az/","title":"Connect to Azure Services","text":""},{"location":"services/azure/databricks/adb-to-az/#authentication","title":"Authentication","text":""},{"location":"services/azure/databricks/adb-to-az/#using-service-principal","title":"Using Service Principal","text":""},{"location":"services/azure/databricks/adb-to-az/#1-create-service-principal","title":"1) Create Service Principal","text":"<p>To register your application and create your service principal:</p> <ul> <li>Go to Azure Active Directory  Click App registrations  Start get New registration</li> <li>Add the information of this app like <code>name</code> is <code>cnct-adb-dev</code> (The name of app   should be formatted like <code>{app}-{resource-shortname}-{environment}</code>)</li> <li>Click register for create</li> </ul> <p>You will then be required to generate a secret:</p> <ul> <li>Go to <code>App registrations</code> <code>Certificates&amp;secrets</code> <code>New Client Secret</code></li> <li>Save this value to <code>Azure Key Vaults</code></li> </ul> <p>Note</p> <p>We write both the <code>Client ID</code> and <code>Secret</code> to Key Vault for a number of reasons:</p> <ul> <li>The <code>Secret</code> is sensitive and like a <code>Storage Key</code> or <code>Password</code>, we don't want    this to be hardcoded or exposed anywhere in our application.</li> <li>Normally we would have an instance of <code>Databricks</code> and <code>Key Vault</code> per environment    and when we come to referencing the secrets, we want the secrets names to remain    the same, so the code in our Databricks notebooks referencing the <code>Secrets</code> doesn't    need to be modified when we deploy to different environments.</li> </ul> <p>Abstract</p> <p>The App Registration is the template used to create the security principal (like a User) which can be authenticated and authorized.</p>"},{"location":"services/azure/databricks/adb-to-az/#sql-database","title":"SQL Database","text":""},{"location":"services/azure/databricks/adb-to-az/#1-create-external-user","title":"1) Create External User","text":"<p>The app registration still needs permission to log into <code>Azure SQL</code> and access the objects within it. You\u2019ll need to Create that user (App &amp; Service Principal) in the database and then grant it permissions on the underlying objects.</p> <pre><code>CREATE USER [cnct-adb-dev] FROM EXTERNAL PROVIDER;\n</code></pre> <p>Grant Permission:</p> Read OnlyOverwrite <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\n</code></pre> <pre><code>GRANT SELECT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT INSERT ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT ALTER ON SCHEMA::dbo TO [cnct-adb-dev];\nGRANT CREATE TABLE TO [cnct-adb-dev];\n</code></pre> <p>Create SQL User</p> <pre><code>USE [master];\nCREATE LOGIN [cnct-adb-dev] WITH PASSWORD = 'P@ssW0rd'\nGO\n\nUSE [&lt;database-name&gt;];\nCREATE USER [cnct-adb-dev] FOR LOGIN [cnct-adb-dev];\nGO\n</code></pre>"},{"location":"services/azure/databricks/adb-to-az/#2-connection-code","title":"2) Connection Code","text":""},{"location":"services/azure/databricks/adb-to-az/#method-01-spark-connector","title":"Method 01: Spark Connector","text":"<p>To connect to <code>Azure SQL</code>, you will need to install the SQL Spark Connector and the Microsoft Azure Active Directory Authentication Library (ADAL) for Python code.</p> <ul> <li> <p>Go to your cluster in Databricks and Install necessary packages:</p> <ul> <li>Maven: <code>com.microsoft.azure:spark-mssql-connector_2.12_3.0:1.0.0-alpha</code></li> <li>PYPI: <code>adal</code></li> </ul> </li> <li> <p>Also, if you haven\u2019t already, Create a Secret Scope     to your <code>Azure Key Vault</code> where your <code>Client ID</code>, <code>Secret</code>, and <code>Tenant ID</code> have     been generated.</p> </li> <li> <p>Get <code>Access Token</code> from Service Principle authentication request</p> <pre><code>import adal\n\ncontext = adal.AuthenticationContext(\n    f\"https://login.windows.net/{dbutils.secrets.get(scope='defaultScope', key='TenantId')}\"\n)\ntoken = context.acquire_token_with_client_credentials(\n    \"https://database.windows.net/\",\n    dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"),\n    dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"),\n)\naccess_token = token[\"accessToken\"]\n</code></pre> </li> </ul> Read TableWrite Table <pre><code>df = (\n    spark.read\n        .format(\"com.microsoft.sqlserver.jdbc.spark\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;server-instance-name&gt;.database.windows.net\")\n        .option(\"databaseName\", \"{dev}\")\n        .option(\"accessToken\", access_token)\n        .option(\"encrypt\", \"true\")\n        .option(\"hostNameInCertificate\", \"*.database.windows.net\")\n        .option(\"dbtable\", \"[dbo].[&lt;table-name&gt;]\")\n        .option(\"batchsize\", 2500)\n        .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n        .load()\n)\n</code></pre> <p>Note</p> <p>This connector by default uses <code>READ_COMMITTED</code> isolation level when performing the bulk insert into the database. If you wish to override the isolation level, use the <code>mssqlIsolationLevel</code> option as show above.</p> <pre><code>(\n    df.write\n        .format(\"com.microsoft.sqlserver.jdbc.spark\")\n        .mode(\"append\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;server-instance-name&gt;.database.windows.net\")\n        .option(\"dbtable\", \"[dbo].[&lt;table-name&gt;]\")\n        .option(\"accessToken\", access_token)\n        .option(\"schemaCheckEnabled\", \"false\")\n        .save()\n)\n</code></pre> <p>Note</p> <p>When <code>schemaCheckEnabled</code> is <code>false</code>, we can write to the destination table which has less column than dataframe.</p> <p>Read More</p> <p>Note</p> <p>Executing custom SQL through the connector. The previous Azure SQL Connector for Spark provided the ability to execute custom SQL code like DML or DDL statements through the connector. This functionality is out-of-scope of this connector since it is based on the DataSource APIs. This functionality is readily provided by libraries like <code>pyodbc</code>, or you can use the standard java sql interfaces as well.</p>"},{"location":"services/azure/databricks/adb-to-az/#method-02-jdbc-connector","title":"Method 02: JDBC Connector","text":"<p>This method reads or writes the data row by row, resulting in performance issues. Not Recommended.</p> <p>In Databricks Runtime 11.3 LTS and above, you can use the sqlserver keyword to use the included driver for connecting to SQL server.</p> Format JDBCFormat SQLServer ReadRead (Legacy)Write AppendWrite Overwrite <pre><code>df = (\n    spark.read\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"encrypt\", \"true\")\n        .load()\n)\n</code></pre> <pre><code>df = (\n    spark.read\n        .jdbc(\n            url=\"jdbc:sqlserver://&lt;host&gt;:1433;database=&lt;database&gt;\",\n            table=\"&lt;schema-name&gt;.&lt;table-name&gt;\",\n            properties={\n                \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n                \"authentication\": \"ActiveDirectoryServicePrincipal\",\n                \"UserName\": dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"),\n                \"Password\": dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"),\n            },\n        )\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"append\")\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"overwrite\")\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host&gt;:1433;\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> ReadWrite AppendWrite Overwrtie <pre><code>df = (\n    spark.read\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host-name&gt;.database.windows.net\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"encrypt\", \"true\")\n        .option(\"hostNameInCertificate\", \"*.database.windows.net\")\n        .load()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"append\")\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host:***.database.windows.net&gt;\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .save()\n)\n</code></pre> <pre><code>(\n    df.write\n        .mode(\"overwrite\")\n        .format(\"sqlserver\")\n        .option(\"host\", \"&lt;host:***.database.windows.net&gt;\")\n        .option(\"port\", \"1433\")\n        .option(\"authentication\", \"ActiveDirectoryServicePrincipal\")\n        .option(\"user\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnId\"))\n        .option(\"password\", dbutils.secrets.get(scope=\"defaultScope\", key=\"DatabricksSpnSecret\"))\n        .option(\"database\", \"&lt;database-name&gt;\")\n        .option(\"dbtable\", \"&lt;schema-name&gt;.&lt;table-name&gt;\")\n        .option(\"truncate\", true)\n        save()\n)\n</code></pre> <p>When using mode <code>overwrite</code> if you do not use the option truncate on recreation of the table, indexes will be lost. , a columnstore table would now be a heap. If you want to maintain existing indexing please also specify option truncate with value true. For example, <code>.option(\"truncate\",\"true\")</code>.</p> <p>Microsoft: Spark - SQL Server Connector</p> <p>References:</p> <ul> <li>Databricks: External Data - SQL Server</li> <li>Apache Spark: SQL Data Sources - JDBC</li> </ul> <p>SQL User</p> <p>Method 01: ODBC Connector</p> <p>Install ODBC Driver on cluster:</p> <pre><code>%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n</code></pre> <pre><code>import pyodbc\n\nconn = pyodbc.connect(\n    f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n    f'SERVER=&lt;host&gt;;DATABASE=&lt;database_name&gt;;UID=[cnct-adb-dev];PWD=P@ssW0rd;'\n    f'Authentication=SqlPassword;Encrypt=yes;'\n)\n</code></pre> <p>Reference:</p> <ul> <li>StackOverFlow: Using PyODBC in Azure Databricks for Connect to SQL Server</li> <li>Microsoft: SQL ODBC - Using Azure AD</li> </ul> <p>Method 02: JDBC Connector</p> Format SQLServerFormat JDBC <pre><code>df = (\n    spark.read\n        .format(\"sqlserver\")\n        .option(\"host\", \"hostName\")\n        .option(\"port\", \"1433\")\n        .option(\"user\", \"[cnct-adb-dev]\")\n        .option(\"password\", \"password\")\n        .option(\"database\", \"databaseName\")\n        .option(\"dbtable\", \"schemaName.tableName\") # (if schemaName not provided, default to \"dbo\")\n        .load()\n)\n</code></pre> <pre><code>df = (\n    spark.read\n        .format(\"jdbc\")\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n        .option(\"url\", \"jdbc:sqlserver://&lt;host-url&gt;:1433;database=&lt;database-name&gt;\")\n        .option(\"dbtable\", \"tableName\")\n        .option(\"user\", \"[cnct-adb-dev]\")\n        .option(\"password\", \"password\")\n        .load()\n)\n</code></pre> <p>References:</p> <ul> <li>TheDataSwamp: Databricks</li> <li>Databricks: External Data - SQL Server</li> <li>Microsoft</li> <li>Microsoft JDBC Driver for SQL Server</li> </ul>"},{"location":"services/azure/databricks/adb-to-az/#event-hubs","title":"Event Hubs","text":"<p>Warning</p> <p>We should use 1 consumer group per query stream because it will raise <code>ReceiverDisconnectedException</code>. Read More about multiple readers.</p>"},{"location":"services/azure/databricks/adb-to-az/#1-installation-package","title":"1) Installation Package","text":"<pre><code>groupId = com.microsoft.azure\nartifactId = azure-eventhubs-spark_2.12\nversion = 2.3.22\n</code></pre>"},{"location":"services/azure/databricks/adb-to-az/#2-connection-code_1","title":"2) Connection Code","text":"EventhubKafka Protocol <pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName('App Connect Eventhub')\n        .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\")\n        .config(\"spark.locality.wait\", \"15s\")  # Default: 3s\n        .getOrCreate()\n)\n</code></pre> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_namespace}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={eventhubs_name}\"\n)\nehConf = {\n  'eventhubs.connectionString' : spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n  'eventhubs.consumerGroup' : \"$Default\",\n}\n</code></pre> <pre><code>df = (\n  spark\n    .readStream\n    .format(\"eventhubs\")\n    .options(**ehConf)\n    .option(\"maxEventsPerTrigger\", 1_000_000)  # Default: &lt;partition&gt; * 1_000\n    .option(\"useExclusiveReceiver\", False)  # Default: True\n    .option(\"receiverTimeout\", \"PT000100\")  # Default: 60 sec\n    .option(\"operationTimeout\", \"PT000100\")  # Default: 60 sec\n    .load()\n)\n</code></pre> <p>Note</p> <p>This option require enable Kafka on the Azure Event Hubs.</p> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_namespace}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={eventhubs_name}\"\n)\nEH_SASL: str = (\n    f'org.apache.kafka.common.security.plain.PlainLoginModule required '\n    f'username=\"$ConnectionString\" '\n    f'password=\"{connectionString}\";'\n)\n</code></pre> <pre><code>df = (\n    spark\n        .readStream\n        .format(\"kafka\")\n        .option(\"subscribe\", f\"{eventhubs_name}\")\n        .option(\"kafka.bootstrap.servers\", f\"{eventhubs_namespace}.servicebus.windows.net:9093\")\n        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n        .option(\"kafka.sasl.jaas.config\", EH_SASL)\n        .option(\"kafka.request.timeout.ms\", \"60000\")\n        .option(\"kafka.session.timeout.ms\", \"30000\")\n        .option(\"kafka.group.id\", \"$Default\")\n        .option(\"failOnDataLoss\", \"true\")\n        .option(\"spark.streaming.kafka.allowNonConsecutiveOffsets\", \"true\")\n        .load()\n)\n</code></pre> <p>References:</p> <ul> <li>https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#user-configuration</li> <li>https://medium.com/@kaviprakash.2007/structured-streaming-using-azure-databricks-and-event-hub-6b0bcbf029c4</li> <li>Connect your Apache Spark application with Azure Event Hubs</li> <li>Using Apache Spark with Azure Event Hubs for Apache Kafka Ecosystems</li> </ul>"},{"location":"services/azure/databricks/adb-to-az/#iot-hub","title":"IoT Hub","text":""},{"location":"services/azure/databricks/adb-to-az/#1-using-shared-access-key","title":"1) Using Shared Access Key","text":"<ul> <li>Go to Azure IoT Hub &gt; Click on Access Key</li> </ul>"},{"location":"services/azure/databricks/adb-to-az/#2-package-installation","title":"2) Package Installation","text":"<pre><code>groupId = com.microsoft.azure\nartifactId = azure-eventhubs-spark_2.12\nversion = 2.3.22\n</code></pre>"},{"location":"services/azure/databricks/adb-to-az/#3-connection-code","title":"3) Connection Code","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession\n        .builder\n        .appName('App Connect IoT Hub')\n        .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\")\n        .config(\"spark.locality.wait\", \"15s\")  # Default: 3s\n        .getOrCreate()\n)\n</code></pre> <pre><code>connectionString: str = (\n    f\"Endpoint=sb://{eventhubs_compatible_name}.servicebus.windows.net/;\"\n    f\"SharedAccessKeyName={sharekey_name};\"\n    f\"SharedAccessKey={sharekey};\"\n    f\"EntityPath={endpoint_name}\"\n)\nehConf = {\n    'eventhubs.connectionString' : spark._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n    'eventhubs.consumerGroup' : \"$Default\",\n    'eventhubs.partition.count': \"1\",\n    'ehName': f\"{IoTHubs-EventHub-Compatible-Name}\",\n}\n</code></pre> <pre><code>df = (\n  spark\n    .readStream\n    .format(\"eventhubs\")\n    .options(**ehConf)\n    .option(\"maxEventsPerTrigger\", 1_000_000)  # Default: &lt;partition&gt; * 1_000\n    .option(\"useExclusiveReceiver\", False)  # Default: True\n    .option(\"receiverTimeout\", \"PT000100\")  # Default: 60 sec\n    .option(\"operationTimeout\", \"PT000100\")  # Default: 60 sec\n    .load()\n)\n</code></pre> <p>References:</p> <ul> <li>https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md#user-configuration</li> <li>https://medium.com/@kaviprakash.2007/structured-streaming-using-azure-databricks-and-event-hub-6b0bcbf029c4</li> </ul>"},{"location":"services/azure/databricks/adb-to-az/#synapse-sql-pool","title":"Synapse SQL Pool","text":"<p>Read More about Connect to Synapse SQL Pool</p>"},{"location":"services/azure/databricks/adb-to-google/","title":"Connect to Google Services","text":""},{"location":"services/azure/databricks/adb-to-google/#authentication","title":"Authentication","text":""},{"location":"services/azure/databricks/adb-to-google/#bigquery","title":"BigQuery","text":""},{"location":"services/azure/databricks/adb-to-google/#using-json-encoding","title":"Using JSON Encoding","text":"<pre><code>credentials_json_str: str = dbutils.secrets.get(scope=\"&lt;scope-name&gt;\", key=\"&lt;secret-key-name&gt;\")\n\ndf = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"credentials\", base64.b64encode(credentials_json_str.encode()).decode('utf-8'))\n        .option(\"parentProject\", \"&lt;project-id&gt;\")\n        .option(\"table\", \"&lt;dataset&gt;.&lt;table-name&gt;\")\n        .load()\n)\ndf.show()\n</code></pre> <p>Bug</p> <p>GitHub: Error getting access token from metadata server</p>"},{"location":"services/azure/databricks/adb-to-google/#using-google_application_credentials","title":"Using GOOGLE_APPLICATION_CREDENTIALS","text":"<pre><code>import os\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"&lt;/path/to/key/file&gt;\"\n</code></pre>"},{"location":"services/azure/databricks/adb-to-google/#using-filepath","title":"Using Filepath","text":"<pre><code>df = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"credentialsFile\", \"&lt;/path/to/key/file&gt;\")\n        .option(\"table\", \"&lt;dataset&gt;.&lt;table-name&gt;\")\n        .load()\n)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-google/#access-token","title":"Access Token","text":"<pre><code># Globally\nspark.conf.set(\"gcpAccessToken\", \"&lt;access-token&gt;\")\n\n# Per read/Write\ndf = (\n    spark.read\n        .format(\"bigquery\")\n        .option(\"gcpAccessToken\", \"&lt;access-token&gt;\")\n)\n</code></pre> <p>References:</p> <ul> <li>(https://docs.databricks.com/en/external-data/bigquery.html#step-2-set-up-databricks)</li> <li>(https://github.com/GoogleCloudDataproc/spark-bigquery-connector)</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/","title":"To Synapse SQL Pool","text":"<p>When you want to read and write data on Azure Synapse Analytic SQL Pool via Azure Databricks, that has 2 types of Azure Synapse SQL Pool:</p> <ul> <li>Serverless SQL Pool</li> <li>Dedicate SQL Pool</li> </ul> <p>Why do we need staging storage?</p> <p>Staging folder is needed to store some temporary data whenever we read/write data from/to Azure Synapse. Whenever we read/write data, we actually leverage PolyBase to move the data, which staging storage is used to achieve high performance.</p>"},{"location":"services/azure/databricks/adb-to-synapse/#serverless-sql-pool","title":"Serverless SQL Pool","text":""},{"location":"services/azure/databricks/adb-to-synapse/#1-prerequisite","title":"1) Prerequisite","text":"<p>If you want to see the list of existing database scope credential, you can use this command:</p> <pre><code>SELECT * FROM [sys].[database_scoped_credentials];\n</code></pre> <p>Create external datasource for connection from Synapse Serverless to Azure Data Lake Storage.</p> <pre><code>IF NOT EXISTS (\n    SELECT *\n    FROM [sys].[external_data_sources]\n    WHERE NAME = 'data_curated_adb'\n)\n    CREATE EXTERNAL DATA SOURCE [data_curated_adb]\n    WITH (\n        CREDENTIAL = [adb_cred],\n        LOCATION = 'abfss://{curated}@{dataplatdev}.dfs.core.windows.net'\n    );\nGO\n</code></pre> <p>Read More about External Data Source</p>"},{"location":"services/azure/databricks/adb-to-synapse/#2-create-user-in-serverless-sql-pool","title":"2) Create User in Serverless SQL Pool","text":"<p>Create login user and grant permission reference above database scope credential</p> <pre><code>CREATE LOGIN [adbuser] WITH PASSWORD = '&lt;password&gt;';\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[adb_cred] TO [adbuser];\nGO\n</code></pre> <p>Create temp view for read data from above external datasource</p> <pre><code>CREATE OR ALTER VIEW [CURATED].[VW_DELTA_SALES]\nAS SELECT *\n   FROM OPENROWSET(\n       BULK '/{delta_silver}/{table_sales}',\n       DATA_SOURCE = 'data_curated_adb',\n       FORMAT = 'DELTA'\n   ) AS [R]\n;\n\nGRANT SELECT ON OBJECT::[CURATED].[VW_DELTA_SALES] TO [adbuser]\n;\n</code></pre> <p>More Detail, Control storage account access for serverless SQL pool in Azure Synapse Analytics</p>"},{"location":"services/azure/databricks/adb-to-synapse/#3-connection-code","title":"3) Connection Code","text":""},{"location":"services/azure/databricks/adb-to-synapse/#method-01-jdbc-connector","title":"Method 01: JDBC Connector","text":"<p>This method reads or writes the data row by row, resulting in performance issues. Not Recommended.</p> <p>Set Spark Config:</p> Azure Data Lake Gen 2Azure Blob Storage <pre><code>spark.conf.set(\n    \"fs.azure.account.key.{dataplatdev}.dfs.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\nsc._jsc.hadoopConfiguration().set(\n    \"fs.azure.account.key.{dataplatdev}.dfs.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\n\ndf = (\n  spark.read\n      .format(\"jdbc\")\n      .option(\"url\", (\n          f\"jdbc:sqlserver://{server}:1433;database={database};user={username};password={password};\"\n          f\"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n      ))\n      .option(\"tempDir\", \"abfss://{curated}@{storage-account}.dfs.core.windows.net/&lt;folder-for-temporary-data&gt;\")\n      .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n      .option(\"query\", \"SELECT * FROM [CURATED].[VW_DELTA_SALES]\")\n      .load()\n  )\n</code></pre> <pre><code>spark.conf.set(\n    \"fs.azure.account.key.{dataplatdev}.blob.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\nsc._jsc.hadoopConfiguration().set(\n    \"fs.azure.account.key.{dataplatdev}.blob.core.windows.net\",\n    \"&lt;storage-account-access-key&gt;\"\n)\n\ndf = (\n  spark.read\n      .format(\"jdbc\")\n      .option(\"url\", (\n          f\"jdbc:sqlserver://{server}:1433;database={database};user={username};password={password};\"\n          f\"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n      ))\n      .option(\"tempDir\", \"wasbs://{curated}@{storage-account}.blob.core.windows.net/&lt;folder-for-temporary-data&gt;\")\n      .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n      .option(\"query\", \"SELECT * FROM [CURATED].[VW_DELTA_SALES]\")\n      .load()\n  )\n</code></pre> <p>Reference:</p> <ul> <li>Microsoft Databricks JDBC</li> <li>Spark SQL Data Source JDBC</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#method-02-spark-connector","title":"Method 02: Spark Connector","text":"<p>This method uses bulk insert to read/write data. There are a lot more options that can be further explored. First Install the Library using Maven Coordinate in the Data-bricks cluster, and then use the below code. Recommended for Azure SQL DB or Sql Server Instance</p> <p>Install Driver on cluster:</p> <ul> <li>Maven: <code>com.microsoft.azure:spark-mssql-connector_2.12:1.2.0</code></li> </ul> SPARK VERSION MAVEN DEPENDENCY Spark 2.4.x groupeId: com.microsoft.azure  artifactId: spark-mssql-connector  version : 1.0.2 Spark 3.0.x groupeId: com.microsoft.azure  artifactId: spark-mssql-connector_2.12  version : 1.1.0 Spark 3.1.x groupeId: com.microsoft.azure  artifactId: spark-mssql-connector_2.12  version : 1.2.0 <pre><code>Read More [Supported Version](https://search.maven.org/search?q=spark-mssql-connector)\n</code></pre> TableCustom Query <pre><code>df = (\n  spark.read\n    .format(\"com.microsoft.sqlserver.jdbc.spark\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;server-name&gt;:1433;database=&lt;database-name&gt;;\")\n    .option(\"user\", username)\n    .option(\"password\", password)\n    .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n    .option(\"encrypt\", \"true\")\n    .option(\"dbTable\", \"[&lt;schema&gt;].[&lt;table-or-view&gt;]\")\n    .load()\n)\n</code></pre> <pre><code>df = (\n  spark.read\n    .format(\"com.microsoft.sqlserver.jdbc.spark\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;server-name&gt;:1433;database=&lt;database-name&gt;;\")\n    .option(\"user\", username)\n    .option(\"password\", password)\n    .option(\"mssqlIsolationLevel\", \"READ_UNCOMMITTED\")\n    .option(\"encrypt\", \"true\")\n    .option(\"query\", \"SELECT * FROM [sys].[external_data_sources]\")\n    .load()\n)\n</code></pre> <p>Reference:</p> <ul> <li>Microsoft SQL Spark Connector</li> <li>SQL Spark Connector</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#dedicate-sql-pool","title":"Dedicate SQL Pool","text":"<p>When connect to Azure Synapse Dedicated SQL Pool, we will use special spark connector, <code>com.databricks.spark.sqldw</code> method. This method previously uses Poly-base to read and write data to and from <code>Azure Synapse</code> using a staging server (mainly, blob storage or a Data Lake storage directory), but now data are being read and write using Copy, as the Copy method has improved performance. Recommended for Azure Synapse</p> <p>Note</p> <p>This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.</p>"},{"location":"services/azure/databricks/adb-to-synapse/#sql-authentication","title":"SQL Authentication","text":""},{"location":"services/azure/databricks/adb-to-synapse/#1-connection-code","title":"1) Connection Code","text":"<pre><code>spark.conf.set(\"spark.databricks.sqldw.writeSemantics\", \"copy\")\n</code></pre> <pre><code>df = (\n    spark.read\n        .format(\"com.databricks.spark.sqldw\")\n        .option(\"url\", f\"jdbc:sqlserver://&lt;work-space-name&gt;;database=&lt;database-name&gt;;\")\n        .option(\"user\", \"&lt;username&gt;\")\n        .option(\"password\", \"&lt;password&gt;\")\n        .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n        .option(\"dbTable\", \"&lt;your-table-name&gt;\")\n        .option(\"tempDir\", \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;directory-name&gt;\")\n        .option(\"hostNameInCertificate\", \"*.sql.azuresynapse.net\")\n        .option(\"loginTimeout\", \"30\")\n        .option(\"encrypt\", \"true\")\n        .option(\"trustServerCertificate\", \"true\")\n        .load()\n)\n</code></pre> <p>Reference:</p> <ul> <li>https://bennyaustin.com/2020/02/05/pysparkupsert/</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#azure-service-principle","title":"Azure Service Principle","text":""},{"location":"services/azure/databricks/adb-to-synapse/#1-create-service-principal","title":"1) Create Service Principal","text":"<ul> <li>Go to <code>Azure Active Directory</code> <code>App registrations</code> <code>New registration</code></li> <li>Add the information of this app like <code>name</code> is <code>adb_to_synapse</code></li> <li>Click register for create</li> <li>Go to <code>App registrations</code> <code>Certificates&amp;secrets</code> <code>New Client Secret</code></li> <li>Save this value to <code>Azure Key Vaults</code></li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#2-create-user-in-azure-synapse","title":"2) Create User in Azure Synapse","text":"<ul> <li>Give it some permissions (On the Dedicated SQL pool, we can add a user and   assign it to the proper role),</li> </ul> <pre><code>CREATE USER [adb_to_synapse] FROM EXTERNAL PROVIDER;\nsp_addrolemember 'db_owner','adb_to_synapse';\nGO\n</code></pre> <p>Warning</p> <p>The permission of the user should be owner of database because it is currently required for Databricks to run <code>CREATE DATABASE SCOPED CREDENTIAL</code>.</p> <p>Note</p> <p>If you do not want to give owner permission to your Service Principle, you can grant <code>CONTROL</code>.</p> <pre><code>CREATE ROLE [databricks_reader];\nEXEC sp_addrolemember 'databricks_reader', 'adb_to_synapse';\nGRANT CONTROL TO [adb_to_synapse];\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse/#3-azure-storage-temp-account","title":"3) Azure Storage Temp Account","text":"<ul> <li>Go to <code>Storage account</code> <code>Access Control (IAM)</code> <code>Add role assignment</code></li> <li>Select Role: <code>Storage Blob Data Contributor</code></li> <li>Select: <code>register application</code></li> <li>Click on save.</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#4-connection-code","title":"4) Connection Code","text":"<p>OAuth Configuration:</p> <pre><code>spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", \"&lt;service-principal-id&gt;\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", \"&lt;service-principal-secret&gt;\")\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", \"https://login.microsoftonline.com/&lt;directory-id&gt;/oauth2/token\")\n\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.id\", \"&lt;service-principal-id&gt;\")\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.secret\", \"&lt;service-principal-secret&gt;\")\n</code></pre> <p>JDBC URL Pattern:</p> <pre><code>URL: str = (\n    \"jdbc:sqlserver://&lt;work-space-name&gt;.sql.azuresynapse.net:1433;\"\n    \"database=&lt;database-name&gt;;\"\n    \"encrypt=true;trustServerCertificate=true;\"\n    \"hostNameInCertificate=*.sql.azuresynapse.net;\"\n    \"loginTimeout=30\"\n)\n</code></pre> <pre><code>df = (\n  spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://&lt;work-space-name&gt;.sql.azuresynapse.net:1433;\")\n    .option(\"tempDir\", \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;directory-name&gt;\")\n    .option(\"enableServicePrincipalAuth\", \"true\")\n    .option(\"dbTable\", \"[&lt;schema&gt;].[&lt;table-name&gt;]\")\n    .load()\n)\n</code></pre> <p>References:</p> <ul> <li>https://pl.seequality.net/load-synapse-analytics-sql-pool-with-azure-databricks/</li> <li>https://learn.microsoft.com/en-us/answers/questions/327270/azure-databricks-to-azure-synapse-service-principa?orderby=newest</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#send-ddl-or-dml-to-azure-synapse-sql-pool","title":"Send DDL or DML to Azure Synapse SQL Pool","text":"<p>When execute DDL or DML statement to Azure Synapse SQL Pool, that has 2 solutions: JDBC, and ODBC drivers.</p>"},{"location":"services/azure/databricks/adb-to-synapse/#jdbc-driver","title":"JDBC Driver","text":""},{"location":"services/azure/databricks/adb-to-synapse/#1-create-jdbc-connection","title":"1) Create JDBC Connection","text":"<pre><code>URL = f\"jdbc:sqlserver://{server}:1433;database={database};\"\n\nprops = spark._sc._gateway.jvm.java.util.Properties()\nprops.putAll({\n    'username': username,\n    'password': password,\n    'Driver': \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n})\n\nConnection = spark._sc._gateway.jvm.java.sql.Connection\n\ndriver_manager = spark._sc._gateway.jvm.java.sql.DriverManager\nconnection: Connection = driver_manager.getConnection(URL, props)\n</code></pre>"},{"location":"services/azure/databricks/adb-to-synapse/#2-connection-code","title":"2) Connection Code","text":"Statement ExecutionBatch ExecutionCall Store Procedure <pre><code>ResultSet = spark._sc._gateway.jvm.java.sql.ResultSet\nResultSetMetaData = spark._sc._gateway.jvm.java.sql.ResultSetMetaData\nConnection = spark._sc._gateway.jvm.java.sql.Connection\nStatement = spark._sc._gateway.jvm.java.sql.Statement\n\nstmt: Statement = connection.createStatement()  # Statement\n</code></pre> <pre><code>query: str = f\"\"\"\n  SELECT * FROM [&lt;schema&gt;].[&lt;table-name&gt;]\n\"\"\"\n\nrs: ResultSet = stmt.executeQuery(query)  # ResultSet\nmetadata: ResultSetMetaData = rs.getMetaData()  # ResultSetMetaData\ncol_numbers = metadata.getColumnCount()\n\ncol_names: list = []\nfor i in range(1, col_numbers + 1):\n    if column:\n      col_names.append(metadata.getColumnName(i))\n    else:\n      col_names.append(f\"col_{i}\")\n\nresults: list = []\nwhile rs.next():\n  result: dict = {}\n  for i in range(col_numbers):\n    name: str = col_names[i]\n    result[name] = rs.getString(name)\n  results.append(result)\n</code></pre> <pre><code>stmt.close()\nconnection.close()\n</code></pre> <pre><code>PreparedStatement = spark._sc._gateway.jvm.java.sql.PreparedStatement\n\npreps: PreparedStatement = connection.prepareStatement(\n  \"INSERT INTO [dev].[people]\"\n  \"VALUES (?, ?, ?);\"\n)\nrows = [\n  [\"Gandhi\", \"politics\", 12],\n  [\"Turing\", \"computers\", 31],\n]\nfor row in rows:\n  for idx, data in enumerate(row, start=1):\n    if isinstance(data, int):\n      preps.setInt(idx, data)\n    else:\n      preps.setString(idx, data)\n  preps.addBatch()\n\nconnection.setAutoCommit(False)\nresult_number: int = preps.executeBatch()\npreps.clearBatch()\nconnection.setAutoCommit(True)\n</code></pre> <p>Note: Add parameter <code>rewriteBatchedStatements=true</code> to JDBC URL for improve execute performance from before add this parameter, <pre><code>INSERT INTO jdbc (`name`) VALUES ('Line 1: Lorem ipsum ...')\nINSERT INTO jdbc (`name`) VALUES ('Line 2: Lorem ipsum ...')\n</code></pre> Then, after add this parameter to JDBC URL, <pre><code>INSERT INTO jdbc (`name`) VALUES ('Line 1: Lorem ipsum ...'), ('Line 2: Lorem ipsum ...')\n</code></pre></p> <pre><code>exec_statement = connection.prepareCall(\n    f\"\"\"{{CALL {schema}.usp_stored_procedure(\n      {master_id}, {parent_id}, {child_id}, '{table}', ?,\n      ?, ?, ?, ?\n    )}}\"\"\"\n)\nexec_statement.setString(5, 'data')\n\nexec_statement.registerOutParameter(1, spark._sc._gateway.jvm.java.sql.Types.INTEGER)\nexec_statement.registerOutParameter(2, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\nexec_statement.registerOutParameter(3, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\nexec_statement.registerOutParameter(4, spark._sc._gateway.jvm.java.sql.Types.VARCHAR)\n\nexec_statement.executeUpdate()\n\nres1 = exec_statement.getInt(1)\nres2 = exec_statement.getString(2)\nres3 = exec_statement.getString(3)\nres4 = exec_statement.getString(4)\n\nexec_statement.close()\nconnection.close()\n</code></pre> <p>Reference:</p> <ul> <li>How to Call MSSQL Stored Procedure</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#odbc-driver","title":"ODBC Driver","text":""},{"location":"services/azure/databricks/adb-to-synapse/#1-create-odbc-connection","title":"1) Create ODBC Connection","text":"<pre><code>%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n</code></pre> <pre><code>import pyodbc\nserver = '&lt;server-name&gt;'\ndatabase = '&lt;database-name&gt;'\nusername = '&lt;username&gt;'\npassword = '&lt;password&gt;'\n\nconn = pyodbc.connect(\n  f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n  f'SERVER={server};DATABASE={database};UID={username};PWD={password}'\n)\n</code></pre> <p>Reference:</p> <ul> <li>Using PyODBC in Azure Databricks for Connecting with MSSQL</li> </ul>"},{"location":"services/azure/databricks/adb-to-synapse/#read-mores","title":"Read Mores","text":"<ul> <li>(https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html)</li> <li>(https://joeho.xyz/blog-posts/how-to-connect-to-azure-synapse-in-azure-databricks/)</li> <li>(https://learn.microsoft.com/en-us/answers/questions/653154/databricks-packages-for-batch-loading-to-azure.html)</li> <li> Spark: optimise writing a DataFrame to SQL Server</li> <li>(https://docs.databricks.com/external-data/synapse-analytics.html)</li> <li>(https://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-set-up-access-control)</li> </ul>"},{"location":"services/azure/databricks/adb-uc-privileges/","title":"Privileges","text":"<p>Initially, users have no access to data in a metastore. Azure Databricks account admins, workspace admins, and metastore admins have default privileges for managing Unity Catalog.</p> <p>All securable objects in Unity Catalog have an owner. Object owners have all privileges on that object, including the ability to grant privileges to other principals.</p> <p>Note</p> <p>Privileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Account admins can also grant privileges directly on a metastore.</p>"},{"location":"services/azure/databricks/adb-uc-privileges/#manage-privileges","title":"Manage Privileges","text":""},{"location":"services/azure/databricks/adb-uc-privileges/#read-mores","title":"Read Mores","text":"<ul> <li> Manage privileges in Unity Catalog</li> <li> Unity Catalog privileges and securable objects</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/","title":"Setup Unity Catalog","text":"<p>Unity Catalog is a data governance solution for Databricks, designed to provide a unified approach across all of your Databricks workspaces.</p> <p>Important</p> <p>Azure Databricks workspace should be Premium pricing tier.</p>"},{"location":"services/azure/databricks/adb-uc-setup/#prerequisite","title":"Prerequisite","text":"<p>To enable the Databricks Account Console and establish your first Account Admin, you will need to engage someone who has the Microsoft Entra ID (Azure Active Directory) Global Administrator role</p> <p>For security purposes, only someone with the Global Administrator role has permission to assign the first account admin role. After completing these steps, you can remove the Global Administrator role from the Azure Databricks account.</p> <ol> <li>Create Resource Group</li> <li>Create Premium Tier Azure Databricks Workspace</li> <li>Create Azure DataLake Gen2 Storage Account and Container</li> <li>Create Access Connector for Azure Databricks</li> <li>Grant Storage Blob Data Contributor to Access Connector for Azure Databricks on Azure DataLake Gen2 Storage Account</li> <li>Enable Unity Catalog by creating Metastore and assigning to Workspace</li> </ol> <p>Note</p> <p>If you do not create new Access Connector and use default provisioning, it will not use any managed identity on this Access Connector. The default Accesss Connector name is <code>unity-catalog-access-connector</code></p>"},{"location":"services/azure/databricks/adb-uc-setup/#getting-started","title":"Getting Started","text":"<ul> <li>Go to Azure Databricks Workspace  Click on Manage Account  Login into Account console</li> <li>Click on the Data tab  Create Metastores tab</li> <li>Provide information to create metastore (1 metastore per Region):<ul> <li>Metastore Name and Region (The best practice is choose same region and resource group)</li> <li>Azure DataLake Storage Gen2 (Example: <code>https://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;path&gt;</code>)</li> <li>Access Connector ID (Resource ID of your Access Connector)</li> </ul> </li> <li>Assign the workspace map to this metastore  Click on Enable Unity Catalog</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/#external-catalog","title":"External Catalog","text":"<p>Create storage credentials:</p> <ul> <li>On your Azure Databricks Workspace  Go to Data Explorer External Data  Select Storage Credentials</li> <li>Click Add and then select Add a storage credential  Select Service Principal</li> <li>Enter the Storage credential name of your choice  Provide   your service principle information  Click Create</li> </ul> <p>Create external location:</p> <ul> <li>In the Data Explorer  Select External Locations  Click Add an external location</li> <li>Enter the External location name and Azure DataLake Storage Gen2 URL    Select the Storage credential you created  Click Create</li> </ul>"},{"location":"services/azure/databricks/adb-uc-setup/#read-mores","title":"Read Mores","text":"<ul> <li> Enabling Unity Catalog on Azure Databricks: A Step-by-Step Guide</li> <li> Set up and manage Unity Catalog</li> <li> Azure Databricks administration introduction</li> <li> Create a Unity Catalog metastore</li> <li> Use Azure managed identities in Unity Catalog to access storage</li> </ul>"},{"location":"services/azure/devops/devops-for-loop/","title":"For Loop","text":""},{"location":"services/azure/devops/devops-for-loop/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/devops/devops-multi-repo/","title":"Multi-Repositories","text":""},{"location":"services/azure/devops/devops-multi-repo/#getting-started","title":"Getting Started","text":"<pre><code>pool:\n  vmImage: ubuntu-latest\n\ntrigger:\n  - none\n\nparameters:\n  - name: repo_branch\n    type: string\n    default: \"main\"\n\nresources:\n  repositories:\n  - repository: self\n    ref: $(branch)\n  - repository: repo\n    type: git\n    name: {project-name}/{repo-name}\n    ref: ${{ parameters.repo_branch }}\n\nsteps:\n  - checkout: self\n  - checkout: repo\n\n  - script: ls -al $(Build.SourcesDirectory)\n    displayName: 'List on source dir'\n\n  - task: CopyFiles@2\n    inputs:\n      SourceFolder: '$(Build.SourcesDirectory)'\n      Contents: '**'\n      TargetFolder: '$(Build.ArtifactStagingDirectory)'\n\n  - task: DeleteFiles@1\n    inputs:\n      SourceFolder: '$(Build.ArtifactStagingDirectory)'\n      Contents: |\n        **/.git\n\n  - task: PublishBuildArtifacts@1\n    inputs:\n      PathtoPublish: '$(Build.ArtifactStagingDirectory)'\n      ArtifactName: 'drop'\n      publishLocation: 'Container'\n    displayName: 'Publish Artifact: drop'\n</code></pre>"},{"location":"services/azure/devops/devops-self-hosted/","title":"Self Hosted with FastAPI","text":"<p>When we want to create a Rest API application like Azure Function App on an on-premises server, it is simply to use the FastAPI package, which is a lightweight Python ASGI web application, for this our purpose.</p>"},{"location":"services/azure/devops/devops-self-hosted/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/devops/devops-self-hosted/#setup-application","title":"Setup Application","text":"<p>Let\u2019s start setting up your FastAPI application and a <code>.bat</code> script for run this application with dynamic input arguments.</p> app/app.py<pre><code>from fastapi import FastAPI\n\ndef create_app() -&gt; FastAPI:\n    \"\"\"Application Factory\"\"\"\n    app = FastAPI()\n\n    @app.get(\"/health/\")\n    async def health():\n        return {\"message\": \"Hello World\"}\n\n    return app\n</code></pre> main.py<pre><code>import uvicorn\nfrom app.app import create_app\n\napp = create_app()\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> scripts/runserver.bat<pre><code>@echo off\ngoto :init\n\n:usage\n    echo USAGE:\n    echo   %__bat_filename% [flags] \"release argument\"\n    echo.\n    echo.  -h, --help           shows this help\n    echo.  -p, --port value     specifies a port number value\n    echo.  --reload             enable auto-reload\n    goto :eof\n\n:missing_args\n    call :usage\n    echo.\n    echo ****\n    echo MISSING RELEASE ARGUMENT !!!\n    goto :eof\n\n:port\n    echo Port does set from argument and port changes from 8000 to %__port% ...\n    goto :eof\n\n:version\n    if \"%~1\"==\"full\" call :usage &amp; goto :eof\n    echo %__version%\n    goto :eof\n\n:init\n    set \"__name=%~n0\"\n    set \"__port=8000\"\n\n    set \"__bat_filepath=%~0\"\n    set \"__bat_path=%~dp0\"\n    set \"__bat_filename=%~nx0\"\n\n    set \"__version=0.1.0\"\n    set \"__reload=\"\n    set \"__release=\"\n\n:parse\n    if \"%~1\"==\"\"                goto :validate\n\n    if /i \"%~1\"==\"-h\"           call :usage \"%~2\" &amp; goto :end\n    if /i \"%~1\"==\"--help\"       call :usage \"%~2\" &amp; goto :end\n\n    if /i \"%~1\"==\"-v\"           call :version      &amp; goto :end\n    if /i \"%~1\"==\"--version\"    call :version full &amp; goto :end\n\n    if /i \"%~1\"==\"-p\"           set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n    if /i \"%~1\"==\"--port\"       set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n\n    if /i \"%~1\"==\"--reload\"     set \"__reload=--reload\" &amp; shift &amp; goto :parse\n\n    if not defined __release    set \"__release=%~1\" &amp; shift &amp; goto :parse\n\n    shift\n    goto :parse\n\n:validate\n    if not defined __release call :missing_args &amp; goto :end\n\n:main\n    echo INFO: Start running server with release \"%__release%\" ...\n    call .\\venv\\Scripts\\activate\n    call uvicorn main:app --port %__port% %__reload%\n\n:end\n    echo.\n    echo End and Clean Up\n    call :cleanup\n    exit /B\n\n:cleanup\n    REM The cleanup function is only really necessary if you\n    REM are _not_ using SETLOCAL.\n\n    set \"__name=\"\n    set \"__port=\"\n\n    set \"__bat_filepath=\"\n    set \"__bat_path=\"\n    set \"__bat_filename=\"\n\n    set \"__release=\"\n    set \"__version=\"\n    set \"__reload=\"\n\n    goto :eof\n</code></pre> <p>Note</p> <p>I use the <code>.bat</code> script because the on-premise server that I want to run is Windows OS.</p>"},{"location":"services/azure/devops/devops-self-hosted/#deploy-to-window-service","title":"Deploy to Window Service","text":"<p>I use the NSSM software for wrap the <code>runserver.bat</code> script file and monitor whether my app is able to run continuously on the Windows service. So, I will Download NSSM and unzip the installed file to the current path.</p> <p>Note</p> <p>We cannot use the Docker container in the target on-premises server because of the Windows Server version does not support, it be Windows Server 2016 which does not support Linux container on VM and WSL.</p> <p>First, we install my application on the Windows service, which can be seen in the Services software.</p> <pre><code>.\\nssm\\win64\\nssm.exe install \"FastAPIService\" \"%cd%\\runserver.bat\"\n</code></pre> <p>Next, we can setup additional the logging component for <code>stdout</code> and <code>stderr</code> in this application.</p> <pre><code>.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppStdout \"%cd%\\logs\\FastAPIService.log\"\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppStderr \"%cd%\\logs\\FastAPIService.log\"\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateFiles 1\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateOnline 1\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateSeconds 86400\n.\\nssm\\win64\\nssm.exe set \"FastAPIService\" AppRotateBytes 1048576\n</code></pre> <p>Finally, we start the application service by <code>sc.exe</code> command.</p> <pre><code>sc.exe start \"FastAPIService\"\n</code></pre> <p>Warning</p> <p>I cannot use the python package, <code>pywin32</code>, because I get the error message;</p> <pre><code>Error 1053: The service did not respond to the start or control request in a timely fashion\n</code></pre> <p>when start this application service on locally.</p>"},{"location":"services/azure/devops/devops-self-hosted/#setup-agent-to-on-premises-server","title":"Setup Agent to On-Premises Server","text":"<p>We will create a CI/CD deployment pipeline with Azure DevOps that able to deploy the application to target server. The purpose is running this application on the Windows service in an on-premises server.</p> <p>Firstly, the on-premises server does not connect to Azure DevOps because It was not listed in the Agent Pools by Self-Hosted agent connection type in my organization setting. So, we create a new agent pool name like <code>MYSTDVM01</code> and follow the Azure document to list this new agent in the Agents menu.</p> <p>Note</p> <p>More detail about the new Agent implementation, How to install Self-hosted Windows agent for Azure DevOps.</p> <p>If you want to let everyone in your group of Azure DevOps can see and use this agent, you should add owner permission to your group by Organization Setting &gt; Agent Pools &gt; Your Agent Pool &gt; Security.</p> <p>Next, We create a folder <code>$(Agent.HomeDirectory)/app</code> for keeping source code without the DevOps pipeline process before deploying my artifact to this agent.</p> <p>Finally, we set up a Python interpreter for running the Python application in this agent.</p> <ul> <li> <p>Download the required version of Python and install it on this agent</p> </li> <li> <p>Copy all the installed python files from <code>C:/Users/{user}/AppData/Local/Programs/Python/Python39</code>   to <code>$(Agent.ToolsDirectory)/Python/3.9.13/x64/*</code></p> </li> <li> <p>Create a complete file at <code>$(Agent.ToolsDirectory)/Python/3.9.13/x64.complete</code> for trigger Azure DevOps   pipeline can use this package in the job</p> </li> </ul> <p>Note</p> <p>If your server set a proxy firwall rule, you can run self-hosted agent config by <code>./config.cmd --proxyurl http://proxy.domain.co.th --proxyusername \"CEMENTH/{user}\" --proxypassword \"*******\"</code>, it will save your password to <code>.proxycredentials</code> file for reuse this password for proxy mode configuration.</p>"},{"location":"services/azure/devops/devops-self-hosted/#deploy-to-on-premises-server","title":"Deploy to On-Premises Server","text":"<p>For the CI pipeline, I test code and package dependencies on the artifact server.</p> <pre><code>jobs:\n  - job: Phase_1\n    displayName: Build and Test\n    pool:\n      vmImage: windows-latest\n    variables:\n      python.version: \"3.9, 3.10\"\n    steps:\n      - checkout: self\n        clean: true\n        fetchDepth: 1\n      - task: UsePythonVersion@0\n        displayName: Use Python $(python.version)\n        inputs:\n          versionSpec: $(python.version)\n      - task: CmdLine@2\n        displayName: Install dependencies\n        inputs:\n          script: python -m pip install --upgrade pip &amp;&amp; pip install -r requirements.txt\n          workingDirectory: fastapi\n          failOnStderr: true\n      - task: CmdLine@2\n        displayName: pytest\n        inputs:\n          script: pip install pytest &amp;&amp; pytest tests --doctest-modules --junitxml=junit/test-results.xml\n          workingDirectory: fastapi\n          failOnStderr: true\n      - task: PublishTestResults@2\n        displayName: Publish Test Results **/test-results.xml\n        inputs:\n          testResultsFiles: \"**/test-results.xml\"\n          failTaskOnFailedTests: true\n          testRunTitle: Python $(python.version)\n  - job: Phase_2\n    displayName: Publish\n    dependsOn: Phase_1\n    pool:\n      vmImage: windows-latest\n    steps:\n      - checkout: self\n        clean: true\n        fetchDepth: 1\n      - task: UsePythonVersion@0\n        displayName: Use Python 3.9.13\n        inputs:\n          versionSpec: 3.9.13\n      - task: CmdLine@2\n        displayName: Create virtual environment\n        inputs:\n          script: python -m pip install --upgrade pip &amp;&amp; python -m venv venv\n          workingDirectory: fastapi\n      - task: CmdLine@2\n        displayName: Install dependencies\n        inputs:\n          script: .\\venv\\Scripts\\activate &amp;&amp; pip install -r requirements.txt --no-cache\n          workingDirectory: fastapi\n      - task: CmdLine@2\n        displayName: Pack dependency files to wheel format\n        inputs:\n          script: .\\venv\\Scripts\\activate &amp;&amp; pip wheel -w wheels -r .\\requirements.txt &amp;&amp; echo \"Create wheel files successful.\"\n          workingDirectory: fastapi\n      - task: PublishBuildArtifacts@1\n        displayName: \"Publish Artifact: drop\"\n        inputs:\n          PathtoPublish: fastapi\n</code></pre> <p>Note</p> <p>For the Python dependencies in the requirement.txt file, I use wheel to download these dependencies from PyPI and save them to <code>\\wheels</code> path by <code>pip wheel -w wheels -r .\\requirements.txt</code></p> <p>For the CD pipeline, I deploy the application to the Windows service and the test service.</p> <pre><code>jobs:\n- job: Phase_1\n  displayName: Deploy job\n  pool:\n    vmImage: Self-hosted agent\n  variables:\n    python.version: '3.9, 3.10'\n  variables:\n    application_name: 'FastAPIServiceDev'\n    application_port: '8001'\n    application_folder: 'dev/app_dmz'\n    health_route: 'health/'\n  steps:\n  - powershell: |\n      echo \"User that this job uses to run:\"\n      whoami\n      $service = Get-Service -Name \"$(application_name)\" -ErrorAction SilentlyContinue\n      if ($service -eq $null)\n      {\n          echo \"Service does not exists.\"\n      } else {\n          echo \"Service does exists ...\"\n          sc.exe stop \"$(application_name)\"\n          nssm.exe remove \"$(application_name)\" confirm\n          echo \"Success stop and remove service.\"\n      }\n      Start-Sleep -Seconds 20\n    displayName: 'Remove existing Windows Service'\n  - task: CopyFiles@2\n    displayName: 'Copy Files to App Folder'\n    inputs:\n      SourceFolder: '$(System.DefaultWorkingDirectory)/_DATA360-DMZ/drop'\n      Contents: |\n        **\n        !.git/**/*\n        !.gitignore\n        !.gitattributes\n        !venv/**\n      TargetFolder: '$(Agent.HomeDirectory)/app/$(application_folder)'\n      CleanTargetFolder: true\n  - task: UsePythonVersion@0\n    displayName: 'Use Python 3.9.13'\n    inputs:\n      versionSpec: 3.9.13\n      disableDownloadFromRegistry: true\n  steps:\n  - powershell: |\n      echo \"Start run setup ...\"\n      .\\scripts\\setup.bat\n      $service = Get-Service -Name \"$(application_name)\" -ErrorAction SilentlyContinue\n      if ($service -eq $null)\n      {\n          echo \"Service does not exists\"\n          nssm.exe install \"$(application_name)\" \"$(pwd)\\scripts\\runserver.bat\" \"--port $(application_port) develop\"\n          nssm.exe set \"$(application_name)\" AppDirectory \"$(pwd)\"\n          echo \"Set AppStdout to $(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppStdout \"$(pwd)\\logs\\$(application_name).log\"\n          echo \"Set AppStderr to $(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppStderr \"$(pwd)\\logs\\$(application_name).log\"\n          nssm.exe set \"$(application_name)\" AppRotateFiles 1\n          nssm.exe set \"$(application_name)\" AppRotateOnline 1\n          nssm.exe set \"$(application_name)\" AppRotateSeconds 86400\n          nssm.exe set \"$(application_name)\" AppRotateBytes 1048576\n      } else {\n          echo \"Service does exists.\"\n      }\n      nssm.exe status \"$(application_name)\"\n      sc.exe start \"$(application_name)\"\n    workingDirectory: '$(Agent.HomeDirectory)\\app\\$(application_folder)\\'\n    displayName: 'Setup and Install Windows service'\n- job: Phase_1\n  displayName: Deploy job\n  pool:\n    vmImage: Self-hosted agent\n  variables:\n    application_port: '8001'\n    health_route: 'health/'\n  steps:\n  - powershell: |\n      $r = curl.exe \"http://localhost:$(application_port)/$(health_route)\" --silent\n      if (($r -ne '{\"detail\":\"Not Found\"}') -or ($r)) {\n        echo \"SUCCESS: The application can run normally.\"\n        exit 0;\n      } else {\n        echo \"ERROR: Failed to request to health check endpoint.\"\n        exit 1;\n      }\n    displayName: 'RestAPI to Health Check'\n</code></pre> <p>Warning</p> <p>We should install NSSM on that on-premises server before running this CI/CD pipeline. If you get a permission issue of your agent job when executing the NSSM file, you can add the self-host agent user, <code>NetworkService</code>, to the Admin group in that server.</p>"},{"location":"services/azure/devops/devops-self-hosted/#setup-environments","title":"Setup Environments","text":"<p>We will set up environments for the CD pipelines by the port of application such as <code>8001</code> for development and <code>8000</code> for production because we want to partition that server for two environments.</p>"},{"location":"services/azure/fabric/","title":"Azure Fabric","text":""},{"location":"services/azure/fabric/#read-mores","title":"Read Mores","text":"<ul> <li> Building a scalable data ingestion framework for Microsoft Fabric</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/","title":"Connect to Azure Services","text":""},{"location":"services/azure/functions/az-func-to-az/#authentication","title":"Authentication","text":""},{"location":"services/azure/functions/az-func-to-az/#using-system-assigned-managed-identity","title":"Using System-Assigned Managed Identity","text":""},{"location":"services/azure/functions/az-func-to-az/#enable-system-assigned-managed-identity","title":"Enable System-Assigned Managed Identity","text":"<ul> <li>Go to Azure Function App  Select Identity  Click nav System Assigned</li> <li>On Status  Enable to On    Click Save</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#services","title":"Services","text":""},{"location":"services/azure/functions/az-func-to-az/#key-vault","title":"Key Vault","text":""},{"location":"services/azure/functions/az-func-to-az/#prerequisite","title":"Prerequisite","text":"<p>Add The Azure Function MSI User to the Azure Key Vault.</p> <ul> <li>Go to Azure Key Vault  Select Access policies  Click nav Create</li> <li>On Configure from a template  Select Secret Management  Click Next</li> <li>On Principle  Search the Azure Function name    Click Create</li> </ul> <p>Add Secret to Azure Function:</p> <ul> <li>Go to Azure Key Vault  Select Secrets  Click nav Generate/Import</li> <li>Create your Secrets  Copy the Secret Identifier URI</li> <li>Go to Azure Function App  Select Configuration  Click New application setting</li> <li>Pass the name to environment variable with this value:   <code>@Microsoft.KeyVault(SecretUri=&lt;secret-identifier-uri&gt;)</code></li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#connection-code","title":"Connection Code","text":"<pre><code>from azure.identity import ManagedIdentityCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredentials = ManagedIdentityCredential()\nsecret_client = SecretClient(\n    vault_url=\"https://&lt;key-vault-name&gt;.vault.azure.net\",\n    credential=credentials\n)\nsecret = secret_client.get_secret(\"secret-name\")\n</code></pre> <p>References</p> <ul> <li>Accessing Azure Key Vault from Python</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#service-bus","title":"Service Bus","text":""},{"location":"services/azure/functions/az-func-to-az/#connection-code_1","title":"Connection Code","text":"<pre><code>import os\nimport asyncio\nfrom aiohttp import ClientSession\nfrom azure.servicebus.aio import ServiceBusClient\n\nconn_str = os.environ['SERVICE_BUS_CONNECTION_STR']\ntopic_name = os.environ['SERVICE_BUS_TOPIC_NAME']\nsubscription_name = os.environ['SERVICE_BUS_SUBSCRIPTION_NAME']\n\nasync def watch(\n    topic_name,\n    subscription_name,\n):\n    async with ServiceBusClient.from_connection_string(conn_str=conn_str) as service_bus_client:\n        subscription_receiver = service_bus_client.get_subscription_receiver(\n            topic_name=topic_name,\n            subscription_name=subscription_name,\n        )\n    async with subscription_receiver:\n         message = await subscription_receiver.receive_messages(max_wait_time=1)\n\n    if message.body is not None:\n        async with ClientSession() as session:\n            await session.post('ip:port/endpoint',\n                               headers={'Content-type': 'application/x-www-form-urlencoded'},\n                               data={'data': message.body.decode()})\n\nasync def do():\n    while True:\n        for topic in ['topic1', 'topic2', 'topic3']:\n            await watch(topic, 'watcher')\n\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(do())\n</code></pre> <p>References</p> <ul> <li>https://stackoverflow.com/questions/63149310/azure-servicebus-using-async-await-in-python-seems-not-to-work</li> <li>https://iqan.medium.com/how-to-use-managed-identity-in-azure-functions-for-service-bus-trigger-fc61fb828b90</li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#synapse","title":"Synapse","text":""},{"location":"services/azure/functions/az-func-to-az/#prerequisite_1","title":"Prerequisite","text":"<p>Enable AAD integration for Azure Synapse workspace.</p> <ul> <li>Go to Azure Synapse Workspace  Select Azure Active Directory</li> <li>Click nav Set admin  Select your user    Click Save</li> </ul> <p>Add The Azure Function MSI User to the Azure Synapse SQL Pool.</p> <ul> <li>Connect to Azure Synapse SQL Pool on target database.</li> <li> <p>Create MSI user that use the Azure Function name</p> <pre><code>CREATE USER &lt;azure-function-name&gt; FROM EXTERNAL PROVIDER\nGO\n</code></pre> </li> </ul>"},{"location":"services/azure/functions/az-func-to-az/#connection-code_2","title":"Connection Code","text":"ActiveDirectoryMsiSQL User <pre><code>import logging\nimport pyodbc\n\nserver = 'tcp:&lt;server-name&gt;.database.windows.net'\ndatabase = '&lt;database-name&gt;'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nwith pyodbc.connect(\n    (\n        f\"Driver={driver};Server={server};PORT=1433;Database={database};\"\n        f\"Authentication=ActiveDirectoryMsi;\"\n    )\n) as conn:\n    logging.info(\"Successful connection to database\")\n    with conn.cursor() as cursor:\n        cursor.execute(\"SELECT &lt;column-name&gt; FROM &lt;table-name&gt;;\")\n        row = cursor.fetchone()\n        while row:\n            logging.info(str(row[0]).strip())\n            row = cursor.fetchone()\n</code></pre> <pre><code>import logging\nimport pyodbc\n\nserver = 'tcp:&lt;server-name&gt;.database.windows.net'\ndatabase = '&lt;database-name&gt;'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nusername = \"&lt;username&gt;\"\npassword = \"&lt;password&gt;\"\n\nwith pyodbc.connect(\n    (\n        f\"Driver={driver};Server={server};PORT=1433;Database={database};\"\n        f\"UID={username};PWD={password}\"\n    )\n) as conn:\n    logging.info(\"Successful connection to database\")\n    with conn.cursor() as cursor:\n        cursor.execute(\"SELECT &lt;column-name&gt; FROM &lt;table-name&gt;;\")\n        row = cursor.fetchone()\n        while row:\n            logging.info(str(row[0]).strip())\n            row = cursor.fetchone()\n</code></pre> <p>Note</p> <p>If the Python runtime has version more than <code>3.11</code>, it will upgrade ODBC driver to version 18.</p>"},{"location":"services/azure/functions/az-func-v2/","title":"Python V2","text":"<ul> <li>https://medium.com/@saurabh.dasgupta1/developing-and-deploying-a-python-azure-function-step-by-step-83c5066a2531</li> <li>https://towardsdatascience.com/tutorial-of-python-azure-functions-81949b1fd6db</li> </ul>"},{"location":"services/azure/functions/az-func-v2/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/functions/az-func-v2/#1-create-function-app","title":"1) Create Function App","text":"<ul> <li>On Azure Portal  Go to Function App    Create your function app that support Python with function version 2.0</li> </ul>"},{"location":"services/azure/functions/az-func-v2/#using-visual-studio-code","title":"Using Visual Studio Code","text":""},{"location":"services/azure/functions/az-func-v2/#prerequisite","title":"Prerequisite","text":"<ul> <li>Go to Visual Studio Code  On Nav Extensions    Install Azure Functions and Azurite extensions</li> </ul>"},{"location":"services/azure/functions/az-func-v2/#create-first-app","title":"Create First App","text":"<ul> <li>On Nav Azure  On WORKSPACE (Local) drop down    Click Azure Functions</li> </ul>"},{"location":"services/azure/functions/az-func-v2/#read-mores","title":"Read Mores","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python?pivots=python-mode-decorators</li> </ul>"},{"location":"services/azure/functions/az-func-with-docker/","title":"Dockerize","text":"<ul> <li>https://azureossd.github.io/2023/03/20/how-to-deploy-azure-functions-as-custom-container-using-azure-devops/</li> </ul>"},{"location":"services/azure/functions/introduction/","title":"Azure Function","text":"<p>Azure Functions is a serverless compute service offered by Microsoft Azure, designed to simplify the development of event-driven applications. With Azure Functions, developers can write and deploy code without managing the underlying infrastructure. This allows for a more efficient development process, where developers can focus on writing code that responds to events or triggers.</p>"},{"location":"services/azure/functions/introduction/#triggers-vs-bindings","title":"Triggers vs Bindings","text":"<p>Azure Functions are event-driven functions triggered by specific events, known as triggers. Each function can have only one trigger, which defines the event source that initiates the function\u2019s execution. When a trigger event occurs, Azure Functions automatically invokes the associated function. Here are some common types of triggers that it offers:</p> <ul> <li>HTTP Trigger: Initiates function execution in response to HTTP requests, acts as an endpoint for a REST API.</li> <li>Timer Trigger: Executes functions on a predefined schedule or at specific intervals.</li> <li>Blob Trigger: Triggers function execution when new data is added to Azure Blob Storage.</li> <li>Queue Trigger: Starts function execution when a new message is added to an Azure Queue Storage queue.</li> <li>Event Grid Trigger: Fires when an event is published to an Azure Event Grid topic or domain.</li> <li>Cosmos DB Trigger: Initiates function execution when documents are added or modified in Azure Cosmos DB.</li> </ul> <p>Bindings define the input and output connections between a function and external resources. They provide a declarative way to integrate functions with data sources and services without writing additional code for connectivity. Bindings abstract away the complexities of interacting with external resources, simplifying function development and enhancing productivity.</p> <p>Some frequently used bindings:</p> <ul> <li>Blob Storage Binding: Allows functions to read from and write to Azure Blob Storage.</li> <li>Queue Storage Binding: Enables functions to interact with Azure Queue Storage queues.</li> <li>Table Storage Binding: Facilitates interactions with Azure Table Storage tables.</li> <li>Cosmos DB Binding: Provides integration with Azure Cosmos DB, allowing functions to read from and write to databases.</li> <li>Service Bus Binding: Allows functions to send and receive messages from Azure Service Bus queues or topics.</li> <li>HTTP Binding: Enables functions to make HTTP requests to external resources.</li> </ul>"},{"location":"services/azure/functions/introduction/#how-costs-are-calculated","title":"How Costs are Calculated?","text":"<p>This pay-as-you-go approach contrasts with traditional hosting plans, eliminating the need for fixed monthly payments. It offers several hosting plans to suit different workloads and performance requirements:</p> <ul> <li>Consumption Plan: This is the default and most cost-effective option.   With the Consumption plan, you only pay for the resources consumed during the   execution of your functions.   It automatically scales out to handle incoming events and scales back down when   the workload decreases.</li> <li>Premium Plan: The Premium plan offers additional features and more predictable   performance compared to the Consumption plan.   It provides more advanced scaling options, larger instance sizes, and longer   execution timeouts.</li> <li>App Service Plan: Azure Functions can also run on an App Service plan, which   is shared with other Azure services like Web Apps and API Apps.   This plan offers more control over the underlying infrastructure and is suitable   for scenarios where you need more customization or have existing resources   in an App Service environment.</li> </ul>"},{"location":"services/azure/functions/introduction/#references","title":"References","text":"<ul> <li>Azure Functions 101: Getting Started with Serverless Computing\u26a1</li> </ul>"},{"location":"services/azure/sql/az-db-auth/","title":"Auth","text":""},{"location":"services/azure/sql/az-db-auth/#users-roles","title":"Users &amp; Roles","text":""},{"location":"services/azure/sql/az-db-auth/#getting-users","title":"Getting Users","text":"AllExternal UsersExternal Group UsersSQL UserSQL User without login <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] like 'E'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'X'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'S'\n</code></pre> <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\nWHERE [type] = 'S'\n</code></pre> <p>Note</p> <p>If you want to list of users on server, you can change information table to</p> <pre><code>...\nFROM [sys].[database_principals]\n...\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#create-user","title":"Create User","text":"External UserExternal GroupSQL UserSQL User without login <pre><code>USE [master];\nCREATE LOGIN [username@email.com] FROM EXTERNAL PROVIDER;\nGO\nUSE [database];\nCREATE USER [username@email.com] FROM LOGIN [username@email.com];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN [groupname@email.com] FROM EXTERNAL PROVIDER;\nGO\nUSE [database];\nCREATE USER [groupname@email.com] FROM LOGIN [groupname@email.com];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN [username@email.com] WITH PASSWORD = 'P@ssW0rd';\nGO\nUSE [database];\nCREATE USER [username@email.com] FROM LOGIN [username@email.com];\nGO\n</code></pre> <pre><code>USE [database];\nCREATE USER [username@email.com] WITHOUT LOGIN;\nGRANT IMPERSONATE ON USER::[username@email.com] TO [anothername@email.com];\nGO\n</code></pre> <p>Note</p> <p>If you want to delete user,</p> <pre><code>USE [database];\nDROP USER [username@email.com];\nGO\nUSE [master];\nDROP LOGIN [username@email.com];\nGO\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#relationship-of-users-and-roles","title":"Relationship of Users and Roles","text":"<pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , ISNULL(m.[name], 'No members')            AS [Member]\n    , m.create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nRIGHT OUTER JOIN [sys].[database_principals]    AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nLEFT OUTER JOIN [sys].[database_principals]     AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type] = 'R'\nORDER BY\n    r.[name]\n    , ISNULL(m.[name], 'No members')\n;\n</code></pre> <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|demo@mail.com   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#create-role","title":"Create Role","text":"<pre><code>CREATE ROLE [role-name];\nALTER ROLE [role-name] ADD MEMBER [username@email.com];\nGO\n</code></pre> <p>Note</p> <p>If you want to remove user from role, you should use</p> <pre><code>ALTER ROLE [role-name] DROP MEMBER [username@email.com];\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#permissions","title":"Permissions","text":""},{"location":"services/azure/sql/az-db-auth/#grant","title":"Grant","text":"AllOperationManage DatabaseManage LoginLoad DataSchemaCreator <pre><code>GRANT ALL PRIVILEGES ON DATABASE [database] TO [username@email.com];\n</code></pre> <pre><code>USE [master];\n\n-- Monitor the Appliance\nGRANT VIEW SERVER STATE TO [username@email.com];\n\n-- Terminate Connections\nGRANT ALTER ANY CONNECTION TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\n-- Manage Databases\nGRANT CONTROL ON DATABASE::[database] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [master]\n\n-- Manage and add logins\nGRANT ALTER ANY LOGIN TO [username@email.com];\n\n-- Grant permissions to view sessions and queries\nGRANT VIEW SERVER STATE TO [username@email.com];\n\n-- Grant permission to end sessions\nGRANT ALTER ANY CONNECTION TO [username@email.com];\nGO\n\nUSE [database];\n\n-- Grant permissions to create and drop users\nGRANT ALTER ANY USER TO [username@email.com];\n\n-- Grant permissions to create and drop roles\nGRANT ALTER ANY ROLE TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [master];\n\n-- Grant BULK Load permissions\nGRANT ADMINISTER BULK OPERATIONS TO [username@email.com];\n\nGO\n\nUSE [database];\n\nGRANT CREATE TABLE ON DATABASE::[database] TO [username@email.com];\n\n-- On Schema Usage\nGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA [schemaname] TO [username@email.com];\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA [schemaname] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\nALTER AUTHORIZATION ON SCHEMA::[schemaname] to [username@email.com];\nGRANT USAGE ON SCHEMA::[schemaname] TO [username@email.com];\nGRANT ALTER ON SCHEMA::[schemaname] TO [username@email.com];\n\nGO\n</code></pre> <pre><code>USE [database];\n\nGRANT CREATE TABLE, CREATE VIEW, CREATE PROCEDURE TO [username@email.com];\n\nGO\n</code></pre> <p>Note</p> <p>If you want to revoke granted permission, you can use:</p> <pre><code>REVOKE ...;\n</code></pre> <p>The REVOKE statement can be used to remove granted permissions, and the DENY statement can be used to prevent a principal from gaining a specific permission through a GRANT</p> <p>Example</p> <pre><code>USE Adventureworks\nDROP TABLE IF EXISTS dbo.mytable, dbo.mytable2, Sales.mytable, Sales.mytable2\nDROP USER IF EXISTS TestRole\n\nCREATE USER TestRole WITHOUT LOGIN\nGRANT CREATE TABLE to TestRole\n\nGRANT ALTER ON SCHEMA :: dbo To TestRole;\nEXECUTE AS USER = 'TestRole'\n--OK\nCREATE TABLE dbo.mytable(c1 int)\nGO\n--Fails\nCREATE TABLE Sales.mytable(c1 int)\nGO\nREVERT\nREVOKE ALTER ON SCHEMA :: dbo To TestRole;\n\nGRANT ALTER To TestRole;\nEXECUTE AS USER = 'TestRole'\n--OK\nCREATE TABLE dbo.mytable2(c1 int)\nGO\n--OK\nCREATE TABLE Sales.mytable2(c1 int)\nGO\nREVERT\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#impersonate","title":"Impersonate","text":"<pre><code>GRANT IMPERSONATE ON USER::[username@mail.com] TO [targetname@mail.com];\nGO\nEXECUTE AS USER = 'username@mail.com';\n...\nREVERT;\nGO\n</code></pre> <p>Note</p> <p>If you want to revoke impersonate, you can use:</p> <pre><code>REVOKE IMPERSONATE ON USER::[&lt;user@mail.com&gt;] TO [&lt;target@mail.com&gt;];\n</code></pre>"},{"location":"services/azure/sql/az-db-auth/#relationship-of-permissions-and-objects","title":"Relationship of Permissions and Objects","text":"<pre><code>SELECT\n    dp.[name]                                   AS [Principle]\n    , dp.[type_desc]                            AS [Principal Type]\n    , o.[name]                                  AS [Object Name]\n    , p.[permission_name]                       AS [Permission]\n    , p.[state_desc]                            AS [Permission State]\nFROM [sys].[database_permissions]               AS p\nLEFT OUTER JOIN [sys].[all_objects]             AS o\n    ON p.[major_id] = o.[OBJECT_ID]\nINNER JOIN [sys].[database_principals]          AS dp\n    ON p.[grantee_principal_id] = dp.[principal_id]\n</code></pre> <pre><code>Principle        |Principal Type|Object Name              |Permission|Permission State|\n-----------------+--------------+-------------------------+----------+----------------+\ndbo              |SQL_USER      |                         |CONNECT   |GRANT           |\nDWHCTRLADMIN     |SQL_USER      |                         |CONNECT   |GRANT           |\nusername@scg.com |EXTERNAL_USER |                         |CONNECT   |GRANT           |\npublic           |DATABASE_ROLE |query_store_query_variant|SELECT    |GRANT           |\n</code></pre> On Schema <pre><code>SELECT\n    state_desc\n    ,permission_name\n    ,'ON'\n    ,class_desc\n    ,SCHEMA_NAME(major_id)\n    ,'TO'\n    ,USER_NAME(grantee_principal_id)\nFROM [sys].[database_permissions]       AS PERM\nJOIN [sys].[database_principals]        AS Prin\n    ON PERM.[major_ID] = Prin.[principal_id]\n    AND class_desc = 'SCHEMA'\nWHERE\n    user_name(grantee_principal_id) = 'username@email.com'\n</code></pre> <pre><code>state_desc|permission_name|  |class_desc|             |  |                  |\n----------+---------------+--+----------+-------------+--+------------------+\nGRANT     |EXECUTE        |ON|SCHEMA    |             |TO|public            |\nGRANT     |SELECT         |ON|SCHEMA    |             |TO|public            |\nGRANT     |ALTER          |ON|SCHEMA    |DWHCURATED   |TO|username@email.com|\nGRANT     |EXECUTE        |ON|SCHEMA    |DWHMDL       |TO|username@email.com|\nGRANT     |EXECUTE        |ON|SCHEMA    |DWHMDL       |TO|de_vendor         |\n</code></pre> <p>Note</p> <p>For more detail, you can follew store procedure statement, sp_dbpermissions</p>"},{"location":"services/azure/sql/az-db-auth/#read-mores","title":"Read Mores","text":"<ul> <li> SQL Server Create User and Grant Permission</li> </ul>"},{"location":"services/azure/sql/az-db-monitoring/","title":"Monitoring","text":""},{"location":"services/azure/sql/az-db-monitoring/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/sql/az-db-monitoring/#partition-state","title":"Partition State","text":"<pre><code>SELECT\n    OBJECT_NAME(s.object_id)        AS [object]\n    ,i.[name]                       AS IndexName\n    ,SUM(s.[used_page_count]) * 8   AS IndexSizeKB\nFROM [sys].[dm_db_partition_stats]  AS s\nINNER JOIN [sys].[indexes]          AS i\n    ON s.[object_id] = i.[object_id]\n    AND s.[index_id] = i.[index_id]\nGROUP BY\n    s.[object_id], i.[name]\nORDER BY\n    OBJECT_NAME(s.object_id), i.[name]\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/","title":"Auth","text":""},{"location":"services/azure/synapse/asa-auth/#users-roles","title":"Users &amp; Roles","text":""},{"location":"services/azure/synapse/asa-auth/#getting-users","title":"Getting Users","text":"All AD User AD Group SQL User SQL User without Login <pre><code>SELECT\n    [name]\n    , [type_desc]\n    , [type]\nFROM [sys].[database_principals]\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'E'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'X'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'S'\n    AND [name] = 'username@mail.com'\n;\n</code></pre> <pre><code>SELECT\n    [name]\nFROM [sys].[database_principals]\nWHERE\n    [type] = 'S'\n    AND [name] = 'username@mail.com'\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#create-user","title":"Create User","text":"AD User AD Group SQL User SQL User without Login <pre><code>USE [&lt;database-name&gt;];\nCREATE USER [username@mail.com] FROM EXTERNAL PROVIDER\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER [group-name] FROM LOGIN [group-name];\nGO\n</code></pre> <pre><code>USE [master];\nCREATE LOGIN &lt;username&gt; WITH PASSWORD = 'P@ssW0rd'\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER &lt;username&gt; FOR LOGIN &lt;username&gt;;\nGO\n</code></pre> <pre><code>USE [&lt;database-name&gt;];\nCREATE USER &lt;username&gt; WITHOUT LOGIN;\nGRANT IMPERSONATE ON USER::&lt;username&gt; TO [&lt;another-username&gt;];\nGO\n</code></pre> <pre><code>EXECUTE AS USER = '&lt;username&gt;';\nGO\n...\nREVERT;\nGO\n</code></pre> <p>Warning</p> <p>Azure Synapse Serverless SQL Pool does not support for create this user type.</p> <p>Getting User Example:</p> <p>Note</p> <p>If you want to drop user, you would use:</p> <pre><code>DROP USER [username@mail.com]\nGO\n</code></pre> <p>If this user have login, you would use:</p> <pre><code>DROP LOGIN [username@mail.com]\nGO\n</code></pre> <p>Generate drop statement with multi-users:</p> <pre><code>SELECT CONCAT('DROP USER [', [name], '];')  AS remove_user\nFROM\n    [sys].[database_principals]\nWHERE\n    [type] = 'E'\n    AND LOWER([name]) IN ('username@mail.com', ...)\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#getting-relations","title":"Getting Relations","text":"List relate roles and users List all roles <pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , m.[name]                                  AS [Member]\n    , m.Create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nJOIN [sys].[database_principals]                AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nJOIN [sys].[database_principals]                AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type_desc] = 'DATABASE_ROLE'\nORDER BY\n    r.[name], m.[name];\n;\n</code></pre> <pre><code>SELECT\n    r.[name]                                    AS [Role]\n    , ISNULL(m.[name], 'No members')            AS [Member]\n    , m.create_date                             AS [Created Date]\n    , m.modify_Date                             AS [Modified Date]\nFROM\n    [sys].[database_role_members]               AS rm\nRIGHT OUTER JOIN [sys].[database_principals]    AS r\n    ON rm.[role_principal_id] = r.[principal_id]\nLEFT OUTER JOIN [sys].[database_principals]     AS m\n    ON rm.[member_principal_id] = m.[principal_id]\nWHERE\n    r.[type] = 'R'\nORDER BY\n    r.[name], ISNULL(m.[name], 'No members')\n;\n</code></pre> <p>Example Results:</p>  List relate roles and users List all roles <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|demo@mail.com   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre> <pre><code>Role         |Member          |Created Date           |Modified Date          |\n-------------+----------------+-----------------------+-----------------------+\nDATA ENGINEER|No members      |NULL                   |NULL                   |\ndb_owner     |dbo             |2003-04-08 00:00:00.000|2021-09-21 00:00:00.000|\ndb_owner     |admin@mail.com  |2021-05-12 00:00:00.000|2021-05-12 00:00:00.000|\ndb_ddladmin  |DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datareader|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\ndb_datawriter|DATA ENGINEER   |2022-11-15 00:00:00.000|2022-11-15 00:00:00.000|\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#roles","title":"Roles","text":"Dedicate SQL Pool Serverless SQL Pool <pre><code>CREATE ROLE [rolename]\nEXEC sp_addrolemember 'rolename', [username@mail.com]\nGO\n</code></pre> <pre><code>CREATE ROLE [rolename];\nALTER ROLE [rolename] ADD MEMBER [username@mail.com];\nGO\n</code></pre> <p>Note</p> <p>If you want to remove user from this role,</p>  Dedicate SQL Pool Serverless SQL Pool <pre><code>EXEC sp_droprolemember '&lt;role-name&gt;', 'username@mail.com';\n</code></pre> <pre><code>ALTER ROLE [role-name] DROP MEMBER [username@mail.com];\n</code></pre> <p>Generate a drop systax from query,</p>  Dedicate SQL Pool <pre><code>SELECT\n  CONCAT(\n        'EXEC sp_droprolemember ''', r.name, ''', ''', m.name, ''';'\n  ) AS remove_member_command\nFROM\n    sys.database_role_members  AS rm\nRIGHT OUTER JOIN sys.database_principals AS r\n    ON rm.role_principal_id = r.principal_id\nLEFT OUTER JOIN sys.database_principals AS m\n    ON rm.member_principal_id = m.principal_id\nWHERE\n    r.type = 'R'\n    AND LOWER(m.name) IN (\n        'userbane@mail.com',\n        ...\n    )\nORDER BY\n    r.name, ISNULL(m.name, 'No members')\n;\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#permissions","title":"Permissions","text":""},{"location":"services/azure/synapse/asa-auth/#getting-relations_1","title":"Getting Relations","text":"List relate permissions List relate permissions with grantor <pre><code>SELECT DISTINCT\n     pr.principal_id                AS [ID],\n     pr.[name]                      AS [User],\n     pr.[type_desc]                 AS [Type],\n     pr.authentication_type_desc    AS [Auth_Type]\n     pe.state_desc                  AS [State]\n     pe.permission_name             AS [Permission]\n     pe.class_desc                  AS [Class]\n     coalesce(o.[name], sch.name)   AS [Object]\nFROM\n    sys.database_principals         AS pr\nJOIN sys.database_permissions       AS pe\n    ON pe.grantee_principal_id = pr.principal_id\nLEFT JOIN sys.objects               AS o\n    ON o.object_id = pe.major_id\nLEFT JOIN sys.schemas               AS sch\n    ON sch.schema_id = pe.major_id\n    AND class_desc = 'SCHEMA'\n;\n</code></pre> <pre><code>SELECT DISTINCT\n    DB_NAME()                           AS [DB],\n    p.[name]                            AS [User],\n    p.[type_desc]                       AS [Type],\n    p2.[name]                           AS [Grantor],\n    pe.[state_desc]                     AS [State],\n    pe.[permission_name]                AS [Permission],\n    o.[Name]                            AS [Object],\n    o.[type_desc]                       AS [Object Type]\nFROM [sys].[database_permissions]       AS pe\nLEFT JOIN [sys].[objects]               AS o\n    ON pe.[major_id] = o.[object_id]\nLEFT JOIN [sys].[database_principals]   AS p\n    ON pe.[grantee_principal_id] = p.[principal_id]\nLEFT JOIN [sys].[database_principals]   AS p2\n    ON pe.[grantor_principal_id] = p2.[principal_id]\n;\n</code></pre> <p>Example Results:</p>  List relate permissions List relate permissions with grantor <pre><code>ID|User              |Type          |Auth Type|State|Permission             |Class             |Object         |\n--+------------------+--------------+---------+-----+-----------------------+------------------+---------------+\n25|username@mail.com |EXTERNAL_USER |EXTERNAL |GRANT|CONNECT                |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |DATAEXTERNAL   |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |MART           |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER                  |SCHEMA            |CURATED        |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER ANY DATA SOURCE  |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|ALTER ANY FILE FORMAT  |DATABASE          |               |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|EXECUTE                |OBJECT_OR_COLUMN  |FUNC_CHK_ID    |\n26|de_external       |DATABASE_ROLE |NONE     |GRANT|EXECUTE                |OBJECT_OR_COLUMN  |FUNC_CHK_TITLE |\n</code></pre> <pre><code>DB      |User       |Type          |Grantor               |State|Permission                    |Object          |Object Type         |\n--------+-----------+--------------+----------------------+-----+------------------------------+----------------+--------------------+\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER                         |                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER ANY EXTERNAL DATA SOURCE|                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|ALTER ANY EXTERNAL FILE FORMAT|                |                    |\nsyndpdev|adfuser    |SQL_USER      |dbo                   |GRANT|CONNECT                       |                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |dbo                   |GRANT|ALTER ANY EXTERNAL FILE FORMAT|                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |dbo                   |GRANT|EXECUTE                       |                |                    |\nsyndpdev|de_vendor  |DATABASE_ROLE |user@email.com        |GRANT|EXECUTE                       |FUNC_CHK_ID     |SQL_SCALAR_FUNCTION |\nsyndpdev|de_vendor  |DATABASE_ROLE |user@email.com        |GRANT|EXECUTE                       |FUNC_CHK_TITLE  |SQL_SCALAR_FUNCTION |\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#grant","title":"Grant","text":"Data ExecutionSpecific Table <pre><code>GRANT CREATE VIEW TO [role-name];\nGRANT CREATE PROCEDURE TO [role-name];\nGRANT ALTER TO [role-name];\n</code></pre> <pre><code>GRANT EXECUTE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT UPDATE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT INSERT ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT DELETE ON SCHEMA::&lt;schema-name&gt; TO [role-name];\nGRANT ALTER ON SCHEMA::&lt;schema-name&gt; TO [role-name];\n</code></pre> <pre><code>GRANT SELECT ON OBJECT::&lt;schema-name&gt;.&lt;object-name&gt; TO [role-name]\n</code></pre> <pre><code>GRANT IMPERSONATE ON USER::&lt;user-name&gt; TO &lt;user-name&gt;;\n</code></pre> <p>Warning</p> <p>Impersonate can not use on the Azure Synapse Serverless SQL Pool.</p> <p>More Example for Grant Permissions</p>"},{"location":"services/azure/synapse/asa-auth/#workload","title":"Workload","text":"<pre><code>CREATE WORKLOAD GROUP &lt;group-name&gt;\nWITH (\n    MIN_PERCENTAGE_RESOURCE = 100,\n    CAP_PERCENTAGE_RESOURCE = 100,\n    REQUEST_MIN_RESOURCE_GRANT_PERCENT = 100\n);\n\n-- Classifies load_user with the workload group LoadData\nCREATE WORKLOAD CLASSIFIER [&lt;classifier-name&gt;]\nWITH (\n    WORKLOAD_GROUP = '&lt;group-name&gt;',\n    MEMBERNAME = '&lt;username&gt;'\n);\n</code></pre>"},{"location":"services/azure/synapse/asa-auth/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft Azure Synapse Analytics SQL Authentication</li> <li> Microsoft SQL T-SQL Create User Transaction</li> </ul>"},{"location":"services/azure/synapse/asa-date-timezone/","title":"Date &amp; Timezone","text":""},{"location":"services/azure/synapse/asa-date-timezone/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-date-timezone/#convert-timezone","title":"Convert Timezone","text":"<pre><code>SELECT\n    CAST(\n        CAST(\n            [DateColumn] AS DATETIMEOFFSET\n        ) AT TIME ZONE 'SE Asia Standard Time' AS DATETIME2\n    ),\n    ...\nFROM ...\n</code></pre>"},{"location":"services/azure/synapse/asa-date-timezone/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft: Data Types - Datetimeoffset</li> </ul>"},{"location":"services/azure/synapse/asa-external-data-source/","title":"External Data Source","text":""},{"location":"services/azure/synapse/asa-external-data-source/#database-scope-credential","title":"Database Scope Credential","text":"<p>A Database Credential is not mapped to a server login or database user. The credential is used by the database to access to the external location anytime the database is performing an operation that requires access.</p>"},{"location":"services/azure/synapse/asa-external-data-source/#list-credentials","title":"List Credentials","text":"<pre><code>SELECT * FROM [sys].[database_scoped_credentials];\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-master-key","title":"Create Master Key","text":"<p>Create a new Master Key, <code>ENCRYPTION</code> to encrypt the credentials for the external data source.</p> <pre><code>-- Optional: Create MASTER KEY if not exists in database:\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'P@ssW0rd'\nGO\n</code></pre> <p>If the master key already exists on the database, you can use:</p> <pre><code>OPEN MASTER KEY DECRYPTION BY PASSWORD = 'P@ssW0rd';\n...\nCLOSE MASTER KEY;\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-credential","title":"Create Credential","text":"Managed IdentityService PrincipleShared Access Signature <pre><code>CREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = 'Managed Identity'\nGO\n</code></pre> <pre><code>-- authority-url: `https://login.microsoftonline.com/&lt;tenant-id&gt;/oauth2/token`\nCREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = '&lt;client-id&gt;@&lt;authority-url&gt;',\n    SECRET = '&lt;client-secret&gt;'\nGO\n</code></pre> <pre><code>-- The secret value must remove the leading '?'\nCREATE DATABASE SCOPED CREDENTIAL &lt;credential-name&gt;\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\n    SECRET = 'sv=2018-03-28&amp;ss=bfqt&amp;...&amp;sig=lQHczN...'\nGO\n</code></pre> <p>And the permission of User, Managed Identity, or Service Principle that want to access data on target external data source should be any role in <code>Storage Blob Data Owner/Contributor/Reader</code> roles in order for the application to access the data via RBAC in Azure Portal.</p>"},{"location":"services/azure/synapse/asa-external-data-source/#external-data-source_1","title":"External Data Source","text":""},{"location":"services/azure/synapse/asa-external-data-source/#list-data-source","title":"List Data Source","text":"<pre><code>SELECT * FROM [sys].[external_data_sources];\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-data-source","title":"Create Data Source","text":"Dedicate SQL Pool Serverless SQL Pool <pre><code>CREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH(\n    LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net',\n    CREDENTIAL = &lt;credential-name&gt;,\n    PUSHDOWN = ON,\n    TYPE = HADOOP\n);\n</code></pre> <p>Note</p> <p>PolyBase data virtualization is used when the <code>EXTERNAL DATA SOURCE</code> is created with <code>TYPE=HADOOP</code>.</p> <p><code>PUSHDOWN = ON | OFF</code> is set to <code>ON</code> by default, meaning the ODBC Driver can leverage server-side processing for complex queries.</p> <pre><code>CREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH(\n    LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net',\n    CREDENTIAL = &lt;credential-name&gt;\n);\n</code></pre> <p>For the <code>LOCATION</code>, it provide the connectivity protocol and path to the external data source. See More Supported Protocol</p> <p>Note</p> <p>If you want to use Azure AD for access an external data source you can use:</p> <pre><code>-- The Permission from this solution will up to user that want to access\n-- target external data source.\nCREATE EXTERNAL DATA SOURCE [&lt;external-data-source&gt;]\nWITH (\n    LOCATION  = 'https://&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/&lt;path&gt;'\n)\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#external-file-format","title":"External File Format","text":""},{"location":"services/azure/synapse/asa-external-data-source/#list-file-format","title":"List File Format","text":"<pre><code>SELECT * FROM [sys].[external_file_formats]\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#create-file-format","title":"Create File Format","text":"<pre><code>CREATE EXTERNAL FILE FORMAT &lt;parquet_snappy&gt;\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n</code></pre> CSVJSONDELTA <pre><code>CREATE EXTERNAL FILE FORMAT &lt;skip_header_csv&gt;\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS(\n        FIELD_TERMINATOR    = ',',\n        STRING_DELIMITER    = '\"',\n        FIRST_ROW           = 2,\n        USE_TYPE_DEFAULT    = True\n    )\n);\n</code></pre> <pre><code>CREATE EXTERNAL FILE FORMAT &lt;json-format&gt;\nWITH (\n    FORMAT_TYPE = JSON,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n);\n</code></pre> <pre><code>CREATE EXTERNAL FILE FORMAT &lt;delta-format&gt;\nWITH (\n    FORMAT_TYPE = DELTA\n);\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#examples","title":"Examples","text":"Serverless SQL Pool <pre><code>CREATE EXTERNAL TABLE [&lt;schema-name&gt;].[&lt;external-table-name&gt;]\n(\n    [PurposeId] [varchar](max),\n    [RetireOnDate] [datetime],\n    [CreatedDate] [datetime]\n)\nWITH (\n    DATA_SOURCE = [&lt;data-source-name&gt;],\n    FILE_FORMAT = [&lt;external-file-format&gt;],\n    LOCATION = N'/path/of/data/date=20240708'\n)\nGO\n</code></pre> <p>Example</p> <pre><code>USE [master];\nCREATE LOGIN [username] WITH PASSWORD = 'P@ssW0rd';\nGO\n\nUSE [database];\nCREATE USER [username] FROM LOGIN [username];\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[credential-name] TO [username];\nGO\n</code></pre> <pre><code>IF NOT EXISTS (\n    SELECT *\n    FROM [sys].[external_data_sources]\n    WHERE [name] = '&lt;external-data-source-name&gt;'\n)\n    CREATE EXTERNAL DATA SOURCE &lt;external-data-source-name&gt;\n    WITH (\n        CREDENTIAL = &lt;credential-name&gt;,\n        LOCATION = 'abfss://&lt;container&gt;@&lt;storage-account&gt;.dfs.core.windows.net'\n    )\nGO\n</code></pre> <pre><code>CREATE OR ALTER VIEW [CURATED].[&lt;view-name&gt;]\nAS\n    SELECT *\n    FROM OPENROWSET(\n        BULK '/delta_silver/&lt;delta-table-name&gt;',\n        DATA_SOURCE = '&lt;external-data-source-name&gt;',\n        FORMAT = 'DELTA'\n) AS [r]\nGO\n\nGRANT SELECT ON OBJECT::[CURATED].[&lt;view-name&gt;] TO &lt;user-name&gt;\nGO\n</code></pre>"},{"location":"services/azure/synapse/asa-external-data-source/#read-mores","title":"Read Mores","text":"<ul> <li> Microsoft: Develop Storage Files Access Control</li> <li> Microsoft: TSQL - Create External Data Source</li> <li> Microsoft: TSQL - Create External File Format</li> <li> Medium: Query Azure Data Lake via Synapse Serverless Security</li> </ul>"},{"location":"services/azure/synapse/asa-low-level-security/","title":"Low-Level Security","text":""},{"location":"services/azure/synapse/asa-low-level-security/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-low-level-security/#read-mores","title":"Read Mores","text":"<ul> <li>SQLShack: Implementing Row Level Security</li> </ul>"},{"location":"services/azure/synapse/asa-monitoring/","title":"Monitoring","text":""},{"location":"services/azure/synapse/asa-monitoring/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-monitoring/#table-size","title":"Table Size","text":"<pre><code>CREATE VIEW dbo.vTableSizes\nAS\nWITH base\nAS\n(\nSELECT\n GETDATE()                                                             AS  [execution_time]\n, DB_NAME()                                                            AS  [database_name]\n, s.name                                                               AS  [schema_name]\n, t.name                                                               AS  [table_name]\n, QUOTENAME(s.name)+'.'+QUOTENAME(t.name)                              AS  [two_part_name]\n, nt.[name]                                                            AS  [node_table_name]\n, ROW_NUMBER() OVER(PARTITION BY nt.[name] ORDER BY (SELECT NULL))     AS  [node_table_name_seq]\n, tp.[distribution_policy_desc]                                        AS  [distribution_policy_name]\n, c.[name]                                                             AS  [distribution_column]\n, nt.[distribution_id]                                                 AS  [distribution_id]\n, i.[type]                                                             AS  [index_type]\n, i.[type_desc]                                                        AS  [index_type_desc]\n, nt.[pdw_node_id]                                                     AS  [pdw_node_id]\n, pn.[type]                                                            AS  [pdw_node_type]\n, pn.[name]                                                            AS  [pdw_node_name]\n, di.name                                                              AS  [dist_name]\n, di.position                                                          AS  [dist_position]\n, nps.[partition_number]                                               AS  [partition_nmbr]\n, nps.[reserved_page_count]                                            AS  [reserved_space_page_count]\n, nps.[reserved_page_count] - nps.[used_page_count]                    AS  [unused_space_page_count]\n, nps.[in_row_data_page_count]\n    + nps.[row_overflow_used_page_count]\n    + nps.[lob_used_page_count]                                        AS  [data_space_page_count]\n, nps.[reserved_page_count]\n - (nps.[reserved_page_count] - nps.[used_page_count])\n - ([in_row_data_page_count]\n         + [row_overflow_used_page_count]+[lob_used_page_count])       AS  [index_space_page_count]\n, nps.[row_count]                                                      AS  [row_count]\nfrom\n    sys.schemas s\nINNER JOIN sys.tables t\n    ON s.[schema_id] = t.[schema_id]\nINNER JOIN sys.indexes i\n    ON  t.[object_id] = i.[object_id]\n    AND i.[index_id] &lt;= 1\nINNER JOIN sys.pdw_table_distribution_properties tp\n    ON t.[object_id] = tp.[object_id]\nINNER JOIN sys.pdw_table_mappings tm\n    ON t.[object_id] = tm.[object_id]\nINNER JOIN sys.pdw_nodes_tables nt\n    ON tm.[physical_name] = nt.[name]\nINNER JOIN sys.dm_pdw_nodes pn\n    ON  nt.[pdw_node_id] = pn.[pdw_node_id]\nINNER JOIN sys.pdw_distributions di\n    ON  nt.[distribution_id] = di.[distribution_id]\nINNER JOIN sys.dm_pdw_nodes_db_partition_stats nps\n    ON nt.[object_id] = nps.[object_id]\n    AND nt.[pdw_node_id] = nps.[pdw_node_id]\n    AND nt.[distribution_id] = nps.[distribution_id]\n    AND i.[index_id] = nps.[index_id]\nLEFT OUTER JOIN (select * from sys.pdw_column_distribution_properties where distribution_ordinal = 1) cdp\n    ON t.[object_id] = cdp.[object_id]\nLEFT OUTER JOIN sys.columns c\n    ON cdp.[object_id] = c.[object_id]\n    AND cdp.[column_id] = c.[column_id]\nWHERE pn.[type] = 'COMPUTE'\n)\n, size\nAS\n(\nSELECT\n   [execution_time]\n,  [database_name]\n,  [schema_name]\n,  [table_name]\n,  [two_part_name]\n,  [node_table_name]\n,  [node_table_name_seq]\n,  [distribution_policy_name]\n,  [distribution_column]\n,  [distribution_id]\n,  [index_type]\n,  [index_type_desc]\n,  [pdw_node_id]\n,  [pdw_node_type]\n,  [pdw_node_name]\n,  [dist_name]\n,  [dist_position]\n,  [partition_nmbr]\n,  [reserved_space_page_count]\n,  [unused_space_page_count]\n,  [data_space_page_count]\n,  [index_space_page_count]\n,  [row_count]\n,  ([reserved_space_page_count] * 8.0)                                 AS [reserved_space_KB]\n,  ([reserved_space_page_count] * 8.0)/1000                            AS [reserved_space_MB]\n,  ([reserved_space_page_count] * 8.0)/1000000                         AS [reserved_space_GB]\n,  ([reserved_space_page_count] * 8.0)/1000000000                      AS [reserved_space_TB]\n,  ([unused_space_page_count]   * 8.0)                                 AS [unused_space_KB]\n,  ([unused_space_page_count]   * 8.0)/1000                            AS [unused_space_MB]\n,  ([unused_space_page_count]   * 8.0)/1000000                         AS [unused_space_GB]\n,  ([unused_space_page_count]   * 8.0)/1000000000                      AS [unused_space_TB]\n,  ([data_space_page_count]     * 8.0)                                 AS [data_space_KB]\n,  ([data_space_page_count]     * 8.0)/1000                            AS [data_space_MB]\n,  ([data_space_page_count]     * 8.0)/1000000                         AS [data_space_GB]\n,  ([data_space_page_count]     * 8.0)/1000000000                      AS [data_space_TB]\n,  ([index_space_page_count]  * 8.0)                                   AS [index_space_KB]\n,  ([index_space_page_count]  * 8.0)/1000                              AS [index_space_MB]\n,  ([index_space_page_count]  * 8.0)/1000000                           AS [index_space_GB]\n,  ([index_space_page_count]  * 8.0)/1000000000                        AS [index_space_TB]\nFROM base\n)\nSELECT *\nFROM size\n;\n</code></pre> <p>Design tables using dedicated SQL pool in Azure Synapse Analytics</p>"},{"location":"services/azure/synapse/asa-monitoring/#operation-status","title":"Operation Status","text":"<pre><code>-- This query returns the latest operations in the server\n-- it needs to be executed in the master database\n-- the information in this table is removed automatically after 2 or 3 hours\nSELECT [session_activity_id]\n      ,[resource_type]\n      ,[resource_type_desc]\n      ,[major_resource_id]\n      ,[minor_resource_id]\n      ,[operation]\n      ,[state]\n      ,[state_desc]\n      ,[percent_complete]\n      ,[error_code]\n      ,[error_desc]\n      ,[error_severity]\n      ,[error_state]\n      ,[start_time]\n      ,[last_modify_time]\n    FROM sys.dm_operation_status\n</code></pre>"},{"location":"services/azure/synapse/asa-monitoring/#data-skew-outdated-state","title":"Data Skew &amp; Outdated State","text":"<pre><code>-- data skew -&gt; cmp_rows&gt;1mil, skew &gt;= 10%\n-- missing stats -&gt; cmp_rows&gt;1mil, ctl_rows=1000\n-- outdated stats -&gt; cmp_rows&gt;1mil, cmp_rows &lt;&gt; ctl_rows (for (cmp_rows-ctl_rows) &gt; 20%)\n\nDECLARE @minRows INT = 1000000;\nDECLARE @minSkewPercent decimal=10.0;\nDECLARE @missingStatCtlRowCount int=1000;\nDECLARE @CtlCmpRowDifferencePercentageForOutdatedStats decimal=20.0;\n\nWITH cmp_details AS\n(\n       select tm.object_id, ps.index_id, ps.distribution_id, count(ps.partition_number) [partitions], sum(ps.row_count) cmp_row_count\n       from sys.dm_pdw_nodes_db_partition_stats ps\n              join sys.pdw_nodes_tables nt on nt.object_id=ps.object_id and ps.distribution_id=nt.distribution_id\n              join sys.pdw_table_mappings tm on tm.physical_name=nt.name\n       where ps.index_id&lt;2\n       group by tm.object_id, ps.index_id, ps.distribution_id\n)\n, cmp_summary as\n(\n       select object_id, index_id, sum(cmp_row_count) cmp_row_count\n              , (max(cmp_row_count)-min(cmp_row_count)) highest_skew_rows_difference\n              , convert(decimal(10,2),((max(cmp_row_count) - min(cmp_row_count))*100.0 / nullif(sum(cmp_row_count),0))) skew_percent\n       from cmp_details\n       group by object_id, index_id\n)\n, ctl_summary as\n(\n       select t.object_id, i.index_id, s.name sch_name, t.name table_name, i.type_desc table_type, dp.distribution_policy_desc distribution_type, count(p.partition_number) [partitions], sum(p.rows) ctl_row_count\n       from sys.schemas s\n              join sys.tables t on t.schema_id=s.schema_id\n              join sys.pdw_table_distribution_properties dp on dp.object_id=t.object_id\n              join sys.indexes i on i.object_id=t.object_id and i.index_id&lt;2\n              join sys.partitions p on p.object_id=t.object_id and p.index_id=i.index_id\n       group by t.object_id, i.index_id, s.name, t.name, i.type_desc, dp.distribution_policy_desc\n)\n, [all_results] as\n(\n       select ctl.object_id, ctl.index_id, ctl.sch_name, ctl.table_name, ctl.table_type, ctl.distribution_type, ctl.[partitions]\n              , ctl.ctl_row_count, cmp.cmp_row_count, convert(decimal(10,2),(abs(ctl.ctl_row_count - cmp.cmp_row_count)*100.0 / nullif(cmp.cmp_row_count,0))) ctl_cmp_difference_percent\n              , cmp.highest_skew_rows_difference, cmp.skew_percent\n              , case\n                     when (ctl.ctl_row_count = @missingStatCtlRowCount) then 'missing stats'\n                     when ((ctl.ctl_row_count &lt;&gt; cmp.cmp_row_count) and ((abs(ctl.ctl_row_count - cmp.cmp_row_count)*100.0 / nullif(cmp.cmp_row_count,0)) &gt; @CtlCmpRowDifferencePercentageForOutdatedStats)) then 'outdated stats'\n                     else null\n                end stat_info\n              , case when (cmp.skew_percent &gt;= @minSkewPercent) then 'data skew' else null end skew_info\n       from ctl_summary ctl\n              join cmp_summary cmp on ctl.object_id=cmp.object_id and ctl.index_id=cmp.index_id\n)\nselect *\nfrom [all_results]\nwhere cmp_row_count&gt;@minRows and (stat_info is not null or skew_info is not null)\norder by sch_name, table_name\n</code></pre>"},{"location":"services/azure/synapse/asa-monitoring/#read-mores","title":"Read Mores","text":"<ul> <li> Azure Synapse Last Operations in Server</li> <li> GitHub: ProdataSQL - SynapseTools</li> <li> GitHub: ProdataSQL - SynapseTools vTableStats</li> </ul>"},{"location":"services/azure/synapse/asa-partition-view/","title":"Partition View","text":"<p>Warning</p> <p>The <code>OPENROWSET</code> syntax can use Azure Synapse Serverless SQL Pool only</p>"},{"location":"services/azure/synapse/asa-partition-view/#getting-started","title":"Getting Started","text":""},{"location":"services/azure/synapse/asa-partition-view/#manage-permission","title":"Manage Permission","text":"<ol> <li> <p>Remove bulk operations on master</p> <pre><code>USE [master];\nDENY ADMINISTER BULK OPERATIONS TO [username];\nDENY ADMINISTER BULK OPERATIONS TO [public];\nGO\n</code></pre> </li> <li> <p>Grant bulk operations on the database level</p> <pre><code>USE [database];\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [username];\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [public];\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[&lt;credential-name&gt;] TO [&lt;user-name&gt;];\nGO\n</code></pre> </li> </ol>"},{"location":"services/azure/synapse/asa-partition-view/#partition-pruning","title":"Partition Pruning","text":"Parquet Delta CSV Json <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *,\n    CAST(temp.filepath(1) AS INT) AS [year],\n    CAST(temp.filepath(2) AS TINYINT) AS [month],\n    CAST(temp.filepath(3) AS TINYINT) AS [day]\nFROM\n    OPENROWSET(\n        BULK 'data/table/year=*/month=*/day=*/**',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'PARQUET'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'data/delta_table/',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'DELTA'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'data/delta_table/',\n        DATA_SOURCE = '&lt;external-storage-name&gt;',\n        FORMAT = 'CSV',\n        PARSER_VERSION ='2.0',\n        FIELDTERMINATOR =',',\n        ROWTERMINATOR = '\\n',\n        HEADER_ROW = TRUE,\n        FIRSTROW = 1,\n        FIELDQUOTE = '\"',\n        ESCAPECHAR = '\\\\',\n        ROWSET_OPTIONS = '{\"READ_OPTIONS\":[\"ALLOW_INCONSISTENT_READS\"]}'\n    )\nWITH (\n    [timestamp]       [datetime],\n    [edge_id]         [varchar](max),\n    [temperature]     [float],\n    [humidity]        [float],\n    [lqi]             [float],\n    [pm1.0]           [float],\n    [pm2.5]           [float],\n    [pm10.0]          [float],\n    [date]            [date]\n) AS temp\nGO\n</code></pre> <pre><code>CREATE VIEW [&lt;schema-name&gt;].[&lt;view-name&gt;]\nAS\nSELECT\n    *\nFROM\n    ...\n</code></pre>"},{"location":"services/azure/synapse/asa-partition-view/#read-mores","title":"Read Mores","text":"<ul> <li> User Permission in Serverless SQL Pools</li> <li> Serverless SQL: Partition Pruning Delta Tables in Azure Synapse Analytics</li> </ul>"},{"location":"services/databricks/","title":"Databricks","text":"<p>Databricks is a cloud-based big data and machine learning platform based on Apache Spark. It was created by the creators of Apache Spark and provides a convenient interface for working with large volumes of data, as well as tools for analytics and machine learning.</p> <p>Note</p> <p>It is widely used in clouds such as AWS, Google, and Microsoft Azure.</p> <p>Note</p> <p>Azure Databricks hands-on SSO via Azure user.</p> <p>Read more any information on the Databricks Blog</p>"},{"location":"services/databricks/custom-spark-connector/","title":"Custom Spark Connector","text":"<p>Project structure</p> <pre><code>rest_datasource/\n    - init.py\n    - rest_connector.py\n</code></pre> rest_connector.py<pre><code>import requests\nfrom pyspark.sql.datasource import DataSource, DataSourceReader, DataSourceWriter, WriterCommitMessage\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    IntegerType\n)\nfrom typing import Iterator, List\nfrom pyspark.sql.types import Row\nfrom dataclasses import dataclass\n\n# -----------------------------------------------------------------------------\n# 1) Define a custom DataSource\n# -----------------------------------------------------------------------------\nclass MyRestDataSource(DataSource):\n    \"\"\"\n    A custom data source for reading (and optionally writing) data from a REST API.\n\n    Example options:\n    - endpoint: e.g. 'posts' =&gt; https://jsonplaceholder.typicode.com/posts\n    - method: (for writes) e.g. 'POST', 'PUT'\n    \"\"\"\n\n    @classmethod\n    def name(cls):\n        # The short name used in spark.read.format(\"myrestdatasource\")\n        return \"myrestdatasource\"\n\n    def schema(self):\n        \"\"\"\n        Return a schema string (or a StructType) that Spark can use.\n        For this example, we assume a fixed schema for JSONPlaceholder 'posts':\n          userId (int), id (int), title (string), body (string)\n        \"\"\"\n        return \"userId int, id int, title string, body string\"\n\n    def reader(self, schema: StructType):\n        \"\"\"\n        Create and return a DataSourceReader for batch reads.\n        \"\"\"\n        return MyRestDataSourceReader(schema, self.options)\n\n    def writer(self, schema: StructType, overwrite: bool):\n        \"\"\"\n        Create and return a DataSourceWriter for batch writes (if needed).\n        \"\"\"\n        return MyRestDataSourceWriter(self.options, overwrite)\n\n\n# -----------------------------------------------------------------------------\n# 2) Define a DataSourceReader to handle reads\n# -----------------------------------------------------------------------------\nclass MyRestDataSourceReader(DataSourceReader):\n    def __init__(self, schema: StructType, options: dict):\n        self.schema = schema\n        # options is a dictionary of strings\n        self.options = options\n\n    def read(self, partition):\n        \"\"\"\n        Called on each partition to return an iterator of rows.\n        For simplicity, this example does NOT implement multiple partitions.\n        \"\"\"\n        base_url = \"https://jsonplaceholder.typicode.com\"\n        endpoint = self.options.get(\"endpoint\", \"posts\")  # default to 'posts'\n        url = f\"{base_url}/{endpoint}\"\n\n        # Make a GET request\n        resp = requests.get(url)\n        resp.raise_for_status()\n        data = resp.json()\n\n        # data is a list of dicts (JSONPlaceholder format).\n        # We yield tuples matching the schema [userId, id, title, body].\n        for item in data:\n            yield (\n                item.get(\"userId\"),\n                item.get(\"id\"),\n                item.get(\"title\"),\n                item.get(\"body\"),\n            )\n\n    def partitions(self):\n        \"\"\"\n        If you want multiple partitions, you would define them here.\n        For now, we'll return a single partition.\n        \"\"\"\n        from pyspark.sql.datasource import InputPartition\n        return [InputPartition(0)]\n\n\n# -----------------------------------------------------------------------------\n# 3) (Optional) Define a DataSourceWriter to handle writes\n# -----------------------------------------------------------------------------\n@dataclass\nclass SimpleCommitMessage(WriterCommitMessage):\n    partition_id: int\n    count: int\n\nclass MyRestDataSourceWriter(DataSourceWriter):\n    \"\"\"\n    This is a minimal example of writing to a REST API.\n    JSONPlaceholder won't actually persist the data, but let's illustrate.\n    \"\"\"\n\n    def __init__(self, options: dict, overwrite: bool):\n        self.options = options\n        self.overwrite = overwrite\n\n    def write(self, rows: Iterator[Row]) -&gt; WriterCommitMessage:\n        \"\"\"\n        Called on each partition to write data.\n        Return a commit message for the partition.\n        \"\"\"\n        from pyspark import TaskContext\n        context = TaskContext.get()\n        partition_id = context.partitionId()\n\n        base_url = \"https://jsonplaceholder.typicode.com\"\n        endpoint = self.options.get(\"endpoint\", \"posts\")\n        url = f\"{base_url}/{endpoint}\"\n        method = self.options.get(\"method\", \"POST\").upper()\n\n        count = 0\n        for row in rows:\n            count += 1\n            # Convert row to a dict for JSON\n            payload = row.asDict()\n            if method == \"POST\":\n                resp = requests.post(url, json=payload)\n            elif method == \"PUT\":\n                resp = requests.put(url, json=payload)\n            else:\n                raise NotImplementedError(f\"Method {method} not supported.\")\n            if not resp.ok:\n                raise RuntimeError(\n                    f\"Failed to write row {payload}, status: {resp.status_code}\"\n                )\n\n        return SimpleCommitMessage(partition_id=partition_id, count=count)\n\n    def commit(self, messages: List[SimpleCommitMessage]) -&gt; None:\n        total_count = sum(m.count for m in messages)\n        print(f\"SUCCESS: Wrote {total_count} rows to REST API.\")\n\n    def abort(self, messages: List[SimpleCommitMessage]) -&gt; None:\n        print(f\"ABORT: Some tasks failed to write to the REST API.\")\n</code></pre>"},{"location":"services/databricks/custom-spark-connector/#using","title":"Using","text":"<pre><code>from rest_datasource import MyRestDataSource\n\nspark.dataSource.register(MyRestDataSource)\n\ndf = (\n    spark.read\n        .format(\"myrestdatasource\")\n        .option(\"endpoint\", \"posts\")  # NOTE: url: /posts/\n        .load()\n)\n\ndf.show()\n\ntest_data = spark.createDataFrame(\n    [(999, 123, \"Test Title\", \"This is a test body.\")],\n    [\"userId\", \"id\", \"title\", \"body\"],\n)\n\n(\n    test_data.write\n        .format(\"myrestdatasource\")\n        .option(\"endpoint\", \"posts\")\n        .option(\"method\", \"POST\")\n        .mode(\"append\")\n        .save()\n)\n</code></pre>"},{"location":"services/databricks/custom-spark-connector/#references","title":"References","text":"<ul> <li>How to write own connector to Rest API in Spark Databricks</li> </ul>"},{"location":"services/databricks/databricks-app/","title":"Databricks: App","text":"<p>https://levelup.gitconnected.com/packaging-databricks-apps-b5e4cc88cde9</p>"},{"location":"services/databricks/databricks-aws-orchestration/","title":"Databricks: AWS - Orchestration","text":"<p>https://www.databricks.com/blog/2022/01/27/orchestrating-databricks-workloads-on-aws-with-managed-workflows-for-apache-airflow.html</p>"},{"location":"services/databricks/databricks-custom-policy/","title":"Databricks: Custom Policy","text":""},{"location":"services/databricks/databricks-custom-policy/#references","title":"References","text":"<ul> <li>https://medium.com/codex/tutorial-how-to-create-and-assign-a-custom-policy-on-databricks-cluster-7239b974c6e2</li> </ul>"},{"location":"services/databricks/databricks-custom-python-docker/","title":"Databricks: Running Python Wheel Tasks in Custom Docker Containers in Databricks","text":""},{"location":"services/databricks/databricks-custom-python-docker/#references","title":"References","text":"<ul> <li>https://towardsdatascience.com/running-python-wheel-tasks-in-custom-docker-containers-in-databricks-de3ff20f5c79</li> </ul>"},{"location":"services/databricks/databricks-data-quality/","title":"Databricks: Data Quality","text":"<p>https://medium.com/@josemanuelgarciagimenez/implementing-data-quality-with-databricks-2b15d89d3fa5</p>"},{"location":"services/databricks/databricks-delta-live-table/","title":"Delta Live Table","text":"<p>The data engineering domain has evolved recently to contain many data transformation frameworks. In the past years, frameworks like DBT and Dataform have gained momentum and proven their benefit across analytics environments. I had the chance to work with those frameworks and build data architectures that used them as a core for running data transformations and ensuring code quality.</p> <p></p>"},{"location":"services/databricks/databricks-delta-live-table/#getting-started","title":"Getting Started","text":""},{"location":"services/databricks/databricks-delta-live-table/#project-file-structure","title":"Project File Structure","text":"<pre><code>workflows/\n\u251c\u2500\u2500 datasource/\n\u2502  \u2514\u2500\u2500 &lt;env&gt;_deployment.yml\n\ndatasource/\n\u2502\n\u251c\u2500\u2500 raw/\n\u2502 \u251c\u2500\u2500 docs/\n\u2502 \u2502 \u2514\u2500\u2500 docs.yml\n\u2502 \u2502\n\u2502 \u251c\u2500\u2500 configs/\n\u2502 \u2502 \u2514\u2500\u2500 tables.yml\n\u2502 \u2502\n\u2502 \u251c\u2500\u2500 tests/\n\u2502 \u2502 \u2514\u2500\u2500 tests.yml\n\u2502 \u2502\n\u2502 \u2514\u2500\u2500 scripts/\n\u2502 \u251c\u2500\u2500 raw_customers.sql\n\u2502 \u2514\u2500\u2500 raw_orders.sql\n\u2502\n\u251c\u2500\u2500 silver/\n\u2502 \u2514\u2500\u2500 scripts/\n\u2502 \u251c\u2500\u2500 silver_customers.sql\n\u2502 \u2514\u2500\u2500 silver_orders.sql\n\u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 gold/\n\u2514\u2500\u2500 ...\n\u251c\u2500\u2500 datasource_transformation_notebook.py\n</code></pre>"},{"location":"services/databricks/databricks-delta-live-table/#references","title":"References","text":"<ul> <li>Production Ready Project File Structure for Databricks Delta Live Tables</li> </ul>"},{"location":"services/databricks/databricks-deploy-with-aws/","title":"Databricks: Deploy with AWS","text":"<p>https://www.databricks.com/blog/2017/07/13/serverless-continuous-delivery-with-databricks-and-aws-codepipeline.html https://www.linkedin.com/pulse/deploying-databricks-aws-deepak-rajak/</p>"},{"location":"services/databricks/databricks-dynamically-workflow/","title":"Databricks: Workflow Dynamically","text":""},{"location":"services/databricks/databricks-dynamically-workflow/#references","title":"References","text":"<ul> <li>https://medium.com/@rishianand.nits/create-update-databricks-workflow-dynamically-abba4b0916b8</li> </ul>"},{"location":"services/databricks/databricks-fastapi-to-serverless/","title":"Databricks: FastAPI to Serverless","text":""},{"location":"services/databricks/databricks-fastapi-to-serverless/#references","title":"References","text":"<ul> <li>https://betterprogramming.pub/build-a-fastapi-on-the-lakehouse-94e4052cc3c9</li> </ul>"},{"location":"services/databricks/databricks-fine-grained-access-control/","title":"Fine-grained Access Control","text":""},{"location":"services/databricks/databricks-fine-grained-access-control/#references","title":"References","text":"<ul> <li>Fine-grained Access Control with Permission Table in Databricks SQL</li> </ul>"},{"location":"services/databricks/databricks-func-workspace-organize/","title":"Databricks: Functional Workspace Organization","text":"<ul> <li>https://www.databricks.com/blog/2022/03/10/functional-workspace-organization-on-databricks.html</li> </ul>"},{"location":"services/databricks/databricks-lakehouse-monitoring/","title":"Monitoring","text":""},{"location":"services/databricks/databricks-lakehouse-monitoring/#references","title":"References","text":"<ul> <li>Introduction to Databricks Lakehouse monitoring</li> </ul>"},{"location":"services/databricks/databricks-lakehouse/","title":"Databricks: Lakehouse","text":"<p>https://medium.com/@tsiciliani/databricks-lakehouse-federation-1b149b123a4c</p>"},{"location":"services/databricks/databricks-liquibase/","title":"Databricks: Liquibase","text":"<p>https://medium.com/dbsql-sme-engineering/advanced-schema-management-on-databricks-with-liquibase-1900e9f7b9c0</p>"},{"location":"services/databricks/databricks-migration-workspace/","title":"Databricks: Migration Workspace","text":""},{"location":"services/databricks/databricks-migration-workspace/#references","title":"References","text":"<ul> <li>https://medium.com/d-one/databricks-workspace-migration-ce450e3931da</li> </ul>"},{"location":"services/databricks/databricks-realtime-etl/","title":"Databricks: Real-time ETL","text":"<p>https://medium.com/dbsql-sme-engineering/real-time-etl-on-databricks-sql-with-streaming-tables-and-materialized-views-e8930cdf4f1f</p>"},{"location":"services/databricks/databricks-row-and-column-level-filter/","title":"Databricks: Row Level Filtering and Column Level Masking in Databricks","text":""},{"location":"services/databricks/databricks-row-and-column-level-filter/#references","title":"References","text":"<ul> <li>https://medium.com/@rahulsoni4/row-level-filtering-and-column-level-masking-in-databricks-f561d3b81cc6</li> </ul>"},{"location":"services/databricks/databricks-tune-query-performance/","title":"Databricks: Tune Query Performance","text":"<p>https://medium.com/dbsql-sme-engineering/tune-query-performance-in-databricks-sql-with-the-query-profile-439196b18f47</p>"},{"location":"services/databricks/sql-parameter/","title":"SQL Parameterization","text":"<p>Warning</p> <p>Databricks SQL Warehouse supports robust parameterization, which you must know before starting with SQL scripting, which was introduced in DBR 16.3+.</p>"},{"location":"services/databricks/sql-parameter/#variables","title":"Variables","text":"<pre><code>DECLARE OR REPLACE VARIABLE sal_range INT;\nSET VAR sal_range = 20000;\n\nSELECT * FROM employees\nWHERE sal &gt; sal_range;\n</code></pre>"},{"location":"services/databricks/sql-parameter/#parameter-markers","title":"Parameter Markers","text":"<p>Note</p> <p>Widgets in SQL Editor on the notebook.</p> <pre><code>SELECT * FROM employees\nWHERE sal &gt; :sal_range;\n</code></pre>"},{"location":"services/databricks/sql-parameter/#dynamic-objects","title":"Dynamic Objects","text":"<pre><code>DECLARE OR REPLACE VARIABLE table_name STRING = 'employees';\n\nSELECT * FROM IDENTIFIER(table_name);\n</code></pre>"},{"location":"services/databricks/sql-parameter/#dynamic-sql-with-execute-immediate","title":"Dynamic SQL with EXECUTE IMMEDIATE","text":"<pre><code>DECLARE OR REPLACE sqlStr STRING = 'SELECT * FROM employees WHERE sal &gt; ?';\n\nEXECUTE IMMEDIATE sqlStr USING 2000;\n</code></pre>"},{"location":"services/databricks/sql-parameter/#parameterizing-use-catalog-databricks-161-update","title":"Parameterizing USE CATALOG (Databricks 16.1 Update)","text":"<pre><code>DECLARE OR REPLACE VARIABLE catalog_name STRING = 'workspace';\n\nUSE CATALOG IDENTIFIER(catalog_name);\n</code></pre>"},{"location":"services/databricks/sql-parameter/#combining-variables-and-parameters","title":"Combining Variables and parameters","text":"<pre><code>DECLARE OR REPLACE sql_query_top_1 STRING =\n  'SELECT sal FROM employees WHERE name = ? ORDER BY sal DESC LIMIT 1';\nDECLARE OR REPLACE user_salary INT;\nDECLARE OR REPLACE user_name STRING = 'John';\n\nEXECUTE IMMEDIATE sql_query_top_1 INTO user_salary USING user_name;\n\nSELECT user_salary; -- we put result to variable\n</code></pre>"},{"location":"services/databricks/sql-parameter/#conclusion","title":"Conclusion","text":"<ul> <li>Variables allow storing temporary values within a session.</li> <li>Widgets (Parameter Markers) enable UI-driven dynamic queries.</li> <li>IDENTIFIER is helpful for dynamic object references but cannot be embedded   (at least for now) inside a string for EXECUTE IMMEDIATE.</li> <li>EXECUTE IMMEDIATE is a key feature for future stored procedures and loops in   Databricks SQL.</li> </ul>"},{"location":"services/databricks/sql-parameter/#references","title":"References","text":"<ul> <li>SQL Parameterization in Databricks SQL</li> </ul>"},{"location":"services/databricks/sql-script/","title":"SQL Script","text":"<p>Implementation of SQL scripting in a preview of Spark 4 is progressing well. Although a few months are still ahead of us till we see Spark 4 in databricks, thanks to the fact that Spark 4 Preview, as always, is also open source, we can use the back door to see what is coming.</p> <p>Compound Statements &amp; Local Variables</p> <pre><code>BEGIN\n  DECLARE counter INT DEFAULT 1;\n  DECLARE total INT DEFAULT 0;\n\n  WHILE counter &lt;= 5 DO\n    SET total = total + counter;\n    SET counter = counter + 1;\n  END WHILE;\n\n  SELECT total AS sum_of_first_five;\nEND;\n</code></pre> <p>Condition Handling with <code>DECLARE HANDLER</code></p> <pre><code>BEGIN\n  DECLARE EXIT HANDLER FOR SQLSTATE '23505'  -- e.g. unique constraint\n  BEGIN\n    INSERT INTO error_logs VALUES('Duplicate key detected');\n  END;\n\n  -- Attempt to insert a duplicate row\n  INSERT INTO users VALUES (42, 'alice@example.com');\n  INSERT INTO users VALUES (42, 'alice@example.com');  -- triggers the handler\n\n  SELECT 'Done' AS status;\nEND;\n</code></pre> <p>Looping Constructs: <code>FOR</code>, <code>REPEAT</code>, <code>WHILE</code></p> <pre><code>BEGIN\n  DECLARE even_count INT DEFAULT 0;\n\n  FOR row AS\n    SELECT val FROM (VALUES (1), (2), (3), (4), (5)) AS t(val)\n  DO\n    IF (row.val % 2 = 0) THEN\n      SET even_count = even_count + 1;\n    END IF;\n  END FOR;\n\n  SELECT even_count AS number_of_evens;\nEND;\n</code></pre> <p>Working with Cursors (<code>DECLARE</code>, <code>OPEN</code>, <code>FETCH</code>, <code>CLOSE</code>)</p> <pre><code>BEGIN\n  DECLARE temp_val INT;\n  DECLARE cur CURSOR FOR SELECT val FROM range(1, 4);\n\n  OPEN cur;\n  FETCH cur INTO temp_val;  -- fetch row1 =&gt; 1\n  INSERT INTO cursor_demo VALUES (temp_val);\n\n  FETCH cur INTO temp_val;  -- fetch row2 =&gt; 2\n  INSERT INTO cursor_demo VALUES (temp_val);\n\n  -- more FETCH operations ...\n\n  CLOSE cur;\nEND;\n</code></pre> <p>Enhanced <code>SET VARIABLE</code> for Session or Script-Local Variables</p> <pre><code>DECLARE VARIABLE region STRING DEFAULT 'EMEA';\nDECLARE VARIABLE sales_threshold DECIMAL(10,2) DEFAULT 1000.00;\n\n-- Update a variable directly\nSET VARIABLE region = 'APAC';\n\n-- Assign the result of a query to a variable\nSET VARIABLE sales_threshold = (\n  SELECT avg(total_amount) FROM orders WHERE region = region\n);\n\nSELECT region AS current_region, sales_threshold AS threshold;\n</code></pre> <p>Creating and Calling Procedures</p> <pre><code>CREATE OR REPLACE PROCEDURE compute_bonus(\n  IN base_salary DECIMAL(10,2),\n  IN bonus_rate DECIMAL(5,4),\n  OUT result DECIMAL(10,2)\n)\nLANGUAGE SQL\nAS\nBEGIN\n  SET result = base_salary * bonus_rate;\nEND;\n\n-- Call the procedure\nCALL compute_bonus(1500.00, 0.10, ?);\n-- Above call returns: result =&gt; 150.00\n</code></pre> <p>Signaling Errors and Retrieving Diagnostics</p> <pre><code>BEGIN\n  DECLARE CONTINUE HANDLER FOR SQLEXCEPTION\n  BEGIN\n    DECLARE err_state STRING;\n    DECLARE err_message STRING;\n    GET DIAGNOSTICS CONDITION 1\n      err_state = RETURNED_SQLSTATE,\n      err_message = MESSAGE_TEXT;\n    INSERT INTO error_log VALUES(err_state, err_message, current_timestamp());\n  END;\n\n  -- Force an error to test logging\n  SIGNAL SQLSTATE '45001' SET MESSAGE_TEXT = 'Custom error triggered';\n\n  SELECT 'Script continued' AS status;  -- This won't run due to EXIT handler\nEND;\n</code></pre> <p>exceptions thrown from parser/interpreter</p> <pre><code>BEGIN\n  DECLARE x INT;\n  SET x = 'abc';  -- Error: type mismatch\nEND;\n</code></pre> <p>Support for labels</p> <pre><code>BEGIN label_one:\n  BEGIN label_two:\n    LEAVE label_one;  -- Jump out of label_one\u2019s block\n    SELECT 'Should not be reached' AS unreachable;\n  END label_two;\n  SELECT 'Reached' AS success;\nEND label_one;\n</code></pre> <p>Support for IF ELSE statement</p> <pre><code>BEGIN\n  DECLARE test_value INT DEFAULT 2;\n  IF test_value &lt; 5 THEN\n    SELECT 'Small' AS category;\n  ELSE\n    SELECT 'Large' AS category;\n  END IF;\nEND;\n</code></pre> <p>Support for CASE statement</p> <pre><code>BEGIN\n  DECLARE choice INT DEFAULT 3;\n  CASE\n    WHEN choice = 1 THEN SELECT 'One' AS result;\n    WHEN choice = 3 THEN SELECT 'Three' AS result;\n    ELSE SELECT 'Unknown' AS result;\n  END CASE;\nEND;\n</code></pre> <p>Support for LOOP statement</p> <pre><code>BEGIN\n  DECLARE counter INT DEFAULT 0;\n  loop_block: LOOP\n    IF counter &gt;= 2 THEN\n      LEAVE loop_block;\n    END IF;\n    INSERT INTO log VALUES (counter);\n    SET counter = counter + 1;\n  END LOOP loop_block;\nEND;\n</code></pre> <p>Support for REPEAT statement</p> <pre><code>BEGIN\n  DECLARE x INT DEFAULT 0;\n  REPEAT\n    SET x = x + 2;\n  UNTIL x &gt;= 6 END REPEAT;\n  SELECT x AS final_value;  -- 6\nEND;\n</code></pre> <p>Support for ITERATE statement</p> <pre><code>BEGIN\n  DECLARE i INT DEFAULT 0;\n  count_loop: WHILE i &lt; 5 DO\n    SET i = i + 1;\n    IF i = 3 THEN\n      ITERATE count_loop;  -- skip the rest for i=3\n    END IF;\n    INSERT INTO iteration_log VALUES (i);\n  END WHILE;\nEND;\n</code></pre> <p>Exception handling</p> <pre><code>BEGIN\n  DECLARE EXIT HANDLER FOR SQLSTATE '22012'  -- division by zero\n  BEGIN\n    INSERT INTO error_log VALUES('Division by zero occurred');\n  END;\n\n  SELECT 10 / 0;  -- triggers the handler\nEND;\n</code></pre> <p>Support for multiple variable declarations in the same statement</p> <pre><code>BEGIN\n  DECLARE a INT, b STRING DEFAULT 'test', c DECIMAL(10,2);\n  SELECT a, b, c;\nEND;\n</code></pre> <p>Support for RESIGNAL statement</p> <pre><code>BEGIN\n  DECLARE CONTINUE HANDLER FOR SQLEXCEPTION\n  BEGIN\n    RESIGNAL SQLSTATE '45001'\n      SET MESSAGE_TEXT = 'Re-throwing a custom error';\n  END;\n  SIGNAL SQLSTATE '22012';  -- any error triggers the handler\nEND;\n</code></pre> <p>Support for PRINT/TRACE statement</p> <pre><code>BEGIN\n  PRINT 'Starting script';  -- Not yet implemented\n  SELECT 'Some logic here';\nEND;\n</code></pre>"},{"location":"services/databricks/sql-script/#references","title":"References","text":"<ul> <li>SQL scripting in Spark 4</li> </ul>"},{"location":"services/databricks/sys-tables/","title":"System tables","text":""},{"location":"services/databricks/sys-tables/#what-are-databricks-system-tables","title":"What Are Databricks System Tables?","text":"<p>Located within the system catalog, Databricks system tables are a collection of metadata repositories that serve as the backbone for analyzing various aspects of your Databricks environment. They help monitor performance, track resource consumption, and analyze activity logs.</p> <p>Importantly, these tables offer users the ability to:</p> <ul> <li>Track the utilization of Databricks services like SQL Warehouses, Unity Catalog, Notebooks, Jobs, and Delta Live Tables.</li> <li>Gain detailed insights into resource consumption and billing to optimize costs effectively.</li> <li>Establish traceability for data transformations at both the table and column levels.</li> <li>Review operations initiated by Databricks, such as maintenance tasks.</li> </ul>"},{"location":"services/databricks/sys-tables/#real-world-use-cases-and-practical-examples","title":"Real-World Use Cases and Practical Examples","text":""},{"location":"services/databricks/sys-tables/#auditing-user-activities","title":"Auditing User Activities","text":"<p>To maintain a secure environment, tracking user actions is essential. The <code>system.access.audit</code> table provides a detailed log of user events, such as SQL executions and data access.</p> Example Query: Identifying Resource-Intensive Queries <pre><code>SELECT user_email, query_text, execution_time, rows_read\nFROM system.access.audit\nWHERE event_type = 'query' AND rows_read &gt; 1000000\nORDER BY execution_time DESC\nLIMIT 10;\n</code></pre>"},{"location":"services/databricks/sys-tables/#tracing-data-lineage","title":"Tracing Data Lineage","text":"<p>Understanding data flow is crucial for compliance and integrity. The <code>system.access.table_lineage</code> and <code>system.access.column_lineage</code> tables track transformations and relationships between datasets.</p> Example Query: Tracking Table Dependencies <pre><code>SELECT source_table_name, target_table_name, operation\nFROM system.access.table_lineage\nWHERE target_table_name = 'customer_analytics';\n</code></pre>"},{"location":"services/databricks/sys-tables/#managing-costs-effectively","title":"Managing Costs Effectively","text":"<p>The <code>system.billing.usage</code> table simplifies cost analysis, helping organizations allocate budgets wisely and identify inefficiencies.</p> Example Query: Monthly Cost Analysis <pre><code>WITH usage_costs AS (\n  SELECT\n    u.workspace_id,\n    u.sku_name,\n    u.usage_date,\n    DATE_FORMAT(u.usage_date, 'yyyy-MM') AS YearMonth,\n    u.usage_quantity,\n    lp.pricing.default AS list_price,\n    lp.pricing.default * u.usage_quantity AS list_cost,\n    COALESCE(u.usage_metadata.job_id, u.usage_metadata.dlt_pipeline_id, u.usage_metadata.warehouse_id, u.usage_metadata.notebook_id) AS resource_id\n  FROM\n    system.billing.usage u\n    INNER JOIN system.billing.list_prices lp\n      ON u.cloud = lp.cloud\n      AND u.sku_name = lp.sku_name\n      AND u.usage_start_time &gt;= lp.price_start_time\n      AND (u.usage_end_time &lt;= lp.price_end_time OR lp.price_end_time IS NULL)\n  WHERE u.usage_start_time &gt;= '2024-02-01'\n)\nSELECT usage_type, resource_id, SUM(usage_quantity) AS quantity, SUM(list_cost) AS cost\nFROM usage_costs\nGROUP BY usage_type, resource_id\nORDER BY cost DESC\nLIMIT 20;\n</code></pre>"},{"location":"services/databricks/sys-tables/#monitoring-compute-resources","title":"Monitoring Compute Resources","text":"<p>The <code>system.compute.clusters</code> table provides insights into cluster activity, enabling better resource management.</p> Example Query: Recently Terminated Clusters <pre><code>SELECT cluster_name, terminated_time, termination_reason\nFROM system.compute.clusters\nWHERE terminated_time IS NOT NULL\nORDER BY terminated_time DESC\nLIMIT 5;\n</code></pre>"},{"location":"services/databricks/sys-tables/#conclusion","title":"Conclusion","text":"<p>Databricks system tables offer unparalleled opportunities for monitoring and optimization. By combining metadata analysis with actionable insights, these tables help users maintain efficiency, control costs, and ensure compliance. Leverage the sample queries and schemas provided to unlock the full potential of your Databricks environment.</p>"},{"location":"services/databricks/sys-tables/#references","title":"References","text":"<ul> <li>Databricks System Tables for Monitoring and Optimization</li> </ul>"},{"location":"services/databricks/udf/","title":"UDFs","text":""},{"location":"services/databricks/udf/#python-udfs","title":"Python UDFs","text":"<p>Warning</p> <p>Custom dependencies on DBR 16.2+, UC service credentials and batched execution on DBR 16.3+.</p> NormalBatch <pre><code>CREATE OR REPLACE FUNCTION json_stringify(input_str STRING)\nRETURNS STRING\nLANGUAGE PYTHON\nENVIRONMENT (\n    dependencies = '[\"simplejson==3.19.*\"]',\n    environment_version = 'None'\n)\nAS $$\n    import simplejson as json\n    obj = {\"input\": input_str}\n    return json.dumps(obj)\n$$;\n</code></pre> <pre><code>SELECT json_stringify('Hello from Python!');\n</code></pre> <pre><code>CREATE OR REPLACE FUNCTION batch_upper(vals ARRAY&lt;STRING&gt;)\nRETURNS ARRAY&lt;STRING&gt;\nLANGUAGE PYTHON\nENVIRONMENT (\n  dependencies = '[\"pandas==2.0.0\"]',\n  environment_version = 'None'\n)\nAS $$\nimport pandas as pd\n\ndef transform_batch(input_vals):\n    df = pd.DataFrame({\"value\": input_vals})\n    df[\"upper\"] = df[\"value\"].str.upper()\n    return df[\"upper\"].tolist()\n\nreturn transform_batch(vals)\n$$;\n</code></pre> <pre><code>SELECT\n  id,\n  batch_upper(collect_list(text_col)) AS upper_values\nFROM my_table\nGROUP BY id;\n</code></pre>"},{"location":"services/databricks/udf/#references","title":"References","text":"<ul> <li>SQL &amp; Python go together: Unity Catalog Python UDFs with libraries</li> </ul>"},{"location":"services/docker/","title":"Docker","text":"<p>Docker container use only cloud native application which the code can run on container software. The large package software such as CRM, ERP, or SAP can not run on container yet, so it still run on VM.</p> <p>Docker is not compatible manage the large resource such as 128 Core with 1 memory 1TB.</p> <p>Note</p> <p>The Production database should not run on Docker or any container services. You able to go this way, but it has many problems, like maintain solution.</p>"},{"location":"services/docker/#docker-concepts","title":"Docker Concepts","text":"<p>Docker has concept Build, Ship, and Run. This concept make Docker be the most popular container software in this world.</p>"},{"location":"services/docker/docker-cmd-management/","title":"Docker System Management","text":""},{"location":"services/docker/docker-cmd-management/#usage","title":"Usage","text":"<pre><code>docker system df\n</code></pre>"},{"location":"services/docker/docker-cmd-management/#clear-cache","title":"Clear cache","text":"<pre><code>docker system prune -a\n</code></pre> <pre><code>docker builder prune\n</code></pre>"},{"location":"services/docker/docker-compose-postgresql/","title":"Docker: Docker Compose - Postgres","text":"<p>https://medium.com/towards-data-engineering/running-a-postgresql-and-pgadmin4-instance-using-docker-compose-c6dd6e6e03bb</p>"},{"location":"services/docker/docker-file/","title":"Docker: Dockerfile","text":""},{"location":"services/docker/docker-file/#docker-init","title":"Docker Init","text":"<p>Quote</p> <p>The docker init makes dockerization a piece of cake, especially for the Docker newbies. It eliminates the manual task of writing Dockerfiles and other configuration files, saving time and minimizing errors.<sup>1</sup></p> <p><code>docker init</code> is a command-line utility that helps in the initialization of Docker resources within a project. It creates Dockerfiles, Compose files, and <code>.dockerignore</code> files based on the project\u2019s requirements.</p> <p>Note</p> <p>Latest version of <code>docker init</code> supports Go, Python, Node.js, Rust, ASP.NET, PHP, and Java. It is available with Docker Desktop.</p>"},{"location":"services/docker/docker-file/#multi-stage-builds","title":"Multi-Stage Builds","text":"Python <pre><code># temp stage\nFROM python:3.9-slim as builder\n\nWORKDIR /app\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends gcc\n\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n\n# final stage\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY --from=builder /app/wheels /wheels\nCOPY --from=builder /app/requirements.txt .\n\nRUN pip install --no-cache /wheels/*\n</code></pre>"},{"location":"services/docker/docker-file/#references","title":"References","text":"<ul> <li>TestDriven: Tips - Docker multi-stage builds</li> </ul> <ol> <li> <p>You should stop writing Dockerfiles today \u2014 Do this instead \u21a9</p> </li> </ol>"},{"location":"services/google/","title":"Google","text":"<p>Google Cloud Platform (GCP), offered by Google, is a suite of cloud computing services that provides a series of modular cloud services including computing, data storage, data analytics, and machine learning, alongside a set of management tools.</p>"},{"location":"services/google/#examples","title":"Examples","text":"<ul> <li>PySpark in Google Cloud Platform</li> </ul>"},{"location":"services/google/oauth/","title":"OAuth","text":""},{"location":"services/google/oauth/#getting-started","title":"Getting Started","text":""},{"location":"services/google/oauth/#1-create-credential","title":"1) Create Credential","text":"<ul> <li>Go to APIs &amp; Services  On Enabled APIs &amp; Services  Click ENABLE APIS AND SERVICES</li> <li>Find your API that want to authorize  Click ENABLE</li> <li>On Credentials  Click CREATE CREDENTIALS and   select OAuth client ID</li> <li>Drop down and select Web Application  Pass a credential   name like <code>app-bigquery-server</code></li> <li>On Authorized redirect URIs  Pass <code>http://localhost:8080</code>  Click ADD URI and then CREATE</li> <li>Copy Client ID and Client Secret ID from this creation process</li> </ul>"},{"location":"services/google/oauth/#2-client-authenticate","title":"2) Client Authenticate","text":"<ul> <li> <p>Get Authorization Code</p> <pre><code>GET /o/oauth2/v2/auth HTTP/1.1\nHost: accounts.google.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}&amp;\nredirect_uri={redirect-uri}&amp;\nresponse_type=code&amp;\nscope={scope}&amp;\naccess_type=offline&amp;\ninclude_granted_scopes=true&amp;\nprompt=consent\n</code></pre> </li> <li> <p>Request Access and Refresh tokens</p> <pre><code>POST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\ncode={authorization-code}&amp;\nclient_id={client-id}.apps.googleusercontent.com&amp;\nclient_secret={client-secret}&amp;\nredirect_uri=https://oauth2-login.appclient.com/code&amp;\ngrant_type=authorization_code\n</code></pre> <p>Abstract</p> <p>When does a refresh token expire?:</p> <p>Refresh tokens do not expire, unless there are few special conditions:</p> <ul> <li>The user has removed your Google application.</li> <li>The refresh token has not been used for six months.</li> <li>The user changed password and the refresh token contained Gmail scopes.     This means that the refresh token will be invalidated only when he had previously     given the permisions for managing his Gmail, and then later changed his password.     For the rest of Google services like Youtube, Calendar etc, a changed password     will not invalidate the refresh token.</li> <li>The application generated a new refresh token for the user for more than 50 times.</li> </ul> </li> <li> <p>Re-generate Access Token</p> <pre><code>POST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={client-id}.apps.googleusercontent.com&amp;\nclient_secret={client-secret}&amp;\nrefresh_token={refresh-token}&amp;\ngrant_type=refresh_token\n</code></pre> </li> <li> <p>Varify Access Token</p> <pre><code>GET /oauth2/v3/tokeninfo HTTP/1.1\nHost: googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\naccess_token={access-token}\n</code></pre> </li> </ul> <p> How Can I Verify a Google Authentication API Access Token?</p> <p>Note</p> <p>Revoke:</p> <pre><code>POST /o/oauth2/revoke HTTP/1.1\nHost: accounts.google.com\nContent-Type: application/x-www-form-urlencoded\n\ntoken={refresh-token}\n</code></pre>"},{"location":"services/google/oauth/#oauth-playground","title":"OAuth Playground","text":"<ul> <li> Developer OAuth Playground</li> </ul>"},{"location":"services/google/oauth/#implement","title":"Implement","text":"<ul> <li> Implement OAuth 2.0 Authorization Flow With FastAPI</li> </ul>"},{"location":"services/google/oauth/#read-mores","title":"Read Mores","text":"<ul> <li> Google Developer: Identity - OAuth2 Web Server</li> <li> Getting Google OAuth Access Token using Google APIs</li> <li> How to get Google Authorization Code using PostMan</li> </ul>"},{"location":"services/google/oidc/","title":"OpenID Connect","text":""},{"location":"services/google/oidc/#workload-identity-federation-through-a-service-account","title":"Workload Identity Federation through a Service Account","text":"<p>Warning</p> <p>You should enable IAM Service Account Credentials API before.</p> <ol> <li> <p>(Optional) Create a Google Cloud Service Account. If you already have a     Service Account, take note of the email address and skip this step.</p> <pre><code>gcloud iam service-accounts create \"my-service-account\" \\\n  --project \"${PROJECT_ID}\"\n</code></pre> </li> <li> <p>Create a Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools create \"github\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"GitHub Actions Pool\"\n</code></pre> </li> <li> <p>Get the full ID of the Workload Identity Pool:</p> <pre><code>gcloud iam workload-identity-pools describe \"github\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --format=\"value(name)\"\n</code></pre> <p>This value should be of the format:</p> <pre><code>projects/123456789/locations/global/workloadIdentityPools/github\n</code></pre> </li> <li> <p>Create a Workload Identity Provider in that pool:</p> <pre><code>gcloud iam workload-identity-pools providers create-oidc \"my-repo\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"github\" \\\n  --display-name=\"My GitHub repo Provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.repository=assertion.repository,attribute.repository_owner=assertion.repository_owner\" \\\n  --attribute-condition=\"assertion.repository_owner == '${GITHUB_ORG}'\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n</code></pre> <p>Warning</p> <p>Using \"name\" fields in Attribute Conditions or IAM Bindings like repository and repository_owner increase the chances of cybersquatting and typosquatting attacks. If you delete your GitHub repository or GitHub organization, someone could claim that same name and establish an identity. To protect against this situation, use the numeric *_id fields instead, which GitHub guarantees to be unique and never re-used.</p> <p>To get your numeric organization ID:</p> <pre><code>ORG=\"my-org\" # TODO: replace with your org\ncurl -sfL -H \"Accept: application/json\" \"https://api.github.com/orgs/${ORG}\" | jq .id\n</code></pre> <p>To get your numeric repository ID:</p> <pre><code>REPO=\"my-org/my-repo\" # TODO: replace with your full repo including the org\ncurl -sfL -H \"Accept: application/json\" \"https://api.github.com/repos/${REPO}\" | jq .id\n</code></pre> <p>These can be used in an Attribute Condition:</p> <pre><code>assertion.repository_owner_id == '1342004' &amp;&amp; assertion.repository_id == '260064828'\n</code></pre> <p>Read more Security Considerations</p> </li> <li> <p>Allow authentications from the Workload Identity Pool to your Google Cloud    Service Account.</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/${WORKLOAD_IDENTITY_POOL_ID}/attribute.repository/${REPO}\"\n</code></pre> <p>Note</p> <p><code>${REPO}</code> is the full repo name including the parent GitHub organization, such as <code>my-org/my-repo</code>.</p> <p>Note</p> <p><code>${WORKLOAD_IDENTITY_POOL_ID}</code> is the full pool id, such as <code>projects/123456789/locations/global/workloadIdentityPools/github</code>.</p> <p>Abstract</p> <p>If you want to admit all repos in an organization, map on <code>attribute.repository_owner</code> (which will be the org name):</p> <pre><code>gcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project \"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/${WORKLOAD_IDENTITY_POOL_ID}/attribute.repository_owner/${ORG_NAME}\"\n</code></pre> </li> <li> <p>Extract the Workload Identity Provider resource name:</p> <pre><code>gcloud iam workload-identity-pools providers describe \"my-repo\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"github\" \\\n  --format=\"value(name)\"\n</code></pre> <p>Use this value as the workload_identity_provider value in the GitHub Actions YAML:</p> <pre><code>- uses: 'google-github-actions/auth@v2'\n  with:\n    service_account: '...' # my-service-account@my-project.iam.gserviceaccount.com\n    workload_identity_provider: '...' # \"projects/123456789/locations/global/workloadIdentityPools/github/providers/my-repo\"\n</code></pre> </li> <li> <p>As needed, grant the Google Cloud Service Account permissions to access    Google Cloud resources. This step varies by use case.    The following example shows granting access to a secret in Google Secret Manager.</p> <pre><code>gcloud secrets add-iam-policy-binding \"my-secret\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/secretmanager.secretAccessor\" \\\n  --member=\"serviceAccount:my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\"\n</code></pre> </li> </ol>"},{"location":"services/google/oidc/#references","title":"References","text":"<ul> <li>https://docs.github.com/en/actions/security-for-github-actions/security-hardening-your-deployments/configuring-openid-connect-in-google-cloud-platform</li> <li>https://cloud.google.com/iam/docs/workload-identity-federation-with-deployment-pipelines</li> <li>GitHub Action: Authenticate to Google Cloud</li> <li>https://github.com/google-github-actions/auth/issues/77</li> </ul>"},{"location":"services/google/bigquery/getting-started/","title":"BigQuery: Getting Started","text":"<p>https://medium.com/@vutrinh274/i-spent-6-hours-understanding-the-design-principles-of-bigquery-heres-what-i-found-6050cb7880fa</p>"},{"location":"services/google/bigquery/utils-func/","title":"Utilities Function","text":""},{"location":"services/google/bigquery/utils-func/#text-normaliser","title":"Text normaliser","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.text_normaliser (x STRING)\nRETURNS STRING\nAS (\n    TRIM(LOWER(\n        REGEXP_REPLACE(\n            REGEXP_REPLACE(\n                REGEXP_REPLACE(\n                    REGEXP_REPLACE(\n                        REGEXP_REPLACE(\n                            REGEXP_REPLACE(x, r\"[\u00e9\u00e8\u00ea\u00eb]\", \"e\"),\n                                r\"[\u00e1\u00e0\u00e2\u00e4\u00e3\u00e5]\", \"a\"),\n                            r\"[\u00e7]\", \"c\"),\n                        r\"[\u00ef\u00ee\u00ed\u00ec]\", \"i\"),\n                    r\"[\u00f4\u00f6\u00f2\u00f3\u00f5]\", \"o\"),\n                r\"[\u00fc\u00f9\u00fa\u00fb]\", \"u\")\n        )\n    ))\n);\n</code></pre> <pre><code>Input: ' CaF\u00e9 '\nOutput: 'cafe'\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#unicode-decoder","title":"Unicode decoder","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.unicode_to_fr (x STRING)\nRETURNS STRING\nAS (\n    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(\n    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(\n    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(\n    REPLACE(\n    REPLACE(x, '\\u00e9', '\u00e9'),  -- \u00e9\n           '\\u00ea', '\u00ea'),       -- \u00ea\n           '\\u00e0', '\u00e0'),       -- \u00e0\n           '\\u00e8', '\u00e8'),       -- \u00e8\n           '\\u00e2', '\u00e2'),       -- \u00e2\n           '\\u00f4', '\u00f4'),       -- \u00f4\n           '\\u00e7', '\u00e7'),       -- \u00e7\n           '\\u00e9', '\u00e9'),       -- \u00e9\n           '\\u00f9', '\u00f9'),       -- \u00f9\n           '\\u00fb', '\u00fb'),       -- \u00fb\n           '\\u00ee', '\u00ee'),       -- \u00ee\n           '\\u00ef', '\u00ef'),       -- \u00ef\n           '\\u00eb', '\u00eb'),       -- \u00eb\n           '\\u00e4', '\u00e4'),       -- \u00e4\n           '\\u00e3', '\u00e3'),       -- \u00e3\n           '\\u00f6', '\u00f6'),       -- \u00f6\n           '\\u00fc', '\u00fc'),       -- \u00fc\n           '\\u00e5', '\u00e5'),       -- \u00e5\n           '\\u2019', \"'\"),       -- \u2019\n           '\\u2013', \"-\")        -- \u2013\n);\n</code></pre> <pre><code>Input: \u201cJ\\u2019adore coder, surtout cr\\u00e9er des fonctions.\nOutput: J\u2019adore coder, surtout cr\u00e9er des fonctions.\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#a-function-that-splits-the-string","title":"A function that splits the string","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.alpha_sort(input_string STRING, separator STRING)\nRETURNS STRING\nLANGUAGE js AS \"\"\"\n  if (input_string === null || input_string === '') {\n    return '';\n  }\n\n  // Split the input string by the provided separator\n  let splitArray = input_string.split(separator);\n\n  // Trim any extra spaces around the items and sort alphabetically\n  let sortedArray = splitArray.map(item =&gt; item.trim()).sort();\n\n  // Join the sorted array back into a string using the provided separator\n  return sortedArray.join(separator);\n\"\"\";\n</code></pre> <pre><code>Input: 'banana|apple|cherry' and separator: '|'\nOutput: 'apple|banana|cherry'\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#number-of-days-without-weekends","title":"Number of days without weekends","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.unicode_to_fr (x STRING)\nRETURNS int64\nAS\n(DATE_DIFF(date(end_date), date(start_date), day) + 1)\n    -(DATE_DIFF(date(end_date), date(start_date), week ) * 2) --minus weekends\n    -(CASE WHEN FORMAT_DATE('%A',start_date) = 'Sunday' THEN 1 ELSE 0 END) --in case beginning or ending are weekends\n    -(CASE WHEN FORMAT_DATE('%A', end_date) = 'Saturday' THEN 1 ELSE 0 END)\n);\n</code></pre> <pre><code>Input: start_date = '2024-10-08' (Tuesday), end_date = '2024-10-14' (Monday)\nOutput: Weekdays: 5 (Tuesday to Monday, excluding the weekend)\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#number-of-days-without-weekends-and-holidays","title":"Number of days without weekends AND holidays","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.nb_days_without_weekends_and_holidays (start_date timestamp, end_date timestamp, holiday_country string)\nreturns int64\nas\n (\n    -- Subquery to calculate the working days\n   ( SELECT\n      `project.dataset.nb_days_without_weekends`(start_date, end_date)\n      - COALESCE(COUNT(holiday_date), 0) -- Handle cases with no holidays\n    FROM (\n      -- Subquery to select holidays\n      SELECT holiday_date\n      FROM `dataset_of_public_holidays`\n      WHERE date(holiday_date) BETWEEN date(start_date) AND date(end_date)\n        AND country = holiday_country\n        AND EXTRACT(DAYOFWEEK FROM holiday_date) NOT IN (1, 7) -- Exclude weekends\n        AND is_solidarity_day IS NULL\n    ) AS holidays)\n  )\n</code></pre> <pre><code>Input:start_date = '2024-11-10 00:00:00' (Sunday),end_date = '2024-11-15 23:59:59' (Friday),holiday_country = 'FR'\nOutput: Working Days: 5 (Monday to Friday, assuming November 11 (Armistice Day) is a holiday)\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#complete-years-difference","title":"Complete Years Difference","text":"<pre><code>CREATE OR REPLACE FUNCTION project.dataset.complete_years_diff (start_date DATE, end_date DATE)\nreturns int64\nas\n (\nCASE\n    WHEN EXTRACT(MONTH FROM end_date) &gt; EXTRACT(MONTH FROM start_date)\n      OR (EXTRACT(MONTH FROM end_date) = EXTRACT(MONTH FROM start_date)\n          AND EXTRACT(DAY FROM end_date) &gt;= EXTRACT(DAY FROM start_date))\n    THEN EXTRACT(YEAR FROM end_date) - EXTRACT(YEAR FROM start_date)\n    ELSE EXTRACT(YEAR FROM end_date) - EXTRACT(YEAR FROM start_date) - 1\n  END\n)\n</code></pre> <pre><code>Input: start_date = '2016-01-01' , end_date = '2024-01-01'\nOutput: Age: 8 (8 full years from January 1, 2016, to January 1, 2024)\n</code></pre>"},{"location":"services/google/bigquery/utils-func/#read-mores","title":"Read Mores","text":"<ul> <li>6 UDF ideas in BigQuery</li> </ul>"},{"location":"services/google/bigquery/with-iceberg/","title":"BigQuery Iceberg Tables","text":"<p>BigQuery Iceberg tables enable the creation of open-format lakehouses on Google Cloud. They combine the flexibility of customer-managed storage with BigQuery\u2019s managed analytics service, allowing you to work with data stored in your Cloud Storage buckets using the open-source Apache Iceberg table format.</p> <pre><code>CREATE TABLE dataset_name.table_name (\n    column1 STRING,\n    column2 INT64\n)\nWITH CONNECTION connection_name\nOPTIONS (\n    file_format = 'PARQUET',\n    table_format = 'ICEBERG',\n    storage_uri = 'gs://your-bucket/path-to-data'\n);\n</code></pre>"},{"location":"services/google/bigquery/with-iceberg/#references","title":"References","text":"<ul> <li> BigQuery tables for Apache Iceberg</li> </ul>"},{"location":"services/google/functions/gcf-to-managing-secrets/","title":"To Managing Secrets","text":""},{"location":"services/google/functions/gcf-to-managing-secrets/#read-mores","title":"Read Mores","text":"<ul> <li>Medium: Google Cloud - Manage Secrets for GCP Cloud Functions</li> </ul>"},{"location":"services/google/pubsub/","title":"Cloud PubSub","text":""},{"location":"services/infisical/","title":"Infisical","text":"<p>Infisical is the open source secret management platform that developers use to centralize their application configuration and secrets like API keys and database credentials as well as manage their internal PKI. Additionally, developers use Infisical to prevent secrets leaks to git and securely share secrets amongst engineers.</p>"},{"location":"services/infisical/#getting-started","title":"Getting Started","text":""},{"location":"services/infisical/#create-with-python","title":"Create with Python","text":""},{"location":"services/infisical/#install-python-packages","title":"Install Python Packages","text":"<pre><code>pip install flask infisical-python\n</code></pre>"},{"location":"services/infisical/#application-code","title":"Application Code","text":"<pre><code>from flask import Flask\nfrom infisical_client import ClientSettings, InfisicalClient, GetSecretOptions, AuthenticationOptions, UniversalAuthMethod\n\napp = Flask(__name__)\n\nclient = InfisicalClient(ClientSettings(\n    auth=AuthenticationOptions(\n      universal_auth=UniversalAuthMethod(\n        client_id=\"CLIENT_ID\",\n        client_secret=\"CLIENT_SECRET\",\n      )\n    )\n))\n\n@app.route(\"/\")\ndef hello_world():\n    name = client.getSecret(options=GetSecretOptions(\n       environment=\"dev\",\n       project_id=\"PROJECT_ID\",\n       secret_name=\"NAME\"\n    ))\n\n    return f\"Hello! My name is: {name.secret_value}\"\n</code></pre> <p>Read More Guides - Python</p>"},{"location":"services/k8s/","title":"Kubernetes","text":"<p>Kubernetes (K8s)</p>"},{"location":"services/k8s/#installation","title":"Installation","text":"MacOS <pre><code>$ brew install kubectl\n$ brew install helm\n$ brew install k9s\n</code></pre>"},{"location":"services/k8s/#core-concepts","title":"Core Concepts","text":"<ul> <li>Control Plane \u2014 The \u2018brain\u2019 of the cluster, responsible for managing the nodes and applications that are running on them. It consists of several components, such as API server, etcd, scheduler, and controller manager.</li> <li>API Server \u2014 It exposes a RESTful API that allows users to interact with the cluster.</li> <li>etcd \u2014 Database that stores the configuration data for the cluster.</li> <li>Scheduler \u2014 Responsible for placing applications onto nodes, depending on the availability and requirements of the resources.</li> <li>Controller Manager \u2014 monitoring the state of the cluster.</li> </ul>"},{"location":"services/k8s/#data-plane-components","title":"Data Plane Components","text":"<ul> <li> <p>Containers: Kubernetes is built around containerization. A container packages an application with all its dependencies and runtime environment. This ensures consistency across various development, testing, and production environments.</p> </li> <li> <p>Pods: The smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, network, and specifications on how to run the containers. Pods are ephemeral and disposable.</p> </li> <li> <p>Deployments: This is a higher-level concept that manages declarative updates for Pods and ReplicaSets. Deployments use a template for a pod and control parameters to scale the number of pods, rolling update strategies, and desired state. used for managing the rollout and scaling of applications.</p> </li> </ul> <p></p> <ul> <li>Services: Exposing an application running on a POD to the network (other PODS).   A Kubernetes Service is an abstraction layer that defines a logical set of Pods   and a policy by which to access them. This is often used to provide network access   to a set of pods.</li> </ul> <p></p> <ul> <li>Ingress \u2014 Ingresses provide external access to services within the cluster.   They manage external HTTP and HTTPS routing, allowing us to define rules for   handling incoming traffic.</li> </ul> <p></p> <ul> <li>Statefulsets \u2014 A StatefulSet is a set of pods with a unique, persistent hostname   and ID. StatefulSets are designed to run stateful applications in Kubernetes   with dedicated persistent storage. When pods run as part of a StatefulSet,   Kubernetes keeps state data in the persistent storage volumes of the StatefulSet,   even if the pods shut down. StatefulSets provide predictable order and state for   stateful workloads such as databases. They are scaled up or down sequentially   in a predictable order. This is very important for databases. StatefulSets have   a unique DNS name based on this order. This is important for workloads such as   Apache Kafka which distribute the data amongst their brokers; hence, one broker   is not the same as another.</li> </ul> <p></p> <ul> <li> <p>Config map \u2014 Config Maps store configuration data separately from application code, allowing for easier configuration changes without modifying the container image.</p> </li> <li> <p>Secret \u2014 Secrets store sensitive information such as passwords, API keys, and tokens. They are base64-encoded and can be mounted into pods or used as environment variables.</p> </li> </ul> <p></p> <ul> <li>Daemonsets \u2014 DaemonSets ensure that a copy of a pod runs on every node in the cluster. This is particularly useful for system-level tasks such as log collection, monitoring, or node-specific functionality.</li> </ul> <p></p> <ul> <li>Job \u2014 Jobs manage the execution of short-lived tasks to completion. They are useful for batch processing, data migration, or any task that needs to run to completion but doesn\u2019t require continuous execution.</li> <li>Cron Jobs \u2014 Cron Jobs enables the scheduling of recurring tasks in a Kubernetes cluster, similar to the cron jobs in traditional Unix/Linux systems. They are valuable for automating periodic processes.</li> </ul> <p></p> <ul> <li> <p>Namespace \u2014 Namespaces provide a way to divide cluster resources into virtual   clusters, enabling multi-tenancy and resource isolation. They help organize and   manage objects within a cluster.</p> </li> <li> <p>Role \u2014 Roles define a set of permissions within a namespace. They are used in conjunction with role bindings to grant access to resources within the cluster.</p> </li> <li>Role Binding \u2014 A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted</li> <li>Service Account \u2014 A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user.</li> </ul> <p></p> <ul> <li>Nodes: These are worker machines in Kubernetes, which can be either physical or virtual. Each node runs pods and is managed by the master node. Nodes have the necessary services to run pods and are managed by the control plane.</li> <li> <p>Label, Annotation &amp; Selectors \u2014 Labels are key-value pairs attached to objects (such as pods, nodes, services, and more) to help organize and categorize them. Labels can be used with selectors to filter and select specific resources. Annotations in Kubernetes are a mechanism for adding arbitrary metadata to objects (such as pods, services, nodes, and more) that are not used for identification or selection purposes.</p> </li> <li> <p>Affinity \u2014 Affinities are used to express Pod scheduling constraints that can match the characteristics of candidate Nodes and the Pods that are already running on those Nodes. A Pod that has an \u201caffinity\u201d to a given Node is more likely to be scheduled to it; conversely, an \u201canti-affinity\u201d makes it less probable it\u2019ll be scheduled. Affinity can be either an attracting affinity or a repelling anti-affinity.</p> </li> <li>Taints &amp; toleration \u2014 Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</li> <li>Cert Manager \u2014 cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates.</li> <li>Kubectl \u2014 Kubectl is the command-line interface (CLI) for interacting with Kubernetes clusters. It allows users to perform various operations on Kubernetes resources, such as deploying applications, inspecting and managing cluster resources, and troubleshooting issues. Kubectl communicates with the Kubernetes API server to execute these commands and obtain information about the cluster\u2019s state.</li> </ul> <p>??? note \"Kubectl commands\"</p> <pre><code>    ```shell\n    kubectl version --client\n    kubectl cluster-info\n    kubectl config view\n    kubectl config current-context\n    kubectl config use-context &lt;cluster name&gt;\n    kubectl get nodes\n    kubectl label node &lt;node-name&gt; node-role.kubernetes.io/worker=worker\n    kubectl drain &lt;node-name&gt;\n    kubectl cordon &lt;node-name&gt;\n    kubectl uncordon &lt;node-name&gt;\n    kubectl apply -f &lt;K8s-manifest.yaml&gt;\n    kubectl delete -f &lt;K8s-manifest.yaml&gt;\n\n    # Pod\n    kubectl get pods -n &lt;namespace&gt; -o wide\n    kubectl get pods -n &lt;namespace&gt; -w\n    kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs -f pod &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs --since=1h &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl logs --tail=200 &lt;pod-name&gt; -n &lt;namespace&gt;\n    kubectl exec -it &lt;pod-name&gt; -c &lt;container_name&gt; -n &lt;namespace&gt; -- bash\n    kubectl get events --field-selector involvedObject.name=&lt;pod-name&gt; --field-selector involvedObject.kind=Pod\n    kubectl port-forward &lt;pod-name&gt; -n &lt;namespace&gt; &lt;local-port&gt;:&lt;pod-port&gt;\n\n    # Deployment\n    kubectl get deployments -n &lt;namespace&gt;\n    kubectl edit deployments &lt;deployment_name&gt; -n &lt;namespace&gt;\n    kubectl set image deployment/nginx-deployment nginx=nginx:2.0\n    kubectl scale deployment &lt;deployment-name&gt; -n &lt;namespace&gt; --replicas=&lt;desired-replica-count&gt;\n    kubectl rollout restart deployment/&lt;deployment-name&gt;\n    kubectl rollout status deployment &lt;deployment-name&gt; -n &lt;namespace&gt;\n    kubectl delete deployment &lt;deployment_name&gt; -n &lt;namespace&gt;\n\n    # Services\n    kubectl get services -n &lt;namespace&gt;\n    kubectl describe service &lt;service-name&gt; -n &lt;namespace&gt;\n    kubectl expose deployment &lt;deployment-name&gt; -n &lt;namespace&gt; --type=NodePort --port=&lt;port&gt;\n\n    # Config Map &amp; Secret\n    kubectl create configmap &lt;configmap-name&gt; --from-file=&lt;path-to-file&gt; -n &lt;namespace&gt;\n    kubectl create secret generic &lt;secret-name&gt; --from-file=file_name -n &lt;namespace&gt;\n\n    # Role &amp; Role binding\n    kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods\n    kubectl create rolebinding bob-admin-binding --clusterrole=pod-reader --user=bob --namespace=&lt;namespace&gt;\n\n    # Job / CronJob\n    kubectl create cronjob my-cron --image=busybox --schedule=\"*/5 * * * *\" -- echo hello -n &lt;namespace&gt;\n\n    # Patch\n    kubectl patch svc my-apache -n apache-http-server -p '{\"spec\":{\"externalIPs\":[\"172.20.82.86\"]}}'\n\n    # Copy from local to pod\n    kubectl cp -n &lt;namespace&gt; &lt;local_file_path&gt; &lt;pod_name&gt;:/path/on/container/file_name\n\n    # Auth Check\n    kubectl --as=system:serviceaccount:NAMESPACE:SERVICE_ACCOUNT auth can-i get secret/SECRET_NAME\n    ```\n</code></pre> <ul> <li>Helm \u2014 is a package manager used to manage and deploy Kubernetes applications. Helm was developed to make complex application deployments and configurations in Kubernetes environments more manageable and reusable. Examples are apache-airflow/airflow and datahub/datahub.</li> </ul> <p>??? note \"Helm commands\"</p> <pre><code>    ```shell\n    # Helm Chart\n    helm create mychart\n    helm install myrelease ./mychart\n    helm uninstall myrelease\n    helm upgrade myrelease ./mychart\n    helm upgrade myrelease ./mychart --set key1=value1,key2=value2\n    helm rollback myrelease 1\n    helm repo list\n    helm repo add apache-airflow https://airflow.apache.org\n    helm repo update\n    helm install airflow apache-airflow/airflow --namespace airflow --create-namespace -f values.yaml\n    kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflow\n\n    # Examples from DE charts\n    helm repo add kafka https://charts.bitnami.com/bitnami\n    helm install kafka bitnami/kafka\n    helm repo add pinot https://raw.githubusercontent.com/apache/pinot/master/kubernetes/helm\n    helm install pinot pinot/pinot\n    helm repo add superset https://apache.github.io/superset\n    helm install superset superset/superset -f superset-values.yaml\n    ```\n</code></pre> <ul> <li>Operators \u2014 Kubernetes\u2019 operator pattern concept lets you extend the cluster\u2019s behavior without modifying the code of Kubernetes itself by linking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Examples are sparkoperator, and druid-operator.</li> </ul>"},{"location":"services/k8s/#common-kubernetes-errors","title":"Common Kubernetes Errors","text":"<ul> <li>CrashLoopBackOff \u2014 pod repeatedly crashes and is restarted by the kubelet, faulty container image, resource constraints, or problems with the application itself.</li> <li>ImagePullBackOff \u2014 kubelet is unable to pull the container image from the specified registry, incorrect image names or tags, network problems, or authentication issues.</li> <li>OutOfMemory \u2014 pod consumes more memory than its resource limits allow.</li> <li>OutOfMemoryKilled \u2014 The pod consumes more memory than its limits allow, and the kubelet kills the container to prevent the pod from consuming even more memory.</li> <li>OutOfCPU \u2014 The pod consumes more CPU resources than its limits allow.</li> <li>ImagePullError \u2014The kubelet is unable to pull the container image from the specified registry due to a non-HTTP error, network connectivity problems, or registry availability issues.</li> <li>EvictionWarning \u2014 The pod is at risk of being evicted from a node due to resource constraints.</li> <li>FailedScheduling \u2014 The scheduler is unable to find a node that meets the resource and placement requirements of a pod.</li> <li>DiskPressure \u2014 The node runs out of available disk space.</li> <li>NodeUnreachable \u2014 The pod is unable to communicate with its assigned node.</li> <li>RunContainerError \u2014 The issue is usually due to misconfiguration such as: Mounting a not-existent volume such as ConfigMap or Secrets. Mounting a read-only volume as read-write.</li> <li>Pod in pending state \u2014 The cluster doesn\u2019t have enough resources such as CPU and memory to run the Pod. The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota. The Pod is bound to a Pending PersistentVolumeClaim.</li> </ul>"},{"location":"services/k8s/#practices","title":"Practices","text":"<ul> <li>Most common mistakes to avoid when using Kubernetes: Anti-Patterns</li> <li>Modern Data Engineering with Kubernetes: End-to-End Data Pipeline</li> </ul>"},{"location":"services/k8s/#references","title":"References","text":"<ul> <li>LearnK8s - Troubleshooting Deployments</li> <li>Intro to K8s for Data Engineers</li> <li>https://blog.cloudnatician.com/%E0%B8%AD%E0%B8%98%E0%B8%B4%E0%B8%9A%E0%B8%B2%E0%B8%A2-kubernetes-%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99-%E0%B9%83%E0%B8%99-5-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-823cb6190c65</li> <li>Kubernetes for Data Engineering</li> </ul>"},{"location":"services/k8s/k8s-networking/","title":"Networking","text":"<p>The Kubernetes network model makes it possible for different components of a Kubernetes cluster, like Nodes, Pods, Services, and external traffic, to talk to each other. Kubernetes is a distributed system, the network plane spans across our cluster\u2019s physical Nodes. It uses a virtual overlay network that provides a flat structure for our cluster resources to connect to.</p>"},{"location":"services/k8s/k8s-networking/#references","title":"References","text":"<ul> <li>K8s Networking</li> </ul>"},{"location":"services/k8s/k8s-pod-scheduling/","title":"Pod Scheduling","text":"<p>Scheduling in Kubernetes is a core component as it determines where a pod will be launched. For every pod scheduler finds the best Node for launching it. There are several ways available through which one can determine on what node, pods can be placed.</p> <p>Among these features, Node selector, affinity and anti-affinity, taints &amp; toleration, stand out as essential tools for fine-tuning pod placement according to specific requirements.</p> <p>Let\u2019s take a look at some of them.</p>"},{"location":"services/k8s/k8s-pod-scheduling/#node-selector","title":"Node Selector","text":"<p>Node Selector is the simplest yet effective way to control pod scheduling on specific nodes in a Kubernetes cluster based on labels. Node Selector allows us to specify a label query (key-value pairs defined in Pod spec) that must be satisfied by a node for a pod to be scheduled on that node. This feature provides a straightforward mechanism for pod placement according to simple matching attributes like node labels.</p> <p>We can label the nodes where we want specific pods to be scheduled:</p> <pre><code>kubectl label nodes &lt;node-name&gt; disktype=ssd\n</code></pre> <p>Specifying Node Selector in Pod Spec:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: spark-executors\nspec:\n  containers:\n  - name: spark-executors\n    image: apache-spark:v3.4.1\n  nodeSelector:\n    disktype: ssd\n</code></pre> <p>This pod will only be scheduled on nodes labeled with <code>disktype=ssd</code>.</p>"},{"location":"services/k8s/k8s-pod-scheduling/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 Pod Scheduling</li> </ul>"},{"location":"services/k8s/k8s-rbac/","title":"RBAC","text":"<p>Quote</p> <p>Users are mapped to roles, roles are mapped to set of permissions that allow access to resource(s).</p> <p>When K8s receives a new request K8s API server performs the following steps:</p> <ul> <li>Authenticate the user, if validation fails return <code>401 unauthorised</code></li> <li>Authorise the request, if it fails return <code>403 Forbidden</code></li> </ul>"},{"location":"services/k8s/k8s-rbac/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 RBAC</li> </ul>"},{"location":"services/k8s/k8s-state-phase/","title":"State &amp; Phase","text":""},{"location":"services/k8s/k8s-state-phase/#pod-phases","title":"Pod phases","text":"<p>Pods have a predetermined life cycle. We can use the Kubernetes API to check its status. Pod statuses are crucial in understanding the health and state of a pod at any given time. There are five primary phases or statuses that a pod can be in.</p> <p></p>"},{"location":"services/k8s/k8s-state-phase/#pending","title":"Pending","text":"<p>The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</p> <ul> <li>Insufficient resources: When the cluster does not have enough resources to accommodate the pod\u2019s resource requests, the pod cannot be scheduled.</li> <li>Image issues: If there is a wrong image name specified in the pod spec, or issues are pulling the image from the repository, the pod will not start.</li> <li>Node selector and node labels mismatches: Node selectors are used to target specific nodes for pod placement. If the labels on the nodes do not match the selectors defined in the pod specification, the pod will remain unscheduled.</li> <li>Taints and Tolerations: If a node is \u201ctainted\u201d, it\u2019s marked to repel certain Pods. A Pod with a matching \u201ctoleration\u201d can be scheduled on such a node. If no node with a suitable toleration for the Pod\u2019s taints exists, the Pod will stay in Pending status.</li> <li>Unschedulable nodes: Nodes may be marked as unschedulable for various reasons, such as maintenance, which prevents pods from being scheduled on them.</li> <li>Rolling update surge</li> <li>Inter pod affinity &amp; anti affinity</li> <li>PVC linked issues</li> </ul>"},{"location":"services/k8s/k8s-state-phase/#running","title":"Running","text":"<p>The Pod has been bound to a node, and all the containers have been created. At least one container is still running, or is in the process of starting or restarting.</p>"},{"location":"services/k8s/k8s-state-phase/#succeeded","title":"Succeeded","text":"<p>All containers in the Pod have terminated in success, and will not be restarted.</p>"},{"location":"services/k8s/k8s-state-phase/#failed","title":"Failed","text":"<p>All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.</p>"},{"location":"services/k8s/k8s-state-phase/#unknown","title":"Unknown","text":"<p>For some reason the state of the Pod could not be obtained, typically due to an error in communicating with the host of the Pod.</p>"},{"location":"services/k8s/k8s-state-phase/#references","title":"References","text":"<ul> <li>K8s for Data Engineers \u2014 Pod &amp; Container States/Phase</li> </ul>"},{"location":"services/opentofu/","title":"OpenTofu","text":"<p>OpenTofu</p>"},{"location":"services/opentofu/#getting-started","title":"Getting Started","text":""},{"location":"services/opentofu/#read-mores","title":"Read Mores","text":""},{"location":"services/server/","title":"Edge Server","text":""},{"location":"services/server/edge-sftp/","title":"SFTP Server","text":""},{"location":"services/server/edge-sftp/#gen-ssh","title":"Gen SSH","text":"<pre><code>$ ssh-keygen\n</code></pre> <pre><code>$ ls -l /home/user/.ssh\n-rw------- 1 user user 2610 Feb  7 15:11 id_rsa\n-rw-r--r-- 1 user user  573 Feb  7 15:11 id_rsa.pub\n</code></pre> <p>Take note of the permissions of the private key ( id_rsa ). SSH Private Key files should ALWAYS HAVE 600 PERMISSIONS! If not, change its permission to the said value using the chmod command:</p> <pre><code>$ chmod 600 /home/user/.ssh/id_rsa\n</code></pre> <pre><code>$ ssh-copy-id USER@IP\n</code></pre> <p>If you do not have <code>ssh-copy-id</code> available, but you have password-based SSH access to an account on your server, you can upload your keys using a conventional SSH method.</p> <pre><code>$ cat ~/.ssh/id_rsa.pub | ssh username@remote_host \"mkdir -p ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre> <pre><code>$ ssh USER@IP\n</code></pre>"},{"location":"services/server/edge-sftp/#connect-with-python","title":"Connect with Python","text":"<p>...</p>"},{"location":"services/server/edge-ssh/","title":"SSH","text":""},{"location":"services/server/edge-ssh/#getting-started","title":"Getting Started","text":""},{"location":"services/server/edge-ssh/#certificate","title":"Certificate","text":"<p>There are 4 different ways to present certificates and their components:</p> PEM <p>Governed by RFCs, used preferentially by open-source software because it is text-based and therefore less prone to translation/transmission errors. It can have a variety of extensions (<code>.pem</code>, <code>.key</code>, <code>.cer</code>, <code>.cert</code>, more)</p> PKCS7 <p>An open standard used by Java and supported by Windows. Does not contain private key material.</p> PKCS12 <p>A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key and certificate chain material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl.</p> DER <p>The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows.</p>"},{"location":"services/server/edge-ssh/#authenticate","title":"Authenticate","text":""},{"location":"services/server/edge-ssh/#with-password-and-public-key","title":"with Password and Public Key","text":"<p>First, we set the SSHD configuration file for allow support public key and password authentication methods together.</p> ~/etc/ssh/sshd_config<pre><code>AuthenticationMethods \"publickey,password\"\n</code></pre> <p>Add a command to get the public key matching process after the default step for easy maintenance in the future.</p> ~/etc/ssh/sshd_config<pre><code>AuthorizedKeysCommand /etc/ssh/authorized.sh %u\nAuthorizedKeysCommandUser root\n</code></pre> <p>Note</p> <p>AuthorizedKeysCommand will run after SSH daemon read public key from <code>authorized_keys</code> and does not found any matching key.</p> <p>Create bash script with root user,</p> /etc/ssh/authorized.sh<pre><code>#!/bin/bash\nfor file in /home/$1/.ssh/*.pub; do\n   cat $file;\ndone\n</code></pre> <p>Should grant permission with <code>sudo chmod 755 /etc/ssh/authorized.sh</code> for execute by SSH daemon.</p> <p>Add your client public key to <code>~/.ssh/</code> path, for example, I will add my public key with <code>username.pub</code>.</p> <pre><code>~/.ssh/\n    .\n    ..\n    authorized_keys\n    username.pub\n</code></pre> <p>Finally, refresh ssh service</p> Ubuntu/Debian <pre><code>sudo service ssh reload\n</code></pre> <p>Testing,</p> DefaultFix Private Key Path <pre><code>ssh username@hostname\n</code></pre> <pre><code>ssh -i /path/of/private-key/username username@hostname\n</code></pre>"},{"location":"services/server/edge-ssh/#multiple-public-keys","title":"Multiple Public Keys","text":"<p>Generate multi pair of private and public key,</p> <pre><code>ssh-keygen -t rsa -f ~/.ssh/id_rsa.home\nssh-keygen -t rsa -f ~/.ssh/id_rsa.work\n</code></pre> ~/.ssh/config<pre><code>Host home\n    Hostname home.example.com\n    IdentityFile ~/.ssh/id_rsa.home\n    User &lt;your home acct&gt;\n    IdentitiesOnly yes\n\nHost work\n    Hostname work.example.com\n    IdentityFile ~/.ssh/id_rsa.work\n    User &lt;your work acct&gt;\n    IdentitiesOnly yes\n</code></pre>"},{"location":"services/server/edge-ssh/#read-mores","title":"Read Mores","text":"<ul> <li> Key based SSH login that requires both key AND password</li> </ul>"},{"location":"services/server/edge-ssl/","title":"SSL/TSL","text":"<p>Data Engineering often involves the development and use of custom web APIs (REST, GraphQL, etc.), as well as the integration and utilization of various open-source web applications such as Spark UI, Kafka UI, Grafana, Kibana, and many more. Ensuring secure communication for these components is vital to protect sensitive data and maintain data integrity.</p>"},{"location":"services/server/edge-ssl/#read-mores","title":"Read Mores","text":"<ul> <li> Automating SSL/TLS Certificate Management in Data Engineering</li> </ul>"},{"location":"services/snowflake/","title":"Snowflake","text":""},{"location":"services/snowflake/adbc/","title":"ADBC","text":"<p>What is Apache Arrow and ADBC?</p> <p>Apache Arrow is a columnar memory format designed for fast analytics and data transfer between systems. While Arrow itself is the format specification, ADBC (Arrow Database Connectivity) is the database connectivity layer that enables standardized access to databases using Arrow.</p> <p></p> <p>In ODBC land, we hit the Snowflake API with an ODBC driver, Snowflake converts the requested data from columnar format to row format (~100ms), serialize to ODBC format (500ms), network transfer (1\u20132 seconds), deserialize from ODBC (~500ms), and convert the data back from row format into columnar format for DuckDB.</p> <p>In the great land of ADBC, we hit the Snowflake API with an ADBC driver, the data stays in columnar format, network transfer (1\u20132 seconds), and we read the data into DuckDB as is.</p> <p>The challenge in using JDBC or ODBC is due to the rise in columnar databases (Redshift, BigQuery, Snowflake, Databricks, etc.) and columnar formats (Parquet, Avro, ORC, etc.). Using ODBC with columnar databases forces a time-consuming conversion between rows and columns, a process known as serialization/deserialization.</p>"},{"location":"services/snowflake/adbc/#getting-started","title":"Getting Started","text":"<pre><code>pip install adbc_driver_snowflake pyarrow duckdb\n</code></pre> <pre><code>SNOWFLAKE_CONFIG = {\n    'adbc.snowflake.sql.account': os.getenv('SNOWFLAKE_ACCOUNT'),\n    'adbc.snowflake.sql.warehouse': os.getenv('SNOWFLAKE_WAREHOUSE'),\n    'adbc.snowflake.sql.role': os.getenv('SNOWFLAKE_ROLE'),\n    'adbc.snowflake.sql.database': os.getenv('SNOWFLAKE_DATABASE'),\n    'username': os.getenv('SNOWFLAKE_USER'),\n    'password': os.getenv('SNOWFLAKE_PASSWORD'), # Comment this line if you want to use a password\n    # 'adbc.snowflake.sql.client_option.jwt_private_key_pkcs8_value': pem_key, # Uncomment this line if you want to use a password\n    # 'adbc.snowflake.sql.auth_type': 'auth_jwt' # Uncomment this line if you want to use a password\n}\n\n# Create Snowflake connection\nsnowflake_conn = adbc_driver_snowflake.dbapi.connect(\n    db_kwargs={**SNOWFLAKE_CONFIG}\n)\n</code></pre> <pre><code>pip install cryptography\n</code></pre> <pre><code>from cryptography.hazmat.primitives import serialization\n\ndef read_private_key(private_key_path: str, private_key_passphrase: str = None) -&gt; str:\n    \"\"\"Read a private key file and return its PEM representation as a string\"\"\"\n    with open(private_key_path, 'rb') as key_file:\n        private_key = serialization.load_pem_private_key(\n            key_file.read(),\n            password=private_key_passphrase.encode() if private_key_passphrase else None\n        )\n\n        # Convert to PEM format with PKCS8\n        pem_key = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        )\n        return pem_key.decode('utf-8')\n</code></pre> <pre><code>pem_key = read_private_key(os.getenv('SNOWFLAKE_PRIVATE_KEY_PATH'), os.getenv('SNOWFLAKE_PRIVATE_KEY_PASSPHRASE'))\n</code></pre>"},{"location":"services/snowflake/adbc/#full-code","title":"Full Code","text":"<pre><code>import adbc_driver_snowflake.dbapi\nimport duckdb\nimport dotenv\nimport os\nfrom read_private_key import read_private_key # Uncomment if you're using a private key\n\n# Load environment variables\ndotenv.load_dotenv()\n\n# Configuration for Snowflake connection\npem_key = read_private_key(os.getenv('SNOWFLAKE_PRIVATE_KEY_PATH'), os.getenv('SNOWFLAKE_PRIVATE_KEY_PASSPHRASE'))\n\nSNOWFLAKE_CONFIG = {\n    'adbc.snowflake.sql.account': os.getenv('SNOWFLAKE_ACCOUNT'),\n    'adbc.snowflake.sql.warehouse': os.getenv('SNOWFLAKE_WAREHOUSE'),\n    'adbc.snowflake.sql.role': os.getenv('SNOWFLAKE_ROLE'),\n    'adbc.snowflake.sql.database': os.getenv('SNOWFLAKE_DATABASE'),\n    'username': os.getenv('SNOWFLAKE_USER'),\n    'password': os.getenv('SNOWFLAKE_PASSWORD'), # Comment this line if you want to use a password\n    # 'adbc.snowflake.sql.client_option.jwt_private_key_pkcs8_value': pem_key, # Uncomment this line if you want to use a password\n    # 'adbc.snowflake.sql.auth_type': 'auth_jwt' # Uncomment this line if you want to use a password\n}\n\n# Create Snowflake connection\nsnowflake_conn = adbc_driver_snowflake.dbapi.connect(\n    db_kwargs={**SNOWFLAKE_CONFIG}\n)\nprint(\"Connection to Snowflake successful.\")\nsnowflake_cursor = snowflake_conn.cursor()\n\n# Query Snowflake\nquery = \"\"\"\n    SELECT\n        *\n    FROM SANDBOX_DB.KYLE_SCHEMA.RAW_ORDERS\n\"\"\"\nsnowflake_cursor.execute(query)\nprint(\"Query executed successfully.\")\n\n# Store results as an arrow table\narrow_table = snowflake_cursor.fetch_arrow_table()\n\n# Create DuckDB connection and store locally to path\nduckdb_conn = duckdb.connect('demo.db')\n\n# Store arrow table in DuckDB\ntable_name = 'raw_orders'\nquery = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM arrow_table\n\"\"\"\nduckdb_conn.execute(query)\nprint(\"Table created in DuckDB successfully.\")\n\n# Alternatively you can use the from_arrow_table method\n# duckdb_conn.from_arrow_table(arrow_table)\n\n# Close connections\nsnowflake_cursor.close()\nsnowflake_conn.close()\nduckdb_conn.close()\n\nprint(\"All done :)\")\n\n# Now we can spin up DuckDB CLI and work with the data locally! Enter the following command in your terminal to connect:\n# duckdb demo.db\n</code></pre>"},{"location":"services/snowflake/adbc/#references","title":"References","text":"<ul> <li>Cut Costs by Querying Snowflake Tables in DuckDB with Apache Arrow</li> </ul>"},{"location":"services/snowflake/bulk-load/","title":"Bulk Loads","text":""},{"location":"services/snowflake/bulk-load/#references","title":"References","text":"<ul> <li>Mastering Snowflake Bulk Loads \u2014 Airflow, Stages, &amp; Stored Procedures</li> </ul>"},{"location":"services/snowflake/snf-data-wash/","title":"Data Wash","text":""},{"location":"services/snowflake/snf-data-wash/#references","title":"References","text":"<ul> <li>DataWash: An Advanced Snowflake Data Quality Tool Powered by Snowpark \u2014 Part 1</li> </ul>"},{"location":"services/terraform/","title":"Terraform","text":"<p>Terraform for Data Engineer</p>"},{"location":"services/terraform/#read-mores","title":"Read Mores","text":"<ul> <li>https://blog.skooldio.com/terraform-vs-ansible/#:~:text=%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B9%81%E0%B8%A5%E0%B9%89%E0%B8%A7%20%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%20Terraform%20%E0%B8%A1%E0%B8%B5,%E0%B8%97%E0%B8%B3%20Provisioning%20infra%20%E0%B9%84%E0%B8%94%E0%B9%89%E0%B9%80%E0%B8%8A%E0%B9%88%E0%B8%99</li> <li>https://nopnithi.medium.com/%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B9%81%E0%B8%95%E0%B8%81%E0%B8%95%E0%B9%88%E0%B8%B2%E0%B8%87%E0%B8%A3%E0%B8%B0%E0%B8%AB%E0%B8%A7%E0%B9%88%E0%B8%B2%E0%B8%87-terraform-vs-ansible-c912bdee04dd</li> </ul>"},{"location":"services/terraform/tf-aws-glue/","title":"AWS Glue","text":""},{"location":"services/terraform/tf-aws-glue/#references","title":"References","text":"<ul> <li>How I build an ETL pipeline with AWS Glue, Lambda, and Terraform</li> </ul>"},{"location":"services/terraform/tf-azure-databricks/","title":"To Azure Databricks","text":"<p>```terraform titles=\"variables.tf\"</p>"},{"location":"services/terraform/tf-azure-databricks/#-","title":"------------------------","text":""},{"location":"services/terraform/tf-azure-databricks/#authentication-variables","title":"Authentication Variables","text":""},{"location":"services/terraform/tf-azure-databricks/#-_1","title":"------------------------","text":"<p>variable \"aad_tenant_id\" {   type        = string   description = \"The id of the Azure Tenant to which all subscriptions belong\" }</p> <p>variable \"aad_subscription_id\" {   type        = string   description = \"The id of the Azure Subscription\" }</p> <p>variable \"aad_client_id\" {   type        = string   description = \"The client id of the Service Principal for interacting with Azure resources\" }</p> <p>variable \"aad_client_secret\" {   type        = string   description = \"The client secret of the Service Principal for interacting with Azure resources\"   sensitive   = true }</p>"},{"location":"services/terraform/tf-azure-databricks/#-_2","title":"-------------------","text":""},{"location":"services/terraform/tf-azure-databricks/#terraform-variables","title":"Terraform Variables","text":""},{"location":"services/terraform/tf-azure-databricks/#-_3","title":"-------------------","text":"<p>variable \"azure_service_principal_display_name\" {   description = \"A display name for the Azure Active Directory (Azure AD) Service Principal.\"   type        = string   default     = \"Terraform Databricks\" } ```</p>"},{"location":"services/terraform/tf-azure-databricks/#references","title":"References","text":"<ul> <li>https://medium.com/@alonso.md/deploy-azure-databricks-using-terraform-6e8a39aa7287</li> <li>https://gmusumeci.medium.com/how-to-deploy-databricks-in-azure-with-terraform-step-by-step-e1262e456be9</li> </ul>"},{"location":"services/terraform/tf-databricks/","title":"Terraform: Databricks","text":"<p>https://github.com/databricks/terraform-databricks-examples https://github.com/databricks/terraform-databricks-examples/blob/main/modules/aws-databricks-workspace/variables.tf</p>"},{"location":"services/terraform/tf-manage-secret/","title":"Terraform: Manage Secret","text":"<p>One of the most common questions we get about using Terraform to manage infrastructure as code (IaC) is how to handle secrets such as passwords, API keys, and other sensitive data.</p> <p>For example, here\u2019s a snippet of Terraform code that can be used to deploy MySQL using Amazon RDS:</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  engine               = \"mysql\"\n  engine_version       = \"5.7\"\n  instance_class       = \"db.t2.micro\"\n  name                 = \"example\"\n\n  # How should you manage the credentials for the master user?\n  username             = \"???\"\n  password             = \"???\"\n}\n</code></pre>"},{"location":"services/terraform/tf-manage-secret/#technique-environment-variables","title":"Technique: Environment Variables","text":"<pre><code>resource \"aws_db_instance\" \"example\" {\n  engine               = \"mysql\"\n  engine_version       = \"5.7\"\n  instance_class       = \"db.t2.micro\"\n  name                 = \"example\"\n\n  # Set the secrets from variables\n  username             = var.username\n  password             = var.password\n}\n</code></pre>"},{"location":"services/terraform/tf-manage-secret/#references","title":"References","text":"<ul> <li>GruntWork: A Comprehensive Guide to Manage Secrets in Terraform</li> </ul>"},{"location":"tools/","title":"Tools","text":"<p>Warning</p> <p>I will filter Data Engineering Tools on this session that do not dynamically and flexibility for the most Data Architect and Modern Data Strack.</p> <p>Note</p> <p>This session groups any Open-Soure Tools base on Modern Data Stack concept. Some topic I found the tools from the ReStack</p> <p>This tools topic, I will focus with below contents:</p> <ul> <li>Setting Connections</li> <li>Implement its Features</li> <li>Tuning &amp; Optimization</li> </ul>"},{"location":"tools/#tools-stacks","title":"Tools Stacks","text":"<p>The tools stacks choice for each Data Architecture that fit with cost and easy to implement for small to large scale.</p> <ul> <li> <ul> <li>Dagster or Mage.ai for orchestration (TBD)</li> <li>Polars for lightning fast ETL workloads</li> <li>Delta Lake as the storage layer</li> <li>DuckDB as the analytical SQL interface</li> <li>Rill or Evidence for data viz (TBD)</li> </ul> </li> </ul>"},{"location":"tools/git/","title":"Git","text":"<p>Quote</p> <p>Git is an Open Source Distributed Version Control System Designed to handle everything from small to very large projects with speed and efficiency.</p> <p> Git is the best version control tools for any developer, you can read the Official Documents.</p> <ul> <li>Save and Track different versions of your repositories</li> <li>Coordinate changes across different teams without impacting the work of other collaborators</li> <li>Share local copies of the same codebases as other developers when working offline</li> <li>Isolate new fixes and features in development without impacting production</li> </ul>"},{"location":"tools/git/#workflow","title":"Workflow","text":"<pre><code>Initial ---&gt; Working &lt;--&gt; Index/Staging &lt;--&gt; Repository\n</code></pre> InitialWorkingIndex/StagingRepository <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; hooks/\n  \u251c\u2500\u2500\u2500&gt; info/\n  \u251c\u2500\u2500\u2500&gt; objects/\n  \u251c\u2500\u2500\u2500&gt; refs/\n  \u251c\u2500\u2500\u2500&gt; config\n  \u251c\u2500\u2500\u2500&gt; description\n  \u251c\u2500\u2500\u2500&gt; FETCH_HEAD\n  \u251c\u2500\u2500\u2500&gt; HEAD\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; hooks/\n  \u251c\u2500\u2500\u2500&gt; info/\n  \u251c\u2500\u2500\u2500&gt; objects/\n  \u251c\u2500\u2500\u2500&gt; refs/\n  \u251c\u2500\u2500\u2500&gt; config\n  \u251c\u2500\u2500\u2500&gt; description\n  \u251c\u2500\u2500\u2500&gt; FETCH_HEAD\n  \u251c\u2500\u2500\u2500&gt; HEAD\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u251c\u2500\u2500\u2500&gt; objects/\n  |       \u251c\u2500\u2500\u2500&gt; 19/\n  |       |     \u2514\u2500\u2500\u2500&gt; 10283f238bc06de68f6a2ac63f3ec8c7a0dfef\n  |       \u251c\u2500\u2500\u2500&gt; info/\n  |       \u2514\u2500\u2500\u2500&gt; pack/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <pre><code>.git/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u251c\u2500\u2500\u2500&gt; objects/\n  |     \u251c\u2500\u2500\u2500&gt; 3c/\n  |     |     \u2514\u2500\u2500\u2500&gt; 6d5a4...\n  |     \u251c\u2500\u2500\u2500&gt; 19/\n  |     |     \u2514\u2500\u2500\u2500&gt; 10283f238bc06de68f6a2ac63f3ec8c7a0dfef\n  |     \u251c\u2500\u2500\u2500&gt; info/\n  |     \u2514\u2500\u2500\u2500&gt; pack/\n  \u251c\u2500\u2500\u2500&gt; ...\n  \u2514\u2500\u2500\u2500&gt; index\n</code></pre> <p>Details:</p> InitialWorkingIndex/StagingRepository <p>When you want to create <code>.git</code> file in your local project, you can use:</p> <pre><code>$ git init\n</code></pre> <p>Another command, <code>git init \u2013bare</code> will create and keep only configuration values of version control without source code.</p> <pre><code>$ echo -e \"Hello World\" &gt; demo.txt\n$ git hash-object demo.txt\n1910283f238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> <pre><code>$ git status\nOn branch main\n...\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to ...)\n      demo.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> <p>Note</p> <p>Remove all untracked files that was created in working, <code>git clean -f</code>.</p> AddRestoreResetRevert <pre><code>$ git add .\n$ git status\nOn branch main\n...\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n      newfile:    demo.txt\n</code></pre> <p>Restore Added file that was added from staging to working</p> <pre><code># Restore Added file that was committed to repository, to working\n$ git retore --staged .\n</code></pre> <p>Reset all files that was added to staging</p> <pre><code>$ git reset\n$ git prune\n</code></pre> <p>Revert files that was added to staging and delete files in <code>object/</code></p> <pre><code>$ git add .\n$ git rm --cached &lt;filename&gt;\n$ git prune\n</code></pre> <pre><code>$ git ls-files\ndemo.txt\n</code></pre> <p>Note</p> <p>The file in <code>object/</code> was compressed, and you will see value in this file when you use <code>zlib</code> like: <code>blob 11&lt;nil&gt;Hello World</code>.</p> <p>Note</p> <p><code>git add -p &lt;file&gt;</code>, this command will split changed file to hunks of code for review and what to do with that hunk,</p> <p>such as <code>y n q a d / j J g e ?</code>.</p> <ul> <li><code>-y</code>: (yes) for add that hunk to staged zone</li> <li><code>-n</code>: (no) ignore to add this hunk</li> <li><code>-q</code>: (quit) quit this interactive with mode <code>git add -p</code></li> <li><code>-s</code>: (split) divide this hunk to smaller hunks</li> </ul> <p>Sub-Folder</p> <p>This command, <code>git add :/</code>, using when you stay in sub-folder of working area and want to add all changed files include outside to staged status.</p> CommitRestoreReset <pre><code>$ git commit -m \"initial\"\n[main (root-commit) 3c6d5a4] initial\n1 file changed, 1 insertion(+)\ncreate mode 100644 demo.txt\n</code></pre> <p>Restore Changed file that was committed to repository</p> <pre><code>$ git restore .\n</code></pre> <p>Re-position of HEAD to hash commit that was committed to repository</p> <pre><code>$ git reset --hard &lt;commit-hash&gt;\n</code></pre> <pre><code># Get type of Git file\n$ git cat-file -t 3c6d\ncommit\n\n# Get content of Git file\n$ git cat-file -p 3c6d\ntree a611c6cc...\nauthor ...\n...\n\n$ git cat-file -p a611\n100644 blob 1910283f238bc06de68f6a2ac63f3ec8c7a0dfef    demo.txt\n</code></pre> <p>Note</p> <p>In commit hash file, Git will add <code>parent</code> information if you add new commit after first commit.</p> <pre><code>$ git cat-file -p 463f\ntree 9f3dc...\nparent 3c6d5a4...\n...\n</code></pre> <p>Note</p> <p>The <code>git commit --amend</code> or <code>git commit --amend -m \"YOUR MESSAGE\"</code> command use for create new commit replace the latest commit.</p> <pre><code>$ git commit -am \"&lt;message&gt;\"\n$ git add .\n$ git commit --amend\n</code></pre> <p>This solution use only local repository, before push branch to remote repository. If it is not any changed file to Staged, this command will allow you to edit the latest Commit Message.</p> <p>Note</p> <p><code>git ldm</code> for list history commits that you do in local repository before daily standup meeting.</p>"},{"location":"tools/git/#common-command","title":"Common Command","text":""},{"location":"tools/git/#git-hash","title":"Git Hash","text":"<p>Git have build-in hash function that use for create file name in Git.</p> <pre><code>$ git hash-object &lt;filname&gt;.&lt;file-extension&gt;\naf5b63bf238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> Manual Hash <pre><code>$ echo -n \"&lt;type:[blob, tree, commit]&gt; &lt;content-length&gt;\\0&lt;content-information&gt;\" \\\n  | shasum\naf5b63bf238bc06de68f6a2ac63f3ec8c7a0dfef\n</code></pre> <p>You see the above hash process, Git does not create hash file name from the real filename, but it uses content information in the file.</p>"},{"location":"tools/git/#git-config","title":"Git Config","text":"<p>The <code>git config</code> will override config values in local from global configuration.</p> ConfigGlobalLocal <pre><code>$ git config user.name \"username\"\n$ git config --get user.name\n$ git config --list\n</code></pre> <pre><code>$ git config --global user.name \"username\"\n$ git config --global user.email \"username@email.com\"\n$ git config --list\n</code></pre> <pre><code>$ git config --local user.name \"username\"\n$ git config --local user.email \"username@email.com\"\n$ git config --list\n</code></pre> Config Editor <p>Configuration in Git can use: <code>git config</code> command line. The simple way to edit Git configuration is using editor like VS Code, Atom, or Vim by Git default.</p> <pre><code># Set Editor in Git config when you use -e option.\n$ git config --global core.editor \"code -w\"\n$ git config --global -e\nhint: Waiting for your editor to close the file...\n</code></pre> <p>Note</p> <p>If you want to set Atom editor, use</p> <pre><code>$ git config --global core.editor \"atom --wait\"\n</code></pre>"},{"location":"tools/git/#git-log","title":"Git Log","text":"<pre><code># Set Disable Git Log pager\n$ git config --globlal pager.log false\n$ git log\ncommit 3c6d5a4... (HEAD -&gt; main)\nAuthor: ...\n...\n</code></pre> <p>Filtering your commit history:</p> <ul> <li>by date - <code>--before</code> or <code>--after</code></li> <li>by message - <code>--grep</code></li> <li>by author - <code>--author</code></li> <li>by file - <code>-- ${filename}</code></li> <li>by branch - <code>${branch-name}</code></li> </ul> <pre><code>$ git log --after=\"2021-7-1\"\n$ git log --after=\"2021-7-1\" --before=\"2021-6-5\"\n$ git log --grep=\"refactor\"\n</code></pre>"},{"location":"tools/git/#git-diff","title":"Git Diff","text":"<pre><code>$ git diff 073c HEAD\n...\nindex 2a3ee71..84f5955 100644\n...\n</code></pre> Use <code>difftool</code> <pre><code>$ git config --global diff.tool vscode\n$ git config --global difftool.vscode.cmd \"code -w -d \\$LOCAL \\$REMOTE\"\n$ git difftool --staged\n...\nViewing (1/1): '&lt;filename&gt;'\nLauch 'vscode' [Y/n]?\n\n$ git config --global difftool.prompt false\n$ git difftool --staged\n</code></pre>"},{"location":"tools/git/#git-branch","title":"Git Branch","text":"ListCreateDelete <pre><code>$ git branch\n</code></pre> <p>Options:</p> <ul> <li><code>-a</code>, <code>git branch -a</code>: List all branchs on local and remote</li> </ul> <pre><code># Create new branch\n$ git branch &lt;branch-name&gt;\n\n# Create new branch and switch HEAD to this branch\n$ git checkout -b &lt;branch-name&gt;\n\n# Support on Git version &gt;= 2.2.3\n$ git switch -c &lt;branch-name&gt;\n</code></pre> <pre><code># Delete branch that does not any new commit\n$ git branch -d &lt;branch_name&gt;\n\n# Delete branch that was created new commit\n$ git branch -D &lt;branch_name&gt;\n$ git reflog expire --expire-unreachable=now --all\n$ git prune\n</code></pre> <p>Note</p> <p><code>git cleanup</code> for delete branches in local repository that was merged.</p> <p>Note</p> <p>If you want to check out previous branch, you can use: <code>git checkout -</code>.</p>"},{"location":"tools/git/#git-checkout","title":"Git Checkout","text":"<pre><code># Revert all modified files to clean status\n$ git checkout .\n\n# Switch to previous branch\n$ git checkout -\n\n# Revert to specific commit\n$ git checkout \"&lt;commit&gt;\"\n\n# Revert to specific file and commit\n$ git checkout \"&lt;commit&gt;\" &lt;file-name&gt;\n\n# Create new branch in local repository from existing branch in remote repository\n$ git checkout -b feature/xxx origin/feature/xxx\n</code></pre>"},{"location":"tools/git/#git-tag","title":"Git Tag","text":"<p>Git Tags are used to capture the specific point in the history that is further used to point to a released version. A tag does not change like a branch.</p> <pre><code>$ git log --oneline\n816998a &lt;commit-message&gt;\n7c576ab &lt;commit-message&gt;\ndd9a333 stable\n...\n\n# Switch HEAD to that commit\n$ git switch --detach dd9a333\n$ git tag alpha\n$ git switch --detach alpha\n</code></pre> <pre><code>$ git tag \"&lt;tag-name&gt;\" \"&lt;commit&gt;\"\n$ git show \"&lt;tag-name&gt;\"\n$ git tag --list\n$ git tag -d \"&lt;tag-name&gt;\"\n</code></pre> <p>Note</p> <ul> <li>Annotated tags - <code>git tag -a '&lt;tag-name&gt;' -m '&lt;message&gt;' HEAD</code></li> <li>Lightweight tags - <code>git tag &lt;tag-name&gt;</code></li> </ul> <pre><code>$ git ls-remote --tags\n$ git ls-remote --tags origin\n$ git push --delete origin \"&lt;tag-name&gt;\"\n$ git push origin :refs/tags/\"&lt;tag-name&gt;\"\n</code></pre> <p>NOTE: If you want to push tag on local repository to remote, you will use <code>git push my_remote \u2013tags</code></p>"},{"location":"tools/git/#git-stash","title":"Git Stash","text":"<p>The <code>git stash</code> does hide all changes, stash the changes in a dirty working directory away. The local repository will be clean because <code>git stash</code> will tell HEAD commit hash revert to any commit (the latest commit always dirty).</p> StashListApply <pre><code>$ touch file.txt\n$ git add .\n$ git stash\nSaved working directory and index state WIP on gh-pages: fe163ee update and done\n</code></pre> <p>Note</p> <p>You can change the name of stash by this command: <code>git stash save \"&lt;name&gt;\"</code>, and stash included untracked file: <code>git stash -u</code>.</p> <pre><code>$ git stash list\nstash@{0}: WIP on gh-pages: fe163ee update and done\n\n$ ls file.txt\nls: cannot access 'file.txt': No such file or directory\n</code></pre> <pre><code>$ git stash apply\n$ ls file.txt\nfile.txt\n\n$ git stash drop stash@{0}\n</code></pre> <p>Note</p> <p>Above command, <code>git stash apply stash@{0}</code>, apply the latest stash to working area and staged area but difference with <code>git stash pop</code> because <code>pop</code> will delete stash history in list.</p>"},{"location":"tools/git/#git-remote","title":"Git Remote","text":"Set RemotePush to RemoteFetch from RemotePull from Remote <pre><code>$ git remote add origin https://github.com/username/myproject.git\n$ git remote -v\norigin  https://github.com/username/myproject.git (fetch)\norigin  https://github.com/username/myproject.git (push)\n</code></pre> <pre><code>$ git push -u origin master\nEnumerating objects: 7, done.\n...\nTo https://github.com/username/myproject.git\n * [new branch]      master -&gt; master\nBranch 'master' set up to track remote branch 'master' from 'origin'.\n</code></pre> <p>Warning</p> <p>The command <code>git push origin master --force</code> for force push to remote repository that mean it does not use <code>git pull origin</code> command before push.</p> <p>Note</p> <p>Above remote is use <code>http</code> access, so it always pass username/password after <code>git push</code> command. The another way is use ssh for connect to remote.</p> <pre><code>$ git fetch\n...\nFrom https://github.com/username/myproject.git\n   5a30adf..ad*****  master     -&gt; origin/master\n\n$ git status\nOn branch master\nYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n\nnothing to commit, working tree clean\n</code></pre> <pre><code>$ git pull\nUpdating 5a30adf..adfa804\nFast-forward\n &lt;filename&gt; | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n</code></pre>"},{"location":"tools/git/#git-fork","title":"Git Fork","text":"<p>Git Fork is the GitHub's feature for task owner of remote repository, it seems like clone but the repository will be yours. If you want to sync update from original repository to yours repository, you will add <code>upstream</code>.</p> <pre><code>$ git remote add upstream https://github.com/&lt;original_owner&gt;/&lt;original_repo&gt;.git\n$ git remote -v\norigin  git@github.com:Phonbopit/bootstrap.git (fetch)\norigin  git@github.com:Phonbopit/bootstrap.git (push)\nupstream    git@github.com:Phonbopit/bootstrap.git (fetch)\nupstream    git@github.com:Phonbopit/bootstrap.git (push)\n</code></pre> <pre><code>$ git fetch upstream\n$ git checkout master\n$ git merge upstream/master\n</code></pre>"},{"location":"tools/git/#advance-command","title":"Advance Command","text":""},{"location":"tools/git/#git-revert","title":"Git Revert","text":"<p>Revert specific commit to new commit</p> <pre><code>$ git revert \"&lt;commit&gt;\"\n$ git revert -m 1 \"&lt;commit&gt;\"\n\n# Revert merge change\n$ git reset --hard HEAD^\n</code></pre>"},{"location":"tools/git/#git-merge-rebase","title":"Git Merge &amp; Rebase","text":"<p>Merging vs Rebasing</p> MergeMerge SquashMerge Rebase <ul> <li>Will keep all commits history of the feature branch and move them into the master branch</li> <li>Will add extra dummy commit.</li> </ul> <pre><code>$ git switch main\n$ git merge dev\n</code></pre> <ul> <li>Will group all feature branch commits into one commit then append it in the front of the master branch</li> <li>Will add extra dummy commit.</li> </ul> <pre><code>$ git merge --squash HEAD@{1}\n$ git checkout stable\n$ git merge --squash develop\n$ git commit -m \"squash develop\"\n</code></pre> <ul> <li>Will append all commits history of the feature branch in the front of the master branch</li> <li>Will NOT add extra dummy commit.</li> </ul> <pre><code># Rebase all commit from dev branch to main branch\n$ git switch dev\n$ git rebase main\n$ git rebase --continue\n$ git switch main\n$ git merge dev\n$ git branch -d dev\n</code></pre> <p>Note</p> <p>If you want to cancel the rebase process, you can use <code>git rebase --abort</code>.</p>"},{"location":"tools/git/#git-rebase-self","title":"Git Rebase Self","text":"<p>Instead, use it for cleaning up your local commit history before merging it into a shared team branch. <code>git rebase</code> will use for</p> <ul> <li>Change a commit message</li> <li>Delete/Reorder commits</li> <li>Combine multiple commits into one (squash)</li> <li>Edit/Split an existing commit into multiple new ones</li> </ul> <p>Warning</p> <p>Do NOT use Interactive Rebase on commits that you've already pushed/shared on a remote repository.</p> <pre><code>$ git rebase -i HEAD~3\npick adfa804 Update config.yaml\npick 81529d3 update config.yaml\npick 3d8cc7a Update config.yaml\n\n# Rebase 5a30adf..ae76b2e onto 5a30adf (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n:q!\n\n$ git rebase --abort\n</code></pre> <pre><code>$ git log --oneline\nae76b2e (HEAD -&gt; master, origin/master) USE OURS ON CONFLICT FILE\n81529d3 update config.yaml\n3d8cc7a Update config.yaml\nadfa804 Update config.yaml\n5a30adf ADD CONFIG\n4e133da YOUR MESSAGE\n</code></pre> <p>Note</p> <p>When you want to control commit with rebase, <code>git rebase master --interactive</code></p> <p>Note</p> <p>If you want to squash all commit together, you can use <code>git merge --squash \"&lt;branch&gt;\"</code> command. (Or <code>git rebase -i --autosquash</code>)</p>"},{"location":"tools/git/#git-conflict","title":"Git Conflict","text":"<p>Note</p> <p>Above command can use opposite option, like <code>git checkout --theirs config.yaml</code>. Or use it together with merge strategy,</p> <ul> <li><code>git merge --strategy-option ours</code></li> <li><code>git merge --strategy-option theirs</code></li> </ul>"},{"location":"tools/git/#git-cherry-pick","title":"Git Cherry Pick","text":"<p>If you want to take some commit (such as bug fix commit) from another branch to your branch (Allows you to select individual commits to be integrated).</p> <p>Cherry Pick fixed file from dev to main branch</p> <pre><code>$ git add &lt;fix-filename&gt;\n$ git commit -m \"bug fix\"\n$ git switch main\n$ git log dev --oneline\n&lt;commit-hash&gt; (dev) bug fix\n...\n$ git cherry-pick \"&lt;commit-hash&gt;\"\n</code></pre> <p>Note</p> <p>git does not that delete commit from source branch and in your branch will be exists that commit in new id.</p>"},{"location":"tools/git/#git-submodules","title":"Git Submodules","text":"<p>Git Submodule help you to develop main project together with subproject and separate the commits of subproject from main project.</p> <p>The file <code>.submodule</code> is configuration of <code>git submodule</code> that keep project\u2019s URL, local subdirectory, and subproject branches that was tracked.</p> AddClonePushDelete <pre><code>$ git submodule add -b master https://github.com/username/module sub-project\n$ git status\nOn branch master\nYour branch is up to date with 'origin/master'.\nChanges to be committed:\n  (use \"git reset HEAD &lt;file&gt;...\" to unstage)\n      new file:   .gitmodules\n      new file:   sub-project\n\n$ cd sub-project\nsub-project$ ls\nREADME.md ...\n</code></pre> <pre><code>$ cat .gitmodules\n[submodule \"sub-project\"]\n    path = sub-project\n    url = https://github.com/username/sub-project.git\n    branch = master\n</code></pre> <p>Note</p> <p>When you already push, you will see submodule in main project repository and the submodule does not keep the source code but it keep hash commit number of submodule.</p> <pre><code>$ git clone https://github.com/username/mainproject.git\n$ cd sub-project\nsub-project$ ls\n\n\nsub-project$ git submodule init\nsub-project$ git submodule update\nCloning into '/username/mainproject/sub-project'...\nSubmodule path 'sub-project': checked out 'a5635f67626c1c224e733fe407aaa132b5e5d1e3\n</code></pre> <p>Note</p> <p>The <code>git clone --recurse-submodules \"&lt;repository-url&gt;\"</code> is auto initialize and update submodules.</p> <p><code>git submodule update --init --recursive</code></p> <pre><code>$ cd sub-project/\nsub-project$ git fetch\nsub-project$ git merge origin/master\nAlready up to date.\n</code></pre> <p>Note</p> <p><code>git submodule update --remote</code> will auto fetch and merge with track branch. If you do not set trank, you will use</p> <pre><code>username/myproject$ git config -f .gitmodules submodule.sub-project.branch develop\nusername/myproject$ cat .gitmodules\n[submodule \"sub-project\"]\n path = sub-project\n url = https://github.com/username/sub-project.git\n branch = develop\n</code></pre> <p>Optional, <code>git submodule update --remote --merge</code> or <code>git submodule update --remote --rebase</code></p> <pre><code>username/myproject/sub-project$ git commit -am 'update readme.md'\nusername/myproject/sub-project$ git push\n\nusername/myproject/sub-project$ cd ..\nusername/myproject$ git commit -am 'update submodule'\nusername/myproject$ git push\n</code></pre> <p>NOTE: You can push along with the main project, where the submodule is pushed before the main project is pushed using the command <code>git push \u2014 recurse-submodules=on-demand</code></p> <pre><code>username/myproject$ git submodule deinit -f sub-project\n&gt; Cleared directory 'sub-project'\n&gt; Submodule 'sub-project' (https://github.com/username/sub-project.git) unregistered for path 'sub-project'\n\nusername/myproject$ rm -rf .git/modules/sub-project\nusername/myproject$ git rm --cached sub-project\n&gt; rm 'sub-project'\n\nusername/myproject$ rm .gitmodules\nusername/myproject$ git commit -am \"REMOVED submodule\"\n&gt; [master 66b1703] removed submodule\n&gt;  2 files changed, 5 deletions(-)\n&gt;  delete mode 160000 sub-project\n\nusername/myproject$ git push origin master\n</code></pre>"},{"location":"tools/git/#git-reflog","title":"Git Reflog","text":"<p>A protocol of HEAD Pointer movements. Reflog is a mechanism to record when the tip of branches is updated. This command is to manage the information recorded in it.</p> <p>\u0e43\u0e0a\u0e49\u0e41\u0e2a\u0e14\u0e07 Log \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e43\u0e19 HEAD \u0e02\u0e2d\u0e07 Local Repository \u0e21\u0e31\u0e19\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e39\u0e0d\u0e2b\u0e32\u0e22\u0e44\u0e1b</p> <pre><code>username/myproject$ git reflog --all\n&gt; ae76b2e (HEAD -&gt; master, origin/master) HEAD@{0}: rebase -i (abort): updating HEAD\n&gt; 81529d3 HEAD@{1}: rebase -i (start): checkout HEAD~3\n&gt; ae76b2e (HEAD -&gt; master, origin/master) refs/remotes/origin/master@{0}: update by push\n&gt; ae76b2e (HEAD -&gt; master, origin/master) refs/heads/master@{0}: commit (merge): USE OURS ON CONFLICT FILE\n&gt; ae76b2e (HEAD -&gt; master, origin/master) HEAD@{2}: commit (merge): USE OURS ON CONFLICT FILE\n&gt; 81529d3 refs/heads/master@{1}: commit: update config.yaml\n&gt; 81529d3 HEAD@{3}: commit: update config.yaml\n&gt; adfa804 refs/heads/master@{2}: reset: moving to HEAD~1\n&gt; :\n</code></pre>"},{"location":"tools/git/#git-blame","title":"Git Blame","text":"<p>Use to track change inline of file.</p> <pre><code>$ git blame config.yaml\n5a30adf5 (usernaem    2022-03-06 13:52:04 +0700 1) env:\nadfa8044 (usernaemDEV 2022-03-06 19:30:39 +0700 2)   develop: \"/dev\"\nadfa8044 (usernaemDEV 2022-03-06 19:30:39 +0700 3)   remote: \"/remote\"\n81529d32 (usernaem    2022-03-07 11:19:06 +0700 4)   production: \"/prod\"\n</code></pre>"},{"location":"tools/git/#git-bisect","title":"Git Bisect","text":""},{"location":"tools/git/#start-bisect","title":"Start bisect","text":"<pre><code>username/myproject$ git bisect start\nusername/myproject$ git bisect bad \"&lt;commit-start&gt;\"\nusername/myproject$ git bisect good \"&lt;commit-end&gt;\"\n</code></pre>"},{"location":"tools/git/#unittest-until-the-latest-commit-exists","title":"Unittest until the latest commit exists","text":"<pre><code>username/myproject$ run app\nusername/myproject$ git bisect bad\n&gt; Bisecting: 7 revisions left to test after this (roughly 3 steps)\n&gt; [12sasa53261sdfas4235sdab721c2405abv0*****] COMMIT MESSAGE\n\n...\n\nusername/myproject$ run app\nusername/myproject$ git bisect bad\n&gt; Bisecting: 0 revisions left to test after this (roughly 0 steps)\n&gt; [jk1p1634sda78v93kla13asdfscc23140030*****] COMMIT MESSAGE\n\nusername/myproject$ git bisect bad\n&gt; 5a30adf5d4f184fd4586891e26d6826ab66***** COMMIT MESSAGE\n&gt; commit 5a30adf5d4f184fd4586891e26d6826ab66*****\n&gt; Author: username &lt;username@email.com&gt;\n&gt; Date: Sun Jan 01 00:00:00 1999 +0700\n&gt;\n&gt;     COMMIT MESSAGE\n\nusername/myproject$ git show 5a30adf5d4f184fd4586891e26d6826ab66*****\n</code></pre>"},{"location":"tools/git/#if-found-the-bug-success-you-should-exit-bisect-first","title":"If found the bug success, you should exit bisect first","text":"<pre><code>username/myproject$ git bisect reset\n</code></pre> <p>Note</p> <p><code>git bisect log</code> will show the log of bisect, should use this command before reset.</p>"},{"location":"tools/git/#references","title":"References","text":"<ul> <li>TODO -  Git commands I wish I knew earlier as a developer</li> <li>Frequently Used Git Commands</li> <li> Git is your Friend</li> </ul>"},{"location":"tools/git/git-branching-strategies/","title":"Branching Strategies","text":"<p> Git - Branching Strategies Explained</p>"},{"location":"tools/git/git-branching-strategies/#data-engineer-branches","title":"Data Engineer branches","text":"<p>Read More</p>"},{"location":"tools/git/git-branching-strategies/#the-mainline-branches","title":"The Mainline branches","text":"<p>It's the simplest but most effective way for small teams. Because of working only on the main branch. So this main branch has to be deployed/published all the time.</p> <p>For each developer will work on his local branch. If a develop process was finished, it would merge a code to main branch.</p> <ul> <li>master</li> </ul> <p>The source code of <code>HEAD</code> always reflects a production-ready   state.</p> <ul> <li>develop</li> </ul> <p>The source code of <code>HEAD</code> always reflects a state with the   latest delivered development changes for the next release.</p> <p>Note</p> <p>Sometime we will split tag version in <code>master</code> to <code>stable</code> for keeping only release tag change.</p> <p>Warning</p> <p>For each feature, it should be small work. If it has long or more code change, it will affect to main branch and has more conflict code.</p>"},{"location":"tools/git/git-branching-strategies/#the-supporting-branches","title":"The Supporting branches","text":""},{"location":"tools/git/git-branching-strategies/#feature-branches","title":"Feature branches","text":"<p>Manage code in separate branches for each feature. When developers complete development and testing, they merge the code from the feature branch into the integration branch. After that, another round of testing is conducted on the integration branch. When everything is ready, the code is merged back into the main branch.</p> <p>Therefore, the state of the code on the main branch is always ready for deployment/release.</p> <ul> <li>May branch off from: <code>develop</code></li> <li>Must merge back into: <code>develop</code></li> <li>Branch naming convention: anything except <code>master</code>, <code>develop</code>, <code>release-*</code>,   <code>support-*</code>, or <code>hotfix-*</code></li> </ul> <p>Warning</p> <p>Things to be cautious about include the lifespan of feature branches or prolonged development. Maintaining them becomes increasingly difficult.</p> <p>Example</p> <p>Creating a feature branch:</p> <pre><code>$ git checkout -b myfeature develop\nSwitched to a new branch \"myfeature\"\n</code></pre> <p>Incorporating a finished feature on develop:</p> <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff myfeature\nUpdating ea1b82a..05e9557\n(Summary of changes)\n\n$ git branch -d myfeature\nDeleted branch myfeature (was 05e9557).\n\n$ git push origin develop\n</code></pre> <p>Info</p> <p>The <code>--no-ff</code> flag causes the merge to always create a new commit object, even if the merge could be performed with a fast-forward. This avoids losing information about the historical existence of a feature branch and groups together all commits that together added the feature.</p>"},{"location":"tools/git/git-branching-strategies/#environment-branches","title":"Environment branches","text":"<ul> <li>May branch off from: <code>master</code></li> <li>Must merge back into: <code>Next environment</code></li> <li>Branch naming convention: <code>testing</code>, <code>production</code>, <code>staging</code>,   <code>pre-production</code></li> </ul>"},{"location":"tools/git/git-branching-strategies/#release-branches","title":"Release branches","text":"<p>Separate according to each version of the system. However, the main work remains in the main branch.</p> <p>But the problem that follows is the management and maintenance in each branch or release, which, first of all, requires editing and merging into the main branch and then using the Git feature called Cherry Pick.</p> <p>To pick up the changes to different branches or releases. And if you make changes in each branch or release, you must also Cherry Pick them to the main branch.</p> <p>This policy or working method is called Upstream First.</p> <ul> <li>May branch off from: <code>develop</code></li> <li>Must merge back into: <code>develop</code> and <code>master</code></li> <li>Branch naming convention: <code>release-*</code></li> </ul> <p>Example</p> <p>Creating a release branch:</p> <pre><code>$ git checkout -b release-1.2 develop\nSwitched to a new branch \"release-1.2\"\n\n$ ./bump-version.sh 1.2\nFiles modified successfully, version bumped to 1.2.\n\n$ git commit -a -m \"Bumped version number to 1.2\"\n[release-1.2 74d9424] Bumped version number to 1.2\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <p>Finishing a release branch:</p> <pre><code>$ git checkout master\nSwitched to branch 'master'\n\n$ git merge --no-ff release-1.2\nMerge made by recursive.\n(Summary of changes)\n\n$ git tag -a 1.2\n</code></pre> <p>You might as well want to use the -s or -u  flags to sign your tag cryptographically. <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff release-1.2\nMerge made by recursive.\n(Summary of changes)\n\n$ git branch -d release-1.2\nDeleted branch release-1.2 (was ff452fe).\n</code></pre>"},{"location":"tools/git/git-branching-strategies/#hotfix-branches","title":"Hotfix branches","text":"<ul> <li>May branch off from: <code>master</code></li> <li>Must merge back into: <code>develop</code> and <code>master</code></li> <li>Branch naming convention: <code>hotfix-*</code></li> </ul> <p>Example</p> <p>Creating the hotfix branch:</p> <pre><code>$ git checkout -b hotfix-1.2.1 master\nSwitched to a new branch \"hotfix-1.2.1\"\n\n$ ./bump-version.sh 1.2.1\nFiles modified successfully, version bumped to 1.2.1.\n\n$ git commit -a -m \"Bumped version number to 1.2.1\"\n[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <pre><code>$ git commit -m \"Fixed severe production problem\"\n[hotfix-1.2.1 abbe5d6] Fixed severe production problem\n5 files changed, 32 insertions(+), 17 deletions(-)\n</code></pre> <p>Finishing a hotfix branch:</p> <pre><code>$ git checkout master\nSwitched to branch 'master'\n\n$ git merge --no-ff hotfix-1.2.1\nMerge made by recursive.\n(Summary of changes)\n\n$ git tag -a 1.2.1\n</code></pre> <p>You might as well want to use the -s or -u  flags to sign your tag cryptographically. <pre><code>$ git checkout develop\nSwitched to branch 'develop'\n\n$ git merge --no-ff hotfix-1.2.1\nMerge made by recursive.\n(Summary of changes)\n</code></pre> <p>The one exception to the rule here is that, when a release branch currently exists, the hotfix changes need to be merged into that release branch, instead of <code>develop</code>. Back-merging the bugfix into the release branch will eventually result in the bugfix being merged into <code>develop</code> too, when the release branch is finished. (If work in <code>develop</code> immediately requires this bugfix and cannot wait for the release branch to be finished, you may safely merge the bugfix into <code>develop</code> now already as well.)</p> <pre><code>$ git branch -d hotfix-1.2.1\nDeleted branch hotfix-1.2.1 (was abbe5d6).\n</code></pre>"},{"location":"tools/git/git-branching-strategies/#support-branches","title":"Support branches","text":"<ul> <li>May branch off from: <code>tag</code></li> <li>Must merge back into: <code>None</code></li> <li>Branch naming convention: <code>support-*</code></li> </ul> <p>Example</p> <p>Creating the support branch:</p> <pre><code>$ git checkout -b support-1.2.1.1 v1.2.1\nSwitched to a new branch \"support-1.2.1.1\"\n\n$ ./bump-version.sh 1.2.1.1\nFiles modified successfully, version bumped to 1.2.1.1.\n\n$ git commit -a -m \"Bumped version number to 1.2.1.1\"\n[support-1.2.1.1 13ds7e2] Bumped version number to 1.2.1.1\n1 files changed, 1 insertions(+), 1 deletions(-)\n</code></pre> <p>Support branches would be created \u201con demand\u201d when requested by customers who are stuck on legacy releases and are not able to move forward to current releases, but need security and other bug fixes.</p>"},{"location":"tools/git/git-branching-strategies/#references","title":"References","text":"<ul> <li>Successful Git Branch Model</li> </ul>"},{"location":"tools/git/git-commit-release/","title":"Git Commit Release","text":"<p>This topic will talk about how to write commit message in best practice way. That mean it will reuse this message for tracking and grouping changes for <code>CHANGELOG.md</code> file.</p>"},{"location":"tools/git/git-commit-release/#common-message","title":"Common message","text":"<p>A git commit common message should have a format like:</p> <pre><code>$ git commit -am \"&lt;type&gt;(&lt;scope&gt;): &lt;short-summary-in-present-tense&gt;\n&lt;BLANK LINE&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\"\n</code></pre> <code>&lt;short-summary&gt;</code> <p>use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" - don't capitalize first letter - no dot (.) at the end</p> <code>&lt;type&gt;</code> <ul> <li><code>feat</code>: Features : A new feature.</li> <li><code>fix</code>: Bugfixes : A bug fix.</li> <li><code>docs</code>: Documents : Documentation changes.</li> <li><code>style</code>: Style : Changes that do not affect the meaning of the code   (white-space, formatting, missing semi-colons, etc).</li> <li><code>refactor</code>: Refactor : A code change that neither fixes a bug nor adds a feature.</li> <li><code>perf</code>: Improved performance : A code change that improves performance.</li> <li><code>test</code>: Testing : Changes to the test framework.</li> <li><code>build</code>: Build : Changes to the build process or tools.</li> <li><code>dep</code>: Dependencies and Removals : Changes or update dependencies.</li> </ul> <code>&lt;scope&gt;</code> <p>An optional keyword that provides context for where the change was made. It can be anything relevant to your package or development workflow (e.g., it could be the module or function name affected by the change).</p> <p>Different text in the commit message will trigger PSR to make different kinds of releases:</p> <p>A <code>&lt;type&gt;</code> of fix triggers a patch version bump, e.g.</p> <pre><code>git commit -m \"fix(mod_plotting): fix confusing error message in plot_words\"\n</code></pre> <p>A <code>&lt;type&gt;</code> of feat triggers a minor version bump, e.g.</p> <pre><code>git commit -m \"feat(package): add example data and new module to package\"\n</code></pre> <p>The text <code>BREAKING CHANGE:</code> in the footer will trigger a major release, e.g.</p> <pre><code>git commit -m \"feat(mod_plotting): move code from plotting module to pycounts module\n\nBREAKING CHANGE: plotting module won't exist after this release.\"\n</code></pre> <code>&lt;body&gt;</code> <p>Just as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.</p> <code>&lt;footer&gt;</code> <p>The footer should contain any information about Breaking Changes and is also the place to reference GitHub issues that this commit closes.</p> <p>Breaking Changes should start with the word <code>BREAKING CHANGE:</code> with a space or two newlines.</p> <p>Note</p> <p>Any line of the commit message cannot be longer than 100 characters. This allows the message to be easier to read on GitHub as well as in various git tools.</p>"},{"location":"tools/git/git-commit-release/#revert-message","title":"Revert message","text":"<p>If the commit reverts a previous commit, it should begin with <code>revert:</code>, followed by the header of the reverted commit.</p> <p>In the body it should say: <code>This reverts commit &lt;hash&gt;.</code>, where the hash is the SHA of the commit being reverted.</p> <p>The git revert command will undo only the changes associated with a specific commit</p>"},{"location":"tools/git/git-commit-release/#changelogmd","title":"CHANGELOG.md","text":"<p>the CHANGELOG file like release note for developer can see changes history for any tag version.</p> <p>If we set up the commit message and already create release for our project, we can generate a <code>CHANGELOG.md</code> file for show the tracking change histories.</p> <p>We will use <code>git log</code> command to show commit message histories.</p> bash script<pre><code>#!/usr/bin/env bash\n# refs: https://stackoverflow.com/questions/40865597/generate-changelog-from-commit-and-tag\nprevious_tag=0\nfor current_tag in $(git tag --sort=-creatordate)\ndo\nif [ \"$previous_tag\" != 0 ]; then\n    tag_date=$(git log -1 --pretty=format:'%ad' --date=short ${previous_tag})\n    printf \"## ${previous_tag} (${tag_date})\\n\\n\"\n    git log ${current_tag}...${previous_tag} \\\n      --pretty=format:'*  %s [View](https://bitbucket.org/projects/test/repos/my-project/commits/%H)' \\\n      --reverse | grep -v Merge\n    printf \"\\n\\n\"\nfi\nprevious_tag=${current_tag}\ndone\n</code></pre> <pre><code>sh change-log-builder.sh &gt; CHANGELOG.md\n</code></pre> CHANGELOG.md<pre><code>## v1.1.0 (2017-08-29)\n\n- Adds IPv6 support [View](http...)\n- Adds TreeMaker class and its test. [View](http...)\n\n## v1.0.9 (2017-08-22)\n\n- Updates composer.json.lock [View](http...)\n\n## v1.0.8 (2017-08-22)\n\n- Adds S3Gateway as substitute class [View](http...)\n- Remove files no more used [View](http...)\n</code></pre> <p>In the <code>CHANGELOG.md</code> file that will group by typing,</p> CHANGELOG.md<pre><code>## Version 0.1.0\n\n...\n\n**Features**\n\n- `#2094 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2094&gt;`\\_\n  Add `response()` method for closing a stream in a handler\n- `#2097 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2097&gt;`\\_\n  Allow case-insensitive HTTP Upgrade header\n- `#2104 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2104&gt;`\\_\n  Explicit usage of CIMultiDict getters\n- `#2109 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2109&gt;`\\_\n  Consistent use of error loggers\n- `#2114 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2114&gt;`\\_\n  New `client_ip` access of connection info instance\n- `#2133 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2133&gt;`\\_\n  Implement new version of AST router\n\n  - Proper differentiation between `alpha` and `string` param types\n  - Adds a `slug` param type, example: `&lt;foo:slug&gt;`\n\n**Bugfixes**\n\n- `#2119 &lt;https://github.com/&lt;organize&gt;/&lt;project&gt;/pull/2119&gt;`\\_\n  Fix classes on instantiation for `Config`\n\n...\n</code></pre>"},{"location":"tools/git/git-commit-release/#references","title":"References","text":"<ul> <li>Angular Developers Commit</li> <li>Python Package: 07 Releasing Versioning</li> </ul>"},{"location":"tools/git/git-hooks/","title":"Git Hooks","text":"<p><code>git hooks</code> \u0e0a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e17\u0e35\u0e48 Git \u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01 \u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e2b\u0e25\u0e31\u0e07\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e2b\u0e25\u0e31\u0e01\u0e43\u0e14\u0e46 \u0e40\u0e0a\u0e48\u0e19 commit, push</p> <p>Git hooks \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e15\u0e34\u0e14\u0e15\u0e31\u0e27\u0e21\u0e32\u0e01\u0e31\u0e1a Git \u0e2d\u0e22\u0e39\u0e48\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e44\u0e1b\u0e14\u0e32\u0e27\u0e42\u0e2b\u0e25\u0e14\u0e2d\u0e30\u0e44\u0e23\u0e21\u0e32\u0e25\u0e07\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e41\u0e25\u0e30 Git hooks \u0e19\u0e31\u0e49\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e08\u0e33\u0e17\u0e33\u0e07\u0e32\u0e19\u0e41\u0e1a\u0e1a local \u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e19\u0e46\u0e19\u0e31\u0e49\u0e19\u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19</p> <p>Note</p> <pre><code>$ git config --global core.hooksPath /path/to/my/centralized/hooks\n$ git config --local core.hooksPath /path/to/my/centralized/hooks\n</code></pre> <p>Let\u2019s have a look what kind of local hooks we have in our repository\u2019s <code>.git/hooks</code> folder :</p> <pre><code>.git/hooks\n  \u251c\u2500\u2500 applypatch-msg.sample\n  \u251c\u2500\u2500 commit-msg.sample\n  \u251c\u2500\u2500 post-update.sample\n  \u251c\u2500\u2500 pre-applypatch.sample\n  \u251c\u2500\u2500 pre-commit.sample\n  \u251c\u2500\u2500 prepare-commit-msg.sample\n  \u251c\u2500\u2500 pre-rebase.sample\n  \u2514\u2500\u2500 update.sample\n</code></pre>"},{"location":"tools/git/git-hooks/#post-receive","title":"Post Receive","text":"<p><code>git config receive.denycurrentbranch ignore</code></p> <p>post-receive \u0e08\u0e30\u0e17\u0e33\u0e2b\u0e19\u0e49\u0e32\u0e17\u0e35\u0e48\u0e04\u0e37\u0e2d \u0e40\u0e21\u0e37\u0e48\u0e2d push \u0e40\u0e02\u0e49\u0e32 origin master \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e23 code \u0e08\u0e30\u0e16\u0e39\u0e01 update \u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34 \u0e41\u0e25\u0e30\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e20\u0e32\u0e22\u0e43\u0e19 ./git/hooks/post-receive \u0e01\u0e47\u0e08\u0e30\u0e16\u0e39\u0e01\u0e23\u0e31\u0e19</p> <p>\u0e42\u0e14\u0e22\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e43\u0e19 post-receive \u0e08\u0e30\u0e17\u0e33\u0e07\u0e32\u0e19\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e21\u0e35\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07 git push \u0e14\u0e31\u0e07\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32\u0e01\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e08\u0e30\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01 push \u0e42\u0e04\u0e49\u0e14\u0e40\u0e2a\u0e23\u0e47\u0e08</p> Scenario 01: trigger jenkinsScenario 02Scenario 03 <p>\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e21\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 job \u0e43\u0e19 Jenkins \u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2a\u0e31\u0e48\u0e07\u0e43\u0e2b\u0e49 Jenkins \u0e23\u0e31\u0e19\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07 build project \u0e1c\u0e48\u0e32\u0e19\u0e25\u0e34\u0e07\u0e04\u0e4c\u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e25\u0e34\u0e07\u0e04\u0e4c\u0e08\u0e30\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a \u0e25\u0e34\u0e07\u0e04\u0e4c\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e31\u0e48\u0e07 build project \u0e43\u0e19 Jenkins http://jenkins-server/job/projectname/build</p> <pre><code>#!/bin/sh\ncurl http://jenkins-server/job/projectname/build\n</code></pre> <p>\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e1a\u0e2d\u0e01\u0e27\u0e48\u0e32 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e21\u0e35\u0e01\u0e32\u0e23 git push \u0e40\u0e02\u0e49\u0e32\u0e21\u0e32 \u0e43\u0e2b\u0e49\u0e2a\u0e48\u0e07 source code \u0e44\u0e1b\u0e22\u0e31\u0e07 /var/www/domain.com</p> <pre><code>#!/bin/sh\ngit --work-tree=/var/www/domain.com --git-dir=/var/repo/site.git checkout -f\n</code></pre> <pre><code>#!/bin/sh\ngit checkout -f\ntouch restart.txt\n</code></pre>"},{"location":"tools/git/git-hooks/#jira-with-git","title":"Jira with Git","text":"<p>Reference: Hook story ID from Jira</p> <p>Jira is software management tool. \u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 Development life cycle \u0e41\u0e1a\u0e1a\u0e44\u0e2b\u0e19\u0e01\u0e47\u0e41\u0e25\u0e49\u0e27\u0e41\u0e15\u0e48 \u0e40\u0e0a\u0e48\u0e19 Agile, Scrum, Waterfall \u0e42\u0e14\u0e22\u0e41\u0e15\u0e48\u0e25\u0e30\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32 Ticket</p> <p>\u0e16\u0e49\u0e32\u0e2a\u0e31\u0e07\u0e40\u0e01\u0e15\u0e17\u0e38\u0e01 ticket \u0e08\u0e30\u0e21\u0e35 auto-increment number \u0e43\u0e2b\u0e49\u0e40\u0e0a\u0e48\u0e19 JIRA-1 JIRA-2 JIRA-3 \u0e0b\u0e36\u0e48\u0e07\u0e43\u0e19\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e19\u0e35\u0e49\u0e08\u0e30\u0e02\u0e2d\u0e40\u0e23\u0e35\u0e22\u0e01\u0e2b\u0e21\u0e32\u0e22\u0e40\u0e25\u0e02\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19\u0e19\u0e35\u0e49\u0e27\u0e48\u0e32 \u201cStory-ID\u201d (\u0e1a\u0e32\u0e07\u0e17\u0e35\u0e48\u0e2d\u0e32\u0e08\u0e08\u0e30\u0e40\u0e23\u0e35\u0e22\u0e01 Ticket-ID, Jira-ID, etc.)</p> <p>\u0e43\u0e19\u0e40\u0e2b\u0e15\u0e38\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e08\u0e23\u0e34\u0e07\u0e2a\u0e21\u0e21\u0e15\u0e34\u0e27\u0e48\u0e32\u0e21\u0e35 branch \u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e17\u0e33\u0e07\u0e32\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e46\u0e01\u0e31\u0e19 4 branch \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 branch \u0e40\u0e01\u0e34\u0e14 commit \u0e02\u0e36\u0e49\u0e19\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 5\u201320 commits \u0e16\u0e49\u0e32\u0e2a\u0e21\u0e21\u0e15\u0e34\u0e27\u0e48\u0e32 merge \u0e23\u0e27\u0e21\u0e01\u0e31\u0e19 \u0e08\u0e30\u0e40\u0e01\u0e34\u0e14\u0e1b\u0e31\u0e0d\u0e2b\u0e32 commit \u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 4 branch \u0e08\u0e30\u0e16\u0e39\u0e01\u0e19\u0e33\u0e21\u0e32\u0e40\u0e23\u0e35\u0e22\u0e07\u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 (timestamp) \u0e41\u0e25\u0e49\u0e27\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e40\u0e23\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e08\u0e30 trace \u0e01\u0e25\u0e31\u0e1a\u0e44\u0e1b\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e04\u0e37\u0e2d \u0e22\u0e32\u0e01\u0e41\u0e25\u0e30\u0e04\u0e48\u0e2d\u0e19\u0e02\u0e49\u0e32\u0e07\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e27\u0e25\u0e32</p> <p>\u0e43\u0e19\u0e1a\u0e32\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e08\u0e36\u0e07\u0e19\u0e34\u0e22\u0e21\u0e17\u0e35\u0e48\u0e08\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e2a\u0e48 <code>[STORY-ID] Commit message</code> Story-ID \u0e15\u0e48\u0e2d\u0e2b\u0e19\u0e49\u0e32\u0e0a\u0e37\u0e48\u0e2d\u0e04\u0e2d\u0e21\u0e21\u0e34\u0e17\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e07\u0e48\u0e32\u0e22\u0e15\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e41\u0e01\u0e30\u0e23\u0e2d\u0e22\u0e22\u0e49\u0e2d\u0e19\u0e01\u0e25\u0e31\u0e1a</p> <p>\u0e0b\u0e36\u0e48\u0e07\u0e43\u0e19\u0e1a\u0e17\u0e04\u0e27\u0e32\u0e21\u0e19\u0e35\u0e49\u0e40\u0e23\u0e32\u0e08\u0e30\u0e40\u0e08\u0e32\u0e30\u0e25\u0e07\u0e44\u0e1b\u0e43\u0e0a\u0e49 hook \u0e17\u0e35\u0e48\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32 \u201cprepare-commit-msg\u201d \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19 hook \u0e17\u0e35\u0e48\u0e08\u0e30\u0e16\u0e39\u0e01\u0e40\u0e23\u0e35\u0e22\u0e01\u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e2a\u0e31\u0e48\u0e07 commit -m \u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e2a\u0e48 message \u0e19\u0e31\u0e48\u0e19\u0e40\u0e2d\u0e07 \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48 hook \u0e19\u0e35\u0e49\u0e08\u0e30\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e33 message \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e43\u0e2a\u0e48\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32 modify \u0e01\u0e48\u0e2d\u0e19\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e02\u0e35\u0e22\u0e19\u0e25\u0e07\u0e44\u0e1b\u0e44\u0e14\u0e49</p>"},{"location":"tools/git/git-hooks/#create-script-in-githooksprepare-commit-msgsh","title":"Create script in <code>.git/hooks/prepare-commit-msg.sh</code>","text":"<p>we simply rename <code>prepare-commit-msg.sample</code> to <code>prepare-commit-msg</code>, paste the script listed below and ensure that the file is executable.</p>"},{"location":"tools/git/git-hooks/#example01","title":"Example01","text":"<pre><code>#!/bin/bash\n\n# This way you can customize which branches should be skipped when\n# prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n   BRANCHES_TO_SKIP=(master develop test)\nfi\nBRANCH_NAME=$(git symbolic-ref --short HEAD)\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n   sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre> <p>NOTE: Reference code: https://gist.github.com/bartoszmajsak/1396344</p> <pre><code>#!/bin/bash\n\n# For instance with feature/add_new_feature_HEYT-653\n# $ git commit -m\"Fixed bug\"\n# will result with commit \"[HEYT-653] Fixed bug\"\n\n\n# Customize which branches should be skipped when prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n  BRANCHES_TO_SKIP=(master develop test)\nfi\n\nBRANCH_NAME=$(git symbolic-ref --short HEAD | grep -o '[A-Z]\\+-[0-9]\\+')\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\n\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\n\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n  sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre> <p>NOTE:</p> <pre><code>BRANCH_NAME=$(git rev-parse --abbrev-ref HEAD 2&gt; /dev/null | grep -oE \"[A-Z]+-[0-9]+\")\nif [ -n \"$BRANCH_NAME\" ]; then\n    echo \"[$BRANCH_NAME] $(cat $1)\" &gt; $1\nfi\n</code></pre> <pre><code>#!/bin/bash\n\n# This way you can customize which branches should be skipped when\n# prepending commit message.\nif [ -z \"$BRANCHES_TO_SKIP\" ]; then\n  BRANCHES_TO_SKIP=(master develop test)\nfi\n\nBRANCH_NAME=$(git symbolic-ref --short HEAD)\nBRANCH_NAME=\"${BRANCH_NAME##*/}\"\n\nBRANCH_EXCLUDED=$(printf \"%s\\n\" \"${BRANCHES_TO_SKIP[@]}\" | grep -c \"^$BRANCH_NAME$\")\nBRANCH_IN_COMMIT=$(grep -c \"\\[$BRANCH_NAME\\]\" $1)\n\nif [ -n \"$BRANCH_NAME\" ] &amp;&amp; ! [[ $BRANCH_EXCLUDED -eq 1 ]] &amp;&amp; ! [[ $BRANCH_IN_COMMIT -ge 1 ]]; then\n  sed -i.bak -e \"1s/^/[$BRANCH_NAME] /\" $1\nfi\n</code></pre>"},{"location":"tools/git/git-hooks/#deployment","title":"Deployment","text":"<p>Reference: How to setup deployment with Git</p> <p>https://engineerball.com/blog/2014/05/05/%e0%b9%83%e0%b8%8a%e0%b9%89-git-hook-%e0%b9%80%e0%b8%9e%e0%b8%b7%e0%b9%88%e0%b8%ad-deploy-code.html</p> <p>https://www.imooh.com/git-hook-trigger-jenkins-build-job</p>"},{"location":"tools/git/git-hooks/#bump-version","title":"Bump-Version","text":"<p>The bump version is the script that auto update <code>CHANGELOG.md</code> file with all commit messages after the latest bump version or first initial commit.</p> <p>General digit of version naming, like <code>v1.2.3</code>, will mean <code>v${Major}.${Minor}.${Patch}.${Support}</code>, reference from semver</p> <p>Generate commit message:</p> <pre><code>$ git log --pretty=format:\"  - %s\" \"v$BASE_STRING\"...HEAD\n</code></pre> <p>Auto generate version (Only increment last number):</p> <pre><code>$ $(eval VERSION=$(`shell git describe --tags --abbrev=0 | awk -F. '{OFS=\".\"; $NF+=1; print $0}'`))\n$ git add .\n$ git commit -m \"$m\"\n$ git push origin master\n$ git tag -a $(VERSION) -m \"new release\"\n$ git push origin $(VERSION)\n</code></pre> <p>Examples of Bump-version script:</p> <ul> <li>BUMP-VERSION-shell</li> <li>BUMP-VERSION-shell-v2</li> </ul>"},{"location":"tools/git/git-scenarios/","title":"Scenarios","text":"<p>This topic will provide solutions for Git scenario that you want to manage your Git working find after do something mistake.</p>"},{"location":"tools/git/git-scenarios/#reset-removed-file","title":"Reset Removed File","text":"<pre><code>$ rm -r README.md\n$ git status\nOn branch master\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> Delete in WorkingDelete in StagedDelete in Local Repo <pre><code>$ git checkout README.md\nUpdated 1 path from the index\n\n$ git status\nOn branch master\nnothing to commit, working tree clean\n</code></pre> <p>Note</p> <p>Same as <code>git restore --worktree README.md</code></p> <pre><code>$ git add .\n$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        deleted:    README.md\n</code></pre> <pre><code>$ git restore --source=HEAD --staged --worktree README.md\n$ git status\nOn branch master\nnothing to commit, working tree clean\n</code></pre> <p>Note</p> <p>If want to restore to before add to git will use <code>git restore --staged READMD.md</code></p> <pre><code>$ git add .\n$ git commit -m \"ACCIDENT COMMIT\"\n[master 134bf16] ACCIDENT COMMIT\n 1 file changed, 1 deletion(-)\n delete mode 100644 README.md\n\n$ git log\ncommit 5882d01f453beecfec3f252141a9f1bd0761fc35 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    ACCIDENT COMMIT\n\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <code>--soft</code> option<code>--hard</code> option <pre><code>$ git reset --soft HEAD~1\n$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        deleted:    README.md\n\nusername/myproject$ git log\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <pre><code>$ git reset --hard HEAD~1\nHEAD is now at 4e133da YOUR MESSAGE\n\n$ git status\nOn branch master\nnothing to commit, working tree clean\n\nusername/myproject$ git log\ncommit 4e133da8b2b77cf9755095bf967d8be78a70c7b9 (HEAD -&gt; master)\nAuthor: username &lt;username@email.com&gt;\nDate:   Sun Jan 01 00:00:00 1999 +0700\n\n    YOUR MESSAGE\n</code></pre> <p>Note</p> <p><code>git remove --cached README.md</code></p>"},{"location":"tools/git/git-scenarios/#reset-main-branch","title":"Reset <code>main</code> Branch","text":"Deleting <code>.git</code>Deleting Branch <pre><code># Clone the project, e.g. `myproject` is my project repository\n$ git clone https://github/username/myproject.git\n$ cd myproject\n\n# Delete the `.git` folder\n$ git rm -rf .git\n\n# Now, re-initialize the repository\n$ git init\n$ git remote add origin https://github.com/heiswayi/myproject.git\n$ git remote -v\n\n# Add all the files and commit the changes\n$ git add --all\n$ git commit -am \"Initial commit\"\n\n# Force push update to the main branch of our project repository\n$ git push -f origin main\n</code></pre> <p>Deleting the <code>.git</code> folder may cause problems in our git repository. If we want to delete all of our commits history, but keep the code in its current state.</p> <pre><code># Check out to a temporary branch\n$ git checkout --orphan \"TEMP_BRANCH\"\n\n# Add all the files\n$ git add -A\n\n# Commit the changes\n$ git commit -am \"Initial commit\"\n\n# Delete the old branch\n$ git branch -D main\n\n# Rename the temporary branch, \"TEMP_BRANCH\" to main\n$ git branch -m main\n\n# Finally, force update to our repository\n$ git push -f origin main\n</code></pre>"},{"location":"tools/git/git-scenarios/#revert-multi-merge-commits","title":"Revert Multi-Merge Commits","text":"<pre><code>git checkout -b &lt;new-branch-name&gt; &lt;revert-commit-id&gt;\ngit merge --strategy=ours main\ngit checkout main\ngit merge &lt;new-branch-name&gt;\n</code></pre>"},{"location":"tools/lang/go/","title":"GO","text":"<p>https://github.com/codebangkok/golang</p>"},{"location":"tools/lang/go/#go-linter","title":"GO Linter","text":"<p>https://github.com/pallat/uber-go-style-guide-th</p>"},{"location":"tools/lang/go/#package","title":"Package","text":"<p>https://github.com/avelino/awesome-go</p>"},{"location":"tools/lang/go/#learning","title":"Learning","text":"<p>https://levelup.gitconnected.com/learning-go-part-six-command-line-processing-and-config-files-a27d0b7acc0a</p>"},{"location":"tools/lang/go/go-command/","title":"GO Basic Command Line","text":"<p>Table of Content:</p>"},{"location":"tools/lang/go/go-command/#installation-and-setup","title":"Installation and Setup","text":"<ul> <li>Create <code>go.mod</code> file to your project</li> </ul> <pre><code>$ mkdir go-basic &amp;&amp; cd go-basic\n$ go mod init go-basic\n</code></pre> <ul> <li>Create go file and <code>main</code> package</li> </ul> <pre><code>package main\n\nfunc main() {\n    println(\"Hello World\")\n}\n</code></pre> <ul> <li>Import format</li> </ul> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Printf(\"Hello %v\\n\", \"World\")\n}\n</code></pre> <p>Warning: \\ The GO is separate end of line with <code>;</code> and fix the <code>{}</code> syntax.</p> <ul> <li>Test run main package</li> </ul> <pre><code>go run main.go\n</code></pre>"},{"location":"tools/lang/go/go-command/#declare-in-go","title":"Declare in GO","text":"<p><code>var</code> is mutable and <code>const</code> is immutable</p> <pre><code>var x int  // declared but not used\n</code></pre> <pre><code>var x int\n_ = x\n</code></pre> <pre><code>var x int = 10\nprint(x)\n</code></pre> <ul> <li>Short declaration</li> </ul> <pre><code>y := 10\n</code></pre> <p>Note: \\ All variable in GO have default value, in GO it call zero value.</p> <p>Note: \\ When to use short declaration</p> <pre><code>package main\n\nvar x int\n\nfunc main() {\n    y := 10\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#if-statement","title":"If Statement","text":"<pre><code>point := 50\nif point &gt;= 50 &amp;&amp; point &lt;= 100 {\n    println(\"Pass\")\n} else if point &gt;= 20 {\n    println(\"Not Pass\")\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#for-loop","title":"For Loop","text":"<pre><code>values := []int{1, 2, 3, 4, 5}\n\nfor i := 0; i &lt; len(values); i ++ {\n    println(values[i])\n}\n</code></pre> <pre><code>values := []int{10, 20, 30, 40}\n\nfor i, v := range values {\n    println(i, v)\n}\n</code></pre> <pre><code>0 10\n1 20\n2 30\n3 40\n</code></pre> <p>Note: \\ If you do not want to use index in for-each statement, you can use</p> <pre><code>for _, v := range values {\n    println(V)\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#while-loop","title":"While Loop","text":"<pre><code>values := []int{1, 2, 3, 4, 5}\n\ni := 0\nfor i &lt; len(values) {\n    println(values[i])\n    i++\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#array","title":"Array","text":"<pre><code>// array of int with size 3\nvar x [3]int = [3]int{1, 2, 3}\n\nx := [3]int{1, 2, 3}\n\nx := [...]int{1, 2, 3, 4}\n\n// Create slice array\nx := []int{1, 2, 3}\nx = append(x, 4)\n</code></pre>"},{"location":"tools/lang/go/go-command/#map","title":"Map","text":"<pre><code>var countries map[string]string\n\ncountries := map[string]string{}\ncountries[\"th\"] = \"Thailane\"\n\ncountry, ok := countries[\"jp\"]\nif !ok {\n    println(\"No value\")\n    return\n}\n</code></pre>"},{"location":"tools/lang/go/go-command/#function","title":"Function","text":"<p>The scope of function is in <code>package</code></p> <pre><code>func main() {\n    c, _ := add(10, 20)\n    println(c)\n}\n\nfunc add(a, b int) (int, string) {\n    return a + b, \"Hello\"\n}\n</code></pre> <pre><code>x := func(a, b int) int {\n    return a + b\n}\nvalue := x(10, 20)\nprintln(value)\n</code></pre> <pre><code>func sum(a ...int) int {\n    s := 0\n    for _, v := range a {\n        s += v\n    }\n    return s\n}\n\ns := sum(1, 2, 3, 4, 5)\nprintln(s)\n</code></pre>"},{"location":"tools/lang/go/go-command/#high-order-function","title":"High-order Function","text":"<pre><code>func cal(f func(int,int)int) {\n    sumn := f(50, 10)\n    println(sum)\n}\n\ncal(func(a, b int) int {\n    return a + b\n})\n</code></pre>"},{"location":"tools/lang/go/go-command/#package","title":"Package","text":"<pre><code>project\n    |-- customer\n    |   |- customer.go\n    |- go.mod\n    |- main.go\n</code></pre> <pre><code>// customer.go\npackage customer\n\nvar Name = \"Customer\"\n</code></pre> <pre><code>// main.go\nimport \"project/customer\"\n\nfunc main() {\n    println(customer.Name)\n}\n</code></pre> <p>Warning: \\ In GO, if you want to export anything from another package, you should use pascal case like, <code>func Sum()</code> or <code>var Name =</code></p>"},{"location":"tools/lang/go/go-command/#pointer","title":"Pointer","text":"<pre><code>var x, y int\nx = 10\ny = x\n\n// return pointer in memory\nprintln(&amp;x)\nprintln(&amp;y)\n</code></pre> <pre><code>0xc0000140e0\n0xc0000ae010\n</code></pre> <pre><code>var x int\nx = 10\n\nvar y *int\ny = &amp;x\n\n// return pointer in memory\nprintln(&amp;x)\nprintln(y)\n</code></pre> <pre><code>0xc000122008\n0xc000122008\n</code></pre> <p>Note: \\ If you want to use value of pointer <code>y</code>, you can use <code>println(*y)</code>.</p>"},{"location":"tools/lang/go/go-command/#struct","title":"Struct","text":"<p>Struct is a class without method in GO. Attribute in GO say with behavior.</p> <pre><code>type Person struct {\n    Name string\n    Age int\n}\n\nfunc main() {\n    x := Person{\"Tom\", 18}\n    y := Person{Name: \"Tom\", Age: 18}\n    z := Person{\n        Name: \"Tom\",\n        Age: 18,\n    }\n\n    println(x.Name)\n    println(y.Age)\n}\n</code></pre> <ul> <li>Create Extension Method</li> </ul> <pre><code>func Hello(p Person) string {\n    return \"Hello \" + p.Name\n}\n\nprintln(Hello(x))\n</code></pre> <pre><code>func (p Person) Hello() string {\n    return \"Hello \" + p.Name\n}\n\nprintln(Hello(x))\nprintln(x.Hello())\n</code></pre> <ul> <li>Create OOP in Struct</li> </ul> <pre><code>package customer\n\ntype Person struct {\n    name string\n    age int\n}\n\nfunc (p Person) GetName() string {\n    return p.name\n}\n\nfunc (p *Person) SetName() string {\n    p.name = name\n}\n\nfunc (p Person) GetAge() int {\n    return p.age\n}\n</code></pre> <pre><code>x := customer.Person{}\nx.SetName(\"Tom\")\nprintln(x.GetName())\n</code></pre> <pre><code>Tom\n</code></pre>"},{"location":"tools/lang/go/go-command/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=JbIS97exQnQ</li> <li>https://github.com/avelino/awesome-go</li> </ul>"},{"location":"tools/lang/go/go-connect-kafka/","title":"GO Advance with Kafka","text":"<p>Table of Contents:</p> <ul> <li>Setup Kafka</li> <li>Create Simple Consumer with GO</li> <li>Create Simple Producer with GO</li> </ul>"},{"location":"tools/lang/go/go-connect-kafka/#setup-kafka","title":"Setup Kafka","text":"<pre><code>version: \"3.9\"\nservices:\n  zookeeper:\n    image: zookeeper\n    container_name: zookeeper\n    volumes:\n      - ./zookeeper:/data\n\n  kafka:\n    image: bitnami/kafka\n    container_name: kafka\n    ports:\n      - \"9092:9092\"\n    volumes:\n      - ./kafka:/bitnami/kafka/data\n    environment:\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n    depends_on:\n      - zookeeper\n</code></pre> <pre><code>$ docker compose up -d\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-send-some-data-to-kafka-server","title":"Test Send some data to Kafka Server","text":"<ul> <li>List topics on your Kafka server</li> </ul> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --list\n</code></pre> <ul> <li>Create <code>demo</code> topic on your Kafka server</li> </ul> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --topic=demo --create\n</code></pre> <ul> <li>Open consumer to <code>demo</code> topic</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 --topic=demo\n</code></pre> <ul> <li>Send some data by producer to <code>demo</code> topic</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello world\n</code></pre> <pre><code>$ kafka-console-consumer ...\nhello world\n</code></pre> <ul> <li>Open consumer to <code>demo</code> topic with separate by groups</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --topic=demo \\\n  --group=data\n</code></pre> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --topic=demo \\\n  --group=log\n</code></pre> <ul> <li>Open consumer to <code>test</code>, and <code>demo</code> topics</li> </ul> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --include=\"demo|test\" \\\n  --group=log\n</code></pre> <ul> <li>Send some data by producer to different topics</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello demo\n\n$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello test\n</code></pre> <pre><code>$ kafka-console-consumer ... --include=\"demo|test\" ...\nhello demo\nhello test\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-simple-consumer-with-go","title":"Create Simple Consumer with GO","text":"<pre><code>go mod init consumer\ngo get github.com/Shopify/sarama\n</code></pre> <p>See the partition of <code>demo</code> topic</p> <pre><code>$ kafka-topics --bootstrap-server=localhost:9092 --topic=demo --describe\n</code></pre> <pre><code>package main\n\nfunc main() {\n    servers := []string{\"localhost:9092\"}\n\n    consumer, err := sarama.NewConsumer(servers, nil)\n    if err != nil {\n        panic(err)\n    }\n    defer consumer.Close()\n\n    partitionConsumer, err := consumer.ConsumePartition(\"demo\", 0, sarama.OffsetNewest)\n    if err != nil {\n        panic(err)\n    }\n    defer partitionConsumer.Close()\n\n    for {\n        select {\n            case err := &lt;- partitionConsumer.Errors():\n                fmt.Println(err)\n            case msg := &lt;- partitionConsumer.Messages():\n                fmt.Println(string(msg.Value))\n        }\n    }\n\n}\n</code></pre> <pre><code>$ go run .\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-receive-data-from-go-consumer","title":"Test receive data from GO Consumer","text":"<pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=demo\n&gt;hello Go Consumer\n</code></pre> <p>Response from Go Consumer will see:</p> <pre><code>$ go run .\nhello Go Consumer\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-simple-producer-with-go","title":"Create Simple Producer with GO","text":"<pre><code>go mod init producer\ngo get github.com/Shopify/sarama\n</code></pre> <pre><code>package main\n\nfunc main() {\n    servers := []string{\"localhost:9092\"}\n\n    producer, err := sarama.NewSyncProducer(servers, nil)\n    if err != nil {\n        panic(err)\n    }\n    defer producer.Close()\n\n    msg := sarama.ProducerMessage{\n        Topic: \"demo\",\n        Value: sarama.StringEncoder(\"Hello from Go Producer\"),\n    }\n\n    p, o, err := produser.SendMessage(&amp;msg)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(\"partition=%v, offset=%v\", p, o)\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#test-send-data-from-go-producer","title":"Test send data from GO Producer","text":"<pre><code># producer\ngo run .\npartition=0, offset=4%\n</code></pre> <pre><code># consumer\ngo run .\nHello from Go Producer\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-events-with-go","title":"Create Events with GO","text":"<pre><code>go init events\n</code></pre> <pre><code>package events\n\nimport \"reflect\"\n\nvar Topics = []string{\n    reflect.TypeOf(OpenAccountEvent{}).Name(),\n    reflect.TypeOf(DepositFundEvent{}).Name(),\n    reflect.TypeOf(WithdrawFundEvent{}).Name(),\n    reflect.TypeOf(CloseAccountEvent{}).Name(),\n}\n\ntype Event interface {\n\n}\n\ntype OpenAccountEvent struct {\n    ID              string\n    AccountHolder   string\n    AccountType     int\n    OpeningBalance  float64\n}\n\ntype DepositFundEvent struct {\n    ID              string\n    Amount          float64\n}\n\ntype WithdrawFundEvent struct {\n    ID              string\n    Amount          float64\n}\n\ntype CloseAccountEvent struct {\n    ID              string\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-advance-consumer-with-go","title":"Create Advance Consumer with GO","text":"<pre><code>## config.yaml\nkafka:\n  servers:\n    - localhost:9092\n  group: accountConsumer\n\ndb:\n  driver: mysql\n  host: localhost\n  port: 3306\n  username: root\n  password: P@ssw0rd\n  database: demo\n</code></pre> <ul> <li>Import <code>events</code> module from local</li> </ul> <pre><code>// go.mod\n...\nreplace events =&gt; ../events\n...\n</code></pre> <pre><code>go get events\n</code></pre> <pre><code>// repositories/account.go\npackage repositories\n\n...\n\ntype BankAccount struct {\n    ID              string\n    AccountHolder   string\n    AccountType     int\n    Balance         float64\n}\n\ntype AccountRepository interface {\n    Save(bankAccount BankAccount) error\n    Delete(id string) error\n    FindAll() (bankAccounts []BankAccount, err error)\n    FindByID(id string) (bankAccount BankAccount, err error)\n}\n\ntype accountRepository struct {\n    db *gorm.DB\n}\n\nfunc NewAccountRepository(db *gorm.DB) AccountRepository {\n    db.Table(\"demo_banks\").AutoMigrate(&amp;BankAccount{})\n    return accountRepository{db}\n}\n\nfunc (obj accountRepository) Save(bankAccount BankAccount) error {\n    return obj.db.Table(\"demo_banks\").Save(bankAccount).Error\n}\n\nfunc (obj accountRepository) Save(id string) error {\n    return obj.db.Table(\"demo_banks\").Where(\"id=?\", id).Delete(&amp;BankAccount{}).Error\n}\n\nfunc (obj accountRepository) FindAll() (bankAccounts []BankAccount, err error) {\n    err = obj.db.Table(\"demo_banks\").Find(&amp;bankAccounts).Error\n    return bankAccounts, err\n}\n\nfunc (obj accountRepository) FindByID(id string) (bankAccount BankAccount, err error) {\n    err = obj.db.Table(\"demo_banks\").Where(\"id=?\", id).First(&amp;bankAccount).Error\n    return bankAccount, err\n}\n</code></pre> <pre><code>// services/account.go\npackage services\n\ntype EventHandler interface {\n    Handle(topic string, eventBytes []byte)\n}\n\ntype accountEventHandler struct {\n    accountRepo repositories.AccountRepository\n}\n\nfunc NewAccountEventHandler(accountRepo repositories.AccountRepository) EventHandler {\n    return accountEventHandler{accountRepo}\n}\n\nfunc (obj accountEventHandler) Handle(topic string, eventBytes []byte) {\n    switch topic {\n        case reflect.TypeOf(OpenAccountEvent{}).Name():\n            event := events.OpenAccountEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount := repositories.BankAccount {\n                ID:             event.ID,\n                AccountHolder:  event.AccountHolder,\n                AccountType:    event.AccountType,\n                Balance:        event.OpenBalance,\n            }\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(DepositFundEvent{}).Name():\n            event := events.DepositFundEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount, err := obj.accountRepo.FindByID(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount.Balance += event.Amount\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(WithdrawFundEvent{}).Name():\n            event := events.WithdrawFundEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount, err := obj.accountRepo.FindByID(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            bankAccount.Balance -= event.Amount\n            err = obj.accountRepo.Save(bankAccount)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        case reflect.TypeOf(CloseAccountEvent{}).Name():\n            event := events.CloseAccountEvent{}\n            err := json.Unmarshal(eventBytes, event)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            err = obj.accountRepo.Delete(event.ID)\n            if err != nil {\n                log.Println(err)\n                return\n            }\n            log.Println(event)\n        default:\n            log.Println(\"no event handler\")\n    }\n}\n</code></pre> <pre><code>// services/consumer.go\npackage services\n\ntype consumerHandler struct {\n    eventHandler EventHandler\n}\n\nfunc NewConsumerHandler(eventHandler EventHandler) sarama.ConsumerGroupHandler {\n    return consumerHandler{eventHandler}\n}\n\nfunc (obj consumerHandler) Setup(sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (obj consumerHandler) Cleanup(sarama.ConsumerGroupSession) error {\n    return nil\n}\n\nfunc (obj consumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n    for msg := range claim.Messages() {\n        obj.eventHandler.Handle(msg.Topic, msg.Value)\n        session.MarkMessage(msg, \"\")\n    }\n\n    return nil\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nfunc init() {\n    viper.SetConfigName(\"config\")\n    viper.SetConfigType(\"yaml\")\n    viper.AddConfigPath(\".\")\n    viper.AutomaticEnv()\n    viper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n    if err := viper.ReadConfig(); err != nil {\n        panic(err)\n    }\n}\n\nfunc initDatabase() *gorm.DB {\n    dsn := fmt.Sprintf(%v:%v@tcp(%v:%v)/%v,\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n\n    dial := mysql.Open(dsn)\n    db, err := gorm.Open(dial, &amp;gorm.Config{\n        Logger: logger.Default.LogMode(logger.Silent),\n    })\n    if err != nil {\n        panic(err)\n    }\n\n    return db\n}\n\nfunc main() {\n    consumer, err := sarama.NewConsumerGruop(viper.GetStringSlice(\"kafka.servers\"), viper.GetString(\"kafka.group\"), nil)\n    if err != nil {\n        panic(err)\n    }\n    defer consumer.Close()\n\n    db := initDatabase()\n    accountRepo := repositories.NewAccountRepository(db)\n    accountEventHandler := services.NewAccountEventHandler(accountRepo)\n    accountConsumerHandler := services.NewConsumerHandler(accountEventHandler)\n\n    fmt.Println(\"Account consumer started ...\")\n    for {\n        consumer.Consume(context.Backgroud(), event.Topics, accountConsumerHandler)\n    }\n}\n</code></pre> <pre><code>$ go run .\nAccount consumer started ...\n</code></pre> <ul> <li>List groups</li> </ul> <pre><code>$ kafka-consumer-groups --bootstrap-server=localhost:9092 --list\naccountConsumer\n...\n</code></pre> <ul> <li>Test receive data from manual producer</li> </ul> <pre><code>$ kafka-console-producer --bootstrap-server=localhost:9092 --topic=OpenAccountEvent\n&gt;{\"ID\": \"1\",\"AccountHolder\":\"Admin\",\"AccountType\":1,\"OpeningBalance\":1000}\n</code></pre> <pre><code>$ go run .\nAccount consumer started ...\n20**/**/01 00:00:00 &amp;{1 Admin 1 1000}\n</code></pre> <pre><code>$ kafka-console-consumer --bootstrap-server=localhost:9092 \\\n  --include=\"OpenAccountEvent|DepositFundEvent|WithdrawFundEvent|CloseAccountEvent\" \\\n  --group=log\n{\"ID\": \"1\",\"AccountHolder\":\"Admin\",\"AccountType\":1,\"OpeningBalance\":1000}\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#create-advance-consumer-with-go_1","title":"Create Advance Consumer with GO","text":"<pre><code>## config.yaml\nkafka:\n  servers:\n    - localhost:9092\n</code></pre> <ul> <li>Import <code>events</code> module from local</li> </ul> <pre><code>// go.mod\n...\nreplace events =&gt; ../events\n...\n</code></pre> <pre><code>// commands/command.go\npackage commands\n\ntype OpenAccountCommand struct {\n    AccountHolder   string\n    AccountType     int\n    OpeningBalance  float64\n}\n\ntype DepositFundCommand struct {\n    ID              string\n    Amount          float64\n}\n\ntype WithdrawFundCommand struct {\n    ID              string\n    Amount          float64\n}\n\ntype CloseAccountCommand struct {\n    ID              string\n}\n</code></pre> <pre><code>//services/producer.go\npackage services\n\ntype EventProducer interface {\n    Produce(event events.Event) error\n}\n\ntype eventProducer struct {\n    producer sarama.SyncProducer\n}\n\nfunc NewEventProducer(producer sarama.SyncProducer) EventProducer {\n    return eventProducer{producer}\n}\n\nfunc (obj eventProducer) Produce(event events.Event) error {\n    topic := reflect.TypeOf(event).Name()\n\n    value, err := json.Marshal(event)\n    if err != nil {\n        return err\n    }\n\n    msg := sarama.ProducerMessage{\n        Topic: topic,\n        Value: sarama.ByteEncoder(value),\n    }\n\n    _, _, err = obj.producer.SendMessage(&amp;msg)\n    if err != nil {\n        return err\n    }\n    return nil\n}\n</code></pre> <pre><code>$ go get github.com/google/uuid\n</code></pre> <pre><code>// services/account.go\npackage services\n\ntype AccountService interface {\n    OpenAccount(command commands.OpenAccountCommand) (id string, err error)\n    DepositFund(command commands.DepositFundCommand) error\n    WithdrawFund(command commands.WithdrawFundCommand) error\n    CloseAccount(command commands.CloseAccountCommand) error\n}\n\ntype accountService struct {\n    eventProducer EventProducer\n}\n\nfunc NewAccountService(eventProducer EventProducer) AccountService {\n    return accountService{eventProducer}\n}\n\nfunc (obj accountService) OpenAccount(command commands.OpenAccountCommand) (id string, err error) {\n\n    if command.AccountHolder == \"\" || command.AccountType == 0 || command.OpeningBalance == 0 {\n        return \"\", errors.New(\"bad request\")\n    }\n\n    event := events.OpenAccountEvent {\n        ID:             uuid.NewString(),\n        AccountHolder:  command.AccountHolder,\n        AccountType:    command.AccountType,\n        OpeningBalance: command.OpeningBalance,\n    }\n    log.Println(\"%#v\", event)\n    return event.ID, obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) DepositFund(command commands.DepositFundCommand) error {\n    if command.ID == \"\" || command.Amount == 0 {\n        return errors.New(\"bad request\")\n    }\n\n    event := events.DepositFundEvent{\n        ID:     command.ID,\n        Amount  command.Amount,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) WithdrawFund(command commands.WithdrawFundCommand) error {\n    if command.ID == \"\" || command.Amount == 0 {\n        return errors.New(\"bad request\")\n    }\n\n    event := events.WithdrawFundEvent{\n        ID:     command.ID,\n        Amount  command.Amount,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n\nfunc (obj accountService) CloseAccount(command commands.CloseAccountCommand) error {\n    if command.ID == \"\" {\n        return errors.New(\"bad request\")\n    }\n\n    event := event.CloseAccountEvent{\n        ID: command.ID,\n    }\n    log.Println(\"%#v\", event)\n    return obj.eventProducer.Produce(event)\n}\n</code></pre> <pre><code>$ go get github.com/gofiber/fiber/v2\n</code></pre> <pre><code>// controllers/account.go\npackage controllers\n\ntype AccountController interface {\n    OpenAccount(c *fiber.Ctx) error\n    DepositAccount(c *fiber.Ctx) error\n    WithdrawAccount(c *fiber.Ctx) error\n    CloseAccount(c *fiber.Ctx) error\n}\n\ntype accountController struct {\n    accountService services.AccountService\n}\n\nfunc NewAccountController(accountService services.AccountService) AccountController {\n    return accountController{accountService}\n}\n\nfunc (obj accountController) OpenAccount(c *fiber.Ctx) error {\n    command := command.OpenAccountCommand{}\n\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    id, err := obj.accountService.OpenAccount(command)\n    if err != nil {\n        return err\n    }\n\n    c.Status(fiber.StatusCreated)\n    return c.JSON(fiber.Map{\n        \"message\":  \"open account success\",\n        \"id\":       id,\n    })\n}\n\nfunc (obj accountController) DepositAccount(c *fiber.Ctx) error {\n    command := command.DepositFundCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.DepositFund(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"deposit fund success\",\n    })\n}\n\nfunc (obj accountController) WithdrawAccount(c *fiber.Ctx) error {\n    command := command.WithdrawFundCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.WithdrawFund(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"withdraw fund success\",\n    })\n}\n\nfunc (obj accountController) CloseAccount(c *fiber.Ctx) error {\n    command := command.CloseAccountCommand{}\n    err := c.BodyParser(&amp;command)\n    if err != nil {\n        return err\n    }\n\n    err := obj.accountService.CloseAccount(command)\n    if err != nil {\n        return err\n    }\n    return c.JSON(fiber.Map{\n        \"message\": \"close account success\",\n    })\n}\n</code></pre> <pre><code>// main.go\npackage main\n\ndef main() {\n    producer, err := sarama.NewSyncProducer(viper.GetStringSlice(\"kafka.servers\"), nil)\n    if err != nil {\n        panic(err)\n    }\n    defer producer.Close()\n\n    eventProducer := services.NewEventProducer(producer)\n    accountService := services.NewAccountService(eventProducer)\n    accountController := controllers.NewAccountController(accountService)\n\n    app := fiber.New()\n\n    app.Post(\"/openAccount\", accountController.OpenAccount)\n    app.Post(\"/depositFund\", accountController.DepositFund)\n    app.Post(\"/withdrawFund\", accountController.WithdrawFund)\n    app.Post(\"/closeAccount\", accountController.CloseAccount)\n\n    app.Liten(\":8000\")\n}\n</code></pre> <pre><code>$ curl -H 'content-type:application/json' localhost:8000/openaccount \\\n  -d '{\"AccountHolder\": \"Admin\", \"AccountType\": 1, \"OpeningBalance\": 1000}' \\\n  -i\n</code></pre>"},{"location":"tools/lang/go/go-connect-kafka/#references","title":"References","text":"<p>-</p>"},{"location":"tools/lang/go/go-connect-redis/","title":"GO Advance with Redis","text":"<p>Create project folder in your local,</p> <pre><code>goredis\n|--&gt; docker-compose.yml\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <p>Table of Contents:</p> <ul> <li>Setup Redis</li> <li>[]</li> <li>Create API Server with GO</li> </ul>"},{"location":"tools/lang/go/go-connect-redis/#setup-redis","title":"Setup Redis","text":"<ul> <li>Install Redis CLI, use <code>redis-cli</code></li> <li>Install Redis Server from Docker Container</li> </ul> <pre><code>version: \"3.9\"\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    ports:\n      - \"6379:6379\"\n</code></pre> <pre><code>docker-compose up\n</code></pre> <pre><code>$ redis-cli\n127.0.0.1:6379$ ping\nPONG\n127.0.0.1:6379$ set name value\nOK\n127.0.0.1:6379$ get name\n\"value\"\n</code></pre> <p>Note: \\ If you want to delete value in redis with life-cycle, you can use <code>ex</code> such as delete value in 5 seconds: <code>set name hello ex 5</code></p>"},{"location":"tools/lang/go/go-connect-redis/#persisted-data-from-redis","title":"Persisted Data from Redis","text":"<p>We want to config redis server to persisted data (snapshot data) when it has some problem take server down. First, we create data folder and file <code>config/redis.conf</code>:</p> <pre><code>goredis\n|--&gt; config\n|    |--&gt; redis.conf\n|--&gt; data\n|    |--&gt; redis\n|--&gt; ...\n</code></pre> <p>Copy configuration data with your version from redis official redis.io/topics/config document to this file.</p> <pre><code># Allow other network to connect redis\nbind 0.0.0.0\n\n# Disable Snapshot and use empty value\nsave \"\"\n\n# Enable Append-only mode\nappendonly yes\nappendfilename \"appendonly.aof\"\n</code></pre> <p>Edit the docker-compose file:</p> <pre><code># docker-compose.yml\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis\n    container_name: redis\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - ./data/redis:/data\n      - ./config/redis.conf:/redis.conf\n    command: redis-server /redis.conf\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#create-api-server-with-go","title":"Create API Server with GO","text":"<p>We use <code>fiber</code> for API server.</p> <pre><code>go get github.com/gofiber/fiber/v2\n</code></pre> <pre><code>// main.go\npackage main\n\nimport github.com/gofiber/fiber/v2\n\nfunc main() {\n    app := fiber.New()\n    app.Get(\"/hello\", func(c *fiber.Ctx) error {\n        return c.SendString(\"Hello World\")\n    })\n    app.Listen(\":8000\")\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#load-test","title":"Load Test","text":"<p>We use <code>k6</code>.</p> <pre><code>goredis\n|--&gt; ...\n|--&gt; scripts\n|    |--&gt; test.js\n|--&gt; ...\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    k6:\n        image: loadimpact/k6\n        container_name: k6\n        volumes:\n            - ./scripts:/scripts\n</code></pre> <pre><code>import http from \"k6/http\";\n\nexport default function () {\n  http.get(\"http://host.docker.internal:8000/hello\");\n}\n</code></pre> <ul> <li>Load test with 5 users and 5 seconds</li> </ul> <pre><code>docker compose run --rm k6 run /scripts/test.js -u5 -d5s\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#edit-configuration-in-test-file","title":"Edit configuration in test file","text":"<pre><code>import http from 'k6/http'\n\nexport let: options = {\n    vus: 5,\n    duration: '5s'\n}\n\nexport default function() {\n    http.get('http://host.docker.internal:8000/hello')\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#create-timeseries-database-and-grafana","title":"Create TimeSeries Database and Grafana","text":"<p>We use <code>InfluxDB</code> for keep data from <code>k6</code> and use Grafana to visualize test dashboard. If we use InfluxDB version &gt;= 2.0.0, it will support build-in dashboard.</p> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    k6:\n        ...\n        environment:\n            - K6_OUT=influxdb=http://influxdb:8086/k6\n        ...\n\n    influxdb:\n        image: influxdb:1.8.10  # Version 2.0.0 does not support for k6 yet\n        container_name: influxdb\n        environment:\n            - INFLUXDB_DB=k6\n            - INFLUXDB_HTTP_MAX_BODY_SIZE=0\n        ports:\n            - \"8086:8086\"\n        volumes:\n            - ./data/influxdb:/var/lib/influxdb\n\n    grafana:\n        image: grafana/grafana\n        container_name: grafana\n        environment:\n            - GF_AUTH_ANONYMOUS_ENABLED=true\n            - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin  # This is bad practice!!!\n        ports:\n            - \"3000:3000\"\n        volumes:\n            - ./data/grafana:/var/lib/grafana\n</code></pre> <pre><code>docker compose up influxdb grafana\n</code></pre> <p>After <code>Grafana</code> server run successful, we will go to <code>localhost:3000</code> and Add <code>InfluxDB</code> data source with these configurations:</p> <ul> <li>URL: http://influxdb:8086</li> <li>Database: k6</li> </ul> <p>Go to <code>grafana.com/grafana/dashboard</code> and search k6 dashboard. We will copy dashboard ID from official to Local Grafana server in import menu.</p>"},{"location":"tools/lang/go/go-connect-redis/#use-case","title":"Use-case","text":"<pre><code>goredis\n|--&gt; ...\n|--&gt; handlers\n|--&gt; repositories\n|    |--&gt; product_redis.go\n|    |--&gt; product_db.go\n|    |--&gt; product.go\n|--&gt; services\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <pre><code>go get gorm.io/gorm\ngo get gorm.io/driver/mysql\ngo get github.com/go-redis/redis/v8\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.9'\nservices:\n\n    ...\n\n    mariadb:\n        image: mariadb\n        container_name: mariadb\n        environment:\n            - MARIADB_ROOT_PASSWORD=P@ssw0rd\n            - MARIADB_DATABASE=product\n        ports:\n            - \"3306:3306\"\"\n        volumes:\n            - ./data/mariadb:/var/lib/mysql\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#repositories","title":"Repositories","text":"<pre><code>// repositories/product.go\npackage repositories\n\n...\n\ntype product struct {\n    ID          int\n    Name        string\n    Quantity    int\n}\n\ntype ProductRepository interface {\n    GetProducts() ([]product, error)\n}\n\nfunc mockData(db *gorm.DB) error {\n\n    var count int64\n    db.model(&amp;product{}).Count(&amp;count)\n    if count &gt; 0 {\n        return nil\n    }\n\n    seed := rand.NewSource(time.Now().UnixNano())\n    random := rand.New(seed)\n\n    products := []product{}\n    for i := 0; i &lt; 5000; i++ {\n        products = append(products, product{\n            Name:       fmt.Sprintf(\"Product%v\", i + 1),\n            Quantity:   random.Intn(100),\n        })\n    }\n    return db.Create(&amp;products).Error\n}\n</code></pre> <pre><code>// repositories/product_db.go\npackage repositories\n\n...\n\ntype productRepositoryDB struct {\n    db *gorm.DB\n}\n\nfunc NewProductRepositoryDB(db *gorm.DB) ProductRepository {\n    db.AutoMigrate(&amp;product{})\n    mockData(db)\n    return productRepositoryDB{db: db}\n}\n\nfunc (r productRepositoryDB) GetProducts() (products []product, err error) {\n    err = r.db.Order(\"quantity desc\").Limit(30).Find(&amp;products)\n    if err != nil {\n        return nil, err\n    }\n    return products, err\n}\n</code></pre> <pre><code>// repositories/product_redis.go\npackage repositories\n\n...\n\ntype productRepositoryRedis struct {\n    db *gorm.DB\n    redisClient *redis.Client\n}\n\nfunc NewProductRepositoryDB(db *gorm.DB, redisClient *redis.Client) ProductRepository {\n    db.AutoMigrate(&amp;product{})\n    mockData(db)\n    return productRepositoryRedis{db, redisClient}\n}\n\nfunc (r productRepositoryRedis) GetProducts() (products []product, err error) {\n\n    key := \"repository::GetProcusts\"\n\n    // Chack data in Redis\n    productsJson, err := r.redisClient.Get(context.Background(), key).Result()\n    if err == nil {\n        err = json.Unmarshal([]byte(productsJson, &amp;products)\n        if err == nil {\n            return products, nil\n        }\n    }\n\n    err = r.db.Order(\"quantity desc\").Limit(30).Find(&amp;products)\n    if err != nil {\n        return nil, err\n    }\n\n    // Import to Redis\n    data, err := json.Marshal(products)\n    if err != nil {\n        return nil, err\n    }\n    err = r.redisClient.Set(context.Background(), key, string(data), time.Second * 10).Err()\n    if err != nil {\n        return nil, err\n    }\n    return products, err\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#services","title":"Services","text":"<pre><code>// services/catalog.go\npackage services\n\ntype Product struct {\n    ID       int    `json:\"id\"`\n    Name     string `json:\"name\"`\n    Quantity int    `json:\"quantity\"`\n}\n\ntype CatalogService interface {\n    GetProducts() ([]Product, error)\n}\n</code></pre> <pre><code>// services/catalog_service.go\npackage services\n\n...\n\ntype catalogService struct {\n    productRepo repositories.ProductRepository\n}\n\nfunc NewCatalogService(productRepo repositories.ProductRepository) CatalogService {\n    return catalogService{productRepo}\n}\n\nfunc (s catalogService) GetProducts() (products []Product, err error) {\n    productsDB, err := s.productRepo.GetProducts()\n    if err != nil {\n        return nil, err\n    }\n\n    for _, p := range productsDB {\n        products = append(products, Product{\n            ID:         p.ID,\n            Name:       p.Name,\n            Quantity:   p.Quantity,\n        })\n    }\n    return products, nil\n}\n</code></pre> <pre><code>// services/catalog_redis.go\npackage services\n\n...\n\ntype catalogServiceRedis struct {\n    productRepo repositories.ProductRepository\n    redisClient *redis.Client\n}\n\nfunc NewCatalogServiceRedis(productRepo repositories.ProductRepository, redisClient *redis.Client) CatalogService {\n    return catalogServiceRedis{productRepo, redisClient}\n}\n\nfunc (s catalogServiceRedis) GetProducts() (products []Product, err error) {\n    key := \"services::GetProcusts\"\n\n    // Chack data in Redis\n    if productsJson, err := s.redisClient.Get(context.Background(), key).Result(); err == nil {\n        if json.Unmarshal([]byte(productsJson, &amp;products) == nil {\n            return products, nil\n        }\n    }\n\n    // Repository\n    productsDB, err := s.productRepo.GetProducts()\n    if err != nil {\n        return nil, err\n    }\n\n    for _, p := range productsDB {\n        products = append(products, Product{\n            ID:         p.ID,\n            Name:       p.Name,\n            Quantity:   p.Quantity,\n        })\n    }\n\n    // Import to Redis\n    if data, err := json.Marshal(products); err == nil {\n        s.redisClient.Set(context.Background(), key, string(data), time.Second * 10)\n    }\n    return products, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#handler","title":"Handler","text":"<pre><code>// handlers/catalog.go\npackage handlers\n\n...\n\ntype CatalogHandler interface {\n    GetProducts(c *fiber.Ctx) error\n}\n</code></pre> <pre><code>// handlers/catalog.handlers.go\npackage handlers\n\n...\n\ntype catalogHandler struct {\n    catalogSrv services.CatalogService\n}\n\nfunc NewCatalogHandler(catalogSrv services.CatalogService) CatalogHandler {\n    return catalogHandler{catalogSrv}\n}\n\nfunc (h catalogHandler) GetProducts(c *fiber.Ctx) error {\n    products, err := h.catalogSrv.GetProducts()\n    if err != nil {\n        return err\n    }\n    response := fiber.Map{\n        \"status\": \"ok\",\n        \"products\": products,\n    }\n    return c.JSON(response)\n}\n</code></pre> <pre><code>// handlers/catalog_redis.go\n\npackage handlers\n\n...\n\ntype catalogHandlerRedis struct {\n    catalogSrv services.CatalogService\n    redisClinet *redis.Client\n}\n\nfunc NewCatalogHandlerRedis(catalogSrv services.CatalogService, redisClient *redis.Client) catalogHandlerRedis {\n    return catalogHandlerRedis{catalogSrv, redisClient}\n}\n\nfunc (h catalogHandlerRedis) GetProducts(c *fiber.Ctx) error {\n    key := \"handler::GetProcusts\"\n\n    // Chack data in Redis\n    if responseJson, err := s.redisClient.Get(context.Background(), key).Result(); err == nil {\n        c.Set(\"Content-Type\", \"applicatin/json\")\n        return c.SendString(responseJson)\n    }\n\n    products, err := h.catalogSrv.GetProducts()\n    if err != nil {\n        return err\n    }\n    response := fiber.Map{\n        \"status\": \"ok\",\n        \"products\": products,\n    }\n\n    // Import to Redis\n    if data, err := json.Marshal(response); err == nil {\n        h.redisClient.Set(context.Background(), key, string(data), time.Second * 10)\n    }\n\n    return c.JSON(response)\n}\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#database","title":"Database","text":"<pre><code>// main.go\n\n...\n\nfunc main() {\n    db := initDatabase()\n    redisClient := initRedis()\n\n    productRepo := repositories.NewProductRepositoryDB(db)\n    // // Or,\n    // productRepo := repositories.NewProductRepositoryRedis(db, redisClient)\n    // products, err := productRepo.GetProducts()\n\n    productService := services.NewCatalogService(productRepo)\n    // // Or,\n    // productService := services.NewCatalogServiceRedis(productRepo, redisClient)\n    // products, err := productService.GetProducts()\n\n    productHandler := handler.NewCatalogHandler(productService)\n    // Or,\n    productHandler := handler.NewCatalogHandlerRedis(productService, redisClient)\n\n    app := fiber.New()\n    app.Get(\"/products\", productHandler.GetProducts)\n    app.Listen(\":8000\")\n}\n\nfunc initDatabase() *gorm.DB {\n    dial := mysql.Open(\"root:P@ssw0rd@tcp(localhost:3306)/product\")\n    db, err := gorm.Open(dial, &amp;gorm.Config{})\n    if err != nil {\n        panic(err)\n    }\n    return db\n}\n\nfunc initRedis() *redis.Client {\n    return redis.NewClient(&amp;redis.Option{\n        Addr: \"localhost:6379\"\n    })\n}\n</code></pre> <pre><code>curl localhost:8000/products | jq\n</code></pre>"},{"location":"tools/lang/go/go-connect-redis/#references","title":"References","text":"<p>-</p>"},{"location":"tools/lang/go/go-database/","title":"GO Connect to Database","text":"<p>Table of Contents:</p> <ul> <li>[Start to Connect Database]</li> <li>[Create Return Function]</li> <li>[Create CRUD]</li> <li>[Transaction]</li> </ul>"},{"location":"tools/lang/go/go-database/#start-to-connect-database","title":"Start to Connect Database","text":"<pre><code>go get -u github.com/denisenkom/go-mssqldb\n</code></pre> <pre><code>package main\n\nimport {\n    \"database/sql\"\n\n    // import driver that use in /sql package\n    _ \"github.com/denisenkom/go-mssqldb\"\n}\n\nfunc main() {\n    db, err := sql.Open(\"sqlserver\", \"sqlserver://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:1443/&lt;database&gt;\")\n    if err != nil {\n        panic(err)\n    }\n\n    err = db.Ping()\n    if err != nil {\n        panic(err)\n    }\n\n    query := \"select id, name from cover\"\n    row, err := db.Query(query)\n    if err != nil {\n        panic(err)\n    }\n    defer rows.Close()\n\n    for rows.Next() {\n        id := 0\n        name := \"\"\n        err = rows.Scan(&amp;id, &amp;name)\n        if err := nil {\n            panic(err)\n        }\n        println(id, name)\n    }\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#create-struct-for-receive-data","title":"Create Struct for receive data","text":"<pre><code>type Cover struct {\n    Id int\n    Name string\n}\n</code></pre> <pre><code>covers := []Cover{}\n\nfor rows.Next() {\n    cover := Cover{}\n    err = rows.Scan(&amp;cover.Id, &amp;cover.Name)\n    if err != nil {\n        panic(err)\n    }\n    covers = append(covers, cover)\n}\n\nfmt.Printf(\"%#v\", covers)\n</code></pre> <pre><code>[]main.Cover{main.Cover{Id:1, Name:\"cover-lion\"}, main.Cover{Id:1, Name:\"cover-zebra\"}}%\n</code></pre>"},{"location":"tools/lang/go/go-database/#create-return-function","title":"Create Return Function","text":"<pre><code>var db *sql.DB\n\nfunc main() {\n    var err error\n    db, err = sql.Open(\"sqlserver\", \"sqlserver://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:1443/&lt;database&gt;\")\n    if err != nil {\n        panic(err)\n    }\n\n    covers, err := GetCovers()\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n\n    for _, cover := range covers {\n        fmt.Println(cover)\n    }\n}\n\nfunc GetCovers() ([]Cover, error) {\n    err = db.Ping()\n    if err != nil {\n        return nil, err\n    }\n\n    query := \"select id, name from cover\"\n    row, err := db.Query(query)\n    if err != nil {\n        return nil, err\n    }\n    defer rows.Close()\n\n    covers := []Cover{}\n    for rows.Next() {\n        cover := Cover{}\n        err = rows.Scan(&amp;cover.Id, &amp;cover.Name)\n        if err != nil {\n            return nil, err\n        }\n        covers = append(covers, cover)\n    }\n    return covers, nil\n}\n</code></pre> <p>Note: \\ You should not create error handler process in your err.</p>"},{"location":"tools/lang/go/go-database/#create-crud","title":"Create CRUD","text":""},{"location":"tools/lang/go/go-database/#read","title":"Read","text":"<pre><code>func GetCover(id int) (*Cover, error) {\n    err := db.Ping()\n    if err != nil {\n        return nil, err\n    }\n\n    // Use @id for SQL Server\n    query := \"select id, name from cover where id=@id\"\n    row := db.QueryRow(query, sql.Named(\"id\", id))\n\n    cover := Cover{}\n    err = row.Scan(&amp;cover.Id, &amp;cover.Name)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;cover, nil\n}\n</code></pre> <pre><code>cover, err := GetCover(1)\nif err != nil {\n    panic(err)\n}\nprintln(cover)\n</code></pre> <pre><code>&amp;{1 cover-lion}\n</code></pre> <p>Note: \\ If you use MySQL, the above query row syntax will change to</p> <pre><code>query := \"select id, name from cover where id=?\"\nrow := db.QueryRow(query, id)\n</code></pre>"},{"location":"tools/lang/go/go-database/#create","title":"Create","text":"<pre><code>func AddCover(cover Cover) error {\n    query := \"insert into cover (id, name) values (?, ?)\"\n    result, err := db.Exec(query, cover.Id, cover.Name)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot insert to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>cover := Cover{9, \"cover-Tom\"}\nerr = AddCover(cover)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#update","title":"Update","text":"<pre><code>func UpdateCover(cover Cover) error {\n    query := \"update cover set name=? where id=?\"\n    result, err := db.Exec(query, cover.Name, cover.Id)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot update to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>cover := Cover{9, \"cover-Sara\"}\nerr = UpdateCover(cover)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#delete","title":"Delete","text":"<pre><code>func DeleteCover(id int) error {\n    query := \"delete from cover where id=?\"\n    result, err := db.Exec(query, id)\n    if err != nil {\n        return err\n    }\n    affected, err := result.RowAffected()\n    if err != nil {\n        return err\n    }\n    if affected &lt;= 0 {\n        return errors.New(\"cannot delete to cover table\")\n    }\n    return nil\n}\n</code></pre> <pre><code>err = DeleteCover(9)\nif err != nil {\n    panic(err)\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#sqlx","title":"SQLX","text":"<pre><code>go get github.com/jmoiron/sqlx\n</code></pre> <pre><code>package main\n\nimport {\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n}\n\nvar db *sqlx.DB\n\nfunc main() {\n    var err error\n    db, err = sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;)/&lt;database&gt;\")\n    if err != {\n        panic(err)\n    }\n}\n\nfunc GetCoversX() ([]Cover, error) {\n    query := \"select id, name from cover\"\n    covers := []Cover{}\n    err = db.Select(&amp;covers, query)\n    if err != nil {\n        return nil, err\n    }\n    return covers, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#change-read","title":"Change Read","text":"<pre><code>func GetCoverX(id int) (*Cover, error) {\n    query := \"select id, name from cover where id=?\"\n    cover := Cover{}\n    err = db.Get(&amp;cover, query, id)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;cover, nil\n}\n</code></pre>"},{"location":"tools/lang/go/go-database/#transaction","title":"Transaction","text":"<pre><code>tx, err := db.Begin()\nif err != nil {\n    return err\n}\n\nquery := \"...\"\nresult, err := tx.Exec(query, ...)\n\n...\n\naffected, err := result.RowsAffected()\nif err != nil {\n    tx.Rollback()\n    return err\n}\n\n...\n\nerr = tx.Commit()\nif err != nil {\n    return err\n}\nreturn nil\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/","title":"GO Hexagonal Architecture","text":"<p>Update: <code>2023-05-04</code> | Tag: <code>GO</code> <code>Architecture</code> <code>RestAPI</code></p> <p>Hexagonal Architecture or Ports &amp; Adapters is an architectural pattern that guilds you in structuring a component/service/application and managing dependencies.</p> <p>Warning: \\ This is not a best practice!</p> <p>Layer of Architecture:</p> <pre><code>Presentation --&gt; Business --&gt; Database\n</code></pre> <p>Table of Contents:</p> <ul> <li>Get Started</li> <li></li> </ul>"},{"location":"tools/lang/go/go-hexagonal-architecture/#get-started","title":"Get Started","text":"<pre><code>bank\n|--&gt; repository\n|    |--&gt; customer_db.go\n|    |--&gt; customer_mock.go\n|    |--&gt; customer.go\n|--&gt; service\n|    |--&gt; customer.go\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-customer-ports","title":"Create Customer Ports","text":"<pre><code>// repository/customer.go\npackage repository\n\ntype Customer struct {\n    CustomerID  int     `db:\"customer_id\"`\n    Name        string  `db:\"name\"`\n    DateOfBirth string  `db:\"date_of_birth\"`  // or time.Time\n    City        string  `db:\"city\"`\n    ZipCode     string  `db:\"zipcode\"`\n    Status      int     `db:\"status\"`\n}\n\ntype CustomerRepository interface {\n    GetAll() ([]Customer, error)\n    GetById(int) (*Customer, error)\n}\n</code></pre> <pre><code>// service/customer.go\npackage service\n\ntype CustomerResponse struct {\n    CustomerID  int     `json:\"customer_id\"`  // If you want xml, `xml:\"...\"`\n    Name        string  `json:\"name\"`\n    Status      string  `json:\"status\"`\n}\n\ntype CustomerService interface {\n    GetCustomers() ([]CustomerResponse, error)\n    GetCustomer(int) (*CustomerResponse, error)\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-database-adapters","title":"Create Database Adapters","text":"<pre><code>go get github.com/jmoiron/sqlx\n</code></pre> <pre><code>// repository/customer_db.go\npackage repository\n\nimport \"github.com/jmoiron/sqlx\"\n\ntype customerRepositoryDB struct {\n    db *sqlx.DB\n}\n\nfunc NewCustomerRepositoryDB(db *sqlx.DB) customerRepositoryDB {\n    return customerRepositoryDB{db: db}\n}\n\nfunc (r customerRepositoryDB) GetAll() ([]Customer, error) {\n    customers := []Customer{}\n    query := \"select ... from customers\"\n    err := r.db.Select(&amp;customers, query)\n    if err != nil {\n        return nil, err\n    }\n    return customers, nil\n}\n\nfunc (r customerRepositoryDB) GetById(id int) (*Customer, error) {\n    customer := Customer{}\n    query := \"select ... from customers where customer_id=?\"\n    err := r.db.Get(&amp;customer, query, id)\n    if err != nil {\n        return nil, err\n    }\n    return &amp;customer, nil\n}\n</code></pre> <p>Note: \\ If you want to implement create function, you can use:</p> <pre><code>query = \"insert into ... values(?, ?, ?)\"\nresult, err := r.db.Exec(\n    query,\n    &lt;obj&gt;.Param01,\n    &lt;obj&gt;.Param02,\n    &lt;obj&gt;.Param03,\n)\nid, err := result.LastInsertId()\nif err != nil {\n    return nil, err\n}\n&lt;obj&gt;.ID = int(id)\nretunr &amp;&lt;obj&gt;, nil\n</code></pre> <pre><code>go get -u github.com/go-sql-driver/mysql\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    _ = customerRepository\n\n    custoers, err := customerRepository.GetAll()\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customers)\n\n    customer, err := customerRepository.GetById(2000)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customer)  // &amp;{200 Steve ...}\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-service-adapters","title":"Create Service Adapters","text":"<pre><code>// service/customer_service.go\npackage service\n\ntype customerService struct {\n    custRepo repository.CustomerRepository  // ref interface only\n}\n\nfunc NewCustomerService(custRepo repository.CustomerRepository) customerService {\n    return customerService{custRepo: custRepo}\n}\n\nfunc (s customerService) GerCustomers() ([]CustomerResponse, error) {\n    customers, err := s.custRepo.GetAll()\n    if err != nil {\n        log.Println(err)\n        return nil, err\n    }\n    custResponses := []CustomerResponse{}\n    for _, customer := range customers {\n        custResponse := CustomerResponse{\n            CustomerID: customer.CustomerID,\n            Name:       customer.Name,\n            Status:     customer.Status,\n        }\n        custResponses = append(custResponses, custResponse)\n    }\n    return custResponses, nil\n}\n\nfunc (s customerService) GerCustomer(id int) (*CustomerResponse, error) {\n    customer, err := s.custRepo.GetById(id)\n    if err != nil {\n\n        if err == sql.ErrNoRows {\n            return nil, errors.New(\"customer not found\")\n        }\n\n        log.Println(err)\n        return nil, err\n    }\n    custResponse := CustomerResponse{\n        CustomerID: customer.CustomerID,\n        Name:       customer.Name,\n        Status:     customer.Status,\n    }\n    return &amp;custResponse, nil\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    \"back/repository\"\n    \"back/service\"\n\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    customerService := service.NewCustomerService(customerRepository)\n\n    customers, err := customerService.GetCustomers()\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customers)\n\n    customer, err := customerService.GetCustomer(2000)\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(customer)  // &amp;{200 Steve ...}\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-customer-handler","title":"Create Customer Handler","text":"<pre><code>bank\n|--&gt; handler\n|    |--&gt; customer.go\n|--&gt; repository\n|    |--&gt; customer_db.go\n|    |--&gt; customer.go\n|--&gt; service\n|    |--&gt; customer.go\n|--&gt; go.mod\n|--&gt; main.go\n</code></pre> <pre><code>go get -u github.com/gorilla/mux\n</code></pre> <pre><code>package handler\n\nimport (\n    \"bank/service\"\n    \"net/http\"\n)\n\ntype customerHandler struct {\n    custSrv service.CustomerService\n}\n\nfunc NewCustomerHandler(custSrv service.CustomerService) customerHandler {\n    return customerHandler{custSrv: custSrv}\n}\n\nfunc (h customerHandler) GetCustomers(w http.ResponseWriter, r *http.Request) {\n    customers, err := h.custSrv.GetCustomers()\n    if err != nil {\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(customers)\n}\n\nfunc (h customerHandler) GetCustomer(w http.ResponseWriter, r *http.Request) {\n    customerID, _ := strconv.Atoi(mux.Vars(r)[\"customerID\"])\n    customer, err := h.custSrv.GetCustomer(customerID)\n    if err != nil {\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(customer)\n}\n</code></pre> <pre><code>// main.go\npackage main\n\nimport (\n    \"back/repository\"\n    \"back/service\"\n    \"back/handler\"\n    \"net/http\"\n\n    _ \"github.com/go-sql-driver/mysql\"\n    \"github.com/jmoiron/sqlx\"\n)\n\nfunc main() {\n    db, err := sqlx.Open(\"mysql\", \"root:&lt;password&gt;@tcp(&lt;host&gt;:&lt;port&gt;)/&lt;database&gt;\"\n    if err != nil {\n        panic(err)\n    }\n\n    customerRepository := repository.NewCustomerRepositoryDB(db)\n    customerService := service.NewCustomerService(customerRepository)\n    customerHandler := handler.NewCustomerHandler(customerService)\n\n    router := mux.NewRouter()\n\n    router.HandleFunc(\"/customers\", customerHandler.GetCustomers).Methods(http.MethodGet)\n    router.HandleFunc(\"/customers/{customerID:[0-9]+}\", customerHandler.GetCustomer).Methods(http.MethodGet)\n\n    http.ListenAndServe(\":8000\", router)\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#create-mock-repository","title":"Create Mock Repository","text":"<pre><code>bank\n|--&gt; repository\n|    |--&gt; ...\n|    |--&gt; customer_mock.go\n|    |--&gt; ...\n|--&gt; ...\n</code></pre> <pre><code>// repository/customer_mock.go\npackage repository\n\ntype customerRepositoryMock struct {\n    customers []Customer\n}\n\nfunc NewCustomerRepositoryMock() CustomerRepositoryMock {\n    customers := []Customer{\n        {CustomerID: 1001, Name: \"Sara\", ...},\n        {CustomerID: 1002, Name: \"Tom\", ...},\n    }\n    return CustomerRepositoryMock{customers: customers}\n}\n\nfunc (r customerRepositoryMock) GetAll() ([]Customer, error) {\n    return r.cusomers, nil\n}\n\nfunc (r customerRepositoryMock) GetById(id int) (*Customer, error) {\n    for _, customer := range r.cusomers {\n        if customer.CustomerID == id {\n            return &amp;customer, nil\n        }\n    }\n    return nil, errors.New(\"customer not found\")\n}\n</code></pre> <pre><code>...\ncustomerRepository := repository.NewCustomerRepositoryDB(db)\n---&gt; Change to ...\ncustomerRepository := repository.NewCustomerRepositoryMock()\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#configuration","title":"Configuration","text":"<pre><code>go get github.com/spf13/viper\n</code></pre> <pre><code>bank\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; config.yaml\n|--&gt; main.go\n</code></pre> <pre><code>// main.go\n\nfunc main() {\n    initConfig()\n\n    dsn := fmt.Sprintf(\"%v:%v@tcp(%v:%v)/%v?parseTime=true\",\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n    db, err := sqlx.Open(viper.GetString(\"db.driver\"), dns)\n    if err != nil {\n        panic(err)\n    }\n\n    ...\n\n    http.ListenAddServe(fmt.Sprintf(\":%v\", viper.GetInt(\"app.port\")), router)\n}\n\n\nfunc initConfig() {\n    viper.SetConfigName(\"config\")\n    viper.SetConfigType(\"yaml\")\n    viper.AddConfigPath(\".\")\n\n    // Read Environment variable after and override value\n    viper.AutonaticEnv()\n    viper.SetEnvKeyReplacer(string.NewReplacer(\".\", \"_\"))\n\n    err := viper.ReadInConfig()\n    if err != nil {\n        panic(err)\n    }\n}\n</code></pre> <pre><code>app:\n  port: 8000\n\ndb:\n  driver: \"mysql\"\n  host: \"13.00.000.00\"\n  port: 3306\n  username: root\n  password: P@ssw0rd\n  database: banking\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-database-setup","title":"Change Database Setup","text":"<pre><code>// main.go\n\ndef main() {\n    ...\n    db := initDatabase()\n    ...\n}\n\ndef initDatabase() *sqlxDB {\n    dsn := fmt.Sprintf(\"%v:%v@tcp(%v:%v)/%v?parseTime=true\",\n        viper.GetString(\"db.username\"),\n        viper.GetString(\"db.password\"),\n        viper.GetString(\"db.host\"),\n        viper.GetInt(\"db.port\"),\n        viper.GetString(\"db.database\"),\n    )\n    db, err := sqlx.Open(viper.GetString(\"db.driver\"), dns)\n    if err != nil {\n        panic(err)\n    }\n\n    // Database configuration\n    db.SetConnMaxLifetime(3 * time.Minute)\n    db.SetMaxOpenConns(10)\n    db.SetMaxIdleConns(10)\n\n    return db\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#timezone","title":"TimeZone","text":"<pre><code>// main.go\n\nfunc main() {\n    initTimeZone()\n}\n\nfunc initTimeZone() {\n    ict, err := time.LoadLocation(\"Asia/Bangkok\")\n    if err != nil {\n        panic(err)\n    }\n\n    time.Local = ict\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#logging","title":"Logging","text":"<pre><code>go get -u go.uber.org/zap\n</code></pre> <pre><code>bank\n|--&gt; logs\n|    |--&gt; logs.go\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// logs/logs.go\npackage logs\n\nimport \"go.uber.org/zap\"\n\nvar log *zap.Logger\n\nfunc init() {\n    var err error\n\n    log, _ = zap.NewProduction()  // production use json, but development use console\n\n    // Or, ddit Production configuration\n    config := zap.NewProductionConfig()\n    config.EncoderConfig.TimeKey = \"timestamp\"\n    config.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder\n\n    // If you want to close strack trace,\n    config.EncoderConfig.StacktraceKey = \"\"\n\n    log, err = config.Build(zap.AddCallerSkip(1))  // Skip caller 1 step\n    if err != nil {\n        panic(err)\n    }\n}\n\nfunc Info(message string, fields ...zap.Field) {\n    log.Info(message, fields...)\n}\n\nfunc Debug(message string, fields ...zap.Field) {\n    log.Debug(message, fields...)\n}\n\nfunc Error(message interface{}, fields ...zap.Field) {\n    switch y := message.(type) {\n        case error:\n            log.Error(v.Error(), fields...)\n        case string:\n            log.Error(v, fields...)\n    }\n}\n</code></pre> <p>Note: \\ In GO, it has init function that will run before main function. You can use it by <code>func init() { ... }</code></p> <pre><code>// main.go\n\nfunc main() {\n    ...\n    logs.Log.Info(\"Banking service started Info Level log\")\n    ...\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-log-in-service","title":"Change Log in Service","text":"<pre><code>...\nlog.Println(err)\n---&gt; Change to ...\nlogs.Error(err.Error())  // Or, logs.Error(err)\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#error","title":"Error","text":"<pre><code>bank\n|--&gt; errs\n|    |--&gt; errs.go\n|--&gt; ...\n|--&gt; repository\n|    |--&gt; ...\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// errs/errs.go\npackage errs\n\ntype AppError struct {\n    Code    int\n    Message string\n}\n\nfunc (e AppError) Error() string {\n    return e.Message\n}\n\nfunc NewNotFoundError(message string) error {\n    return AppError{\n        Code:    http.StatusNotFound,\n        Message: message,\n    }\n}\n\nfunc NewUnexpectedError(message string) error {\n    return AppError{\n        Code:    http.StatusInternalServerError,\n        Message: \"unexpected error\",\n    }\n}\n</code></pre> <pre><code>// service/customer_service.go\n\nfunc (s customerService) GetCustomer(id int) ... {\n    ...\n    if err != nil {\n        if err == sql.ErrNoRows {\n            return nil, errs.NewNotFoundError(\"customer not found\")\n        }\n        logs.Error(err)\n        return nil, errs.NewUnexpectedError()\n    }\n    ...\n}\n</code></pre> <pre><code>// handler/customer.go\n\nfunc (h customerHandler) GetCustomer(w http.ResponseWriter, ...) {\n    ...\n    if err != nil {\n\n        appErr, ok := err.(errs.AppError)\n        if ok {\n            w.WriteHeader(appErr.Code)\n            fmt.Fprintln(w, appErr.Message)\n            return\n        }\n\n        w.WritHeader(http.StatusInternalServerError)\n        fmt.Fprintln(w, err)\n        return\n    }\n    ...\n}\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#change-code-to-helper","title":"Change Code to helper","text":"<pre><code>bank\n|--&gt; errs\n|    |--&gt; errs.go\n|--&gt; handler\n|    |--&gt; customer.go\n|    |--&gt; handler.go\n|--&gt; ...\n|--&gt; main.go\n</code></pre> <pre><code>// handler/handler.go\npackage handler\n\nimport \"net/http\"\n\nfunc handleError(w http.ResponseWriter, err error) {\n    switch e := err.(type) {\n        case errs.AppError:\n            w.WriteHeader(e.Code)\n            fmt.Fprintln(w, e)\n        case error:\n            w.WriteHeader(http.StatusInternalServerError)\n            fmt.Fprintln(w, e)\n    }\n}\n</code></pre> <pre><code>...\nappErr, ok := err.(errs.AppError)\nif ok {\n    w.WriteHeader(appErr.Code)\n    fmt.Fprintln(w, appErr.Message)\n    return\n}\nw.WritHeader(http.StatusInternalServerError)\nfmt.Fprintln(w, err)\n\n---&gt; Change to ...\n\nhandleError(w, err)\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#add-more","title":"Add More","text":""},{"location":"tools/lang/go/go-hexagonal-architecture/#account","title":"Account","text":"<pre><code>// reposirory/account.go\n\ntype Account struct {\n    AccountID       int     `json:\"account_id\"`\n    CustomerID      int     `json:\"customer_id\"`\n    OpeningDate     string  `json:\"opening_date\"`\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n    Status          int     `json:\"status\"`\n}\n\ntype AccountRepository interface {\n    Create(Account) (*Account, error)\n    GetAll(int) ([]Account, error)\n}\n</code></pre> <pre><code>// service/account.go\npackage serice\n\ntype NewAccountRequest struct {\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n}\n\ntype NewAccountResponse struct {\n    AccountID       int     `json:\"account_id\"`\n    OpeningDate     string  `json:\"opening_date\"`\n    AccountType     string  `json:\"account_type\"`\n    Amount          float64 `json:\"amount\"`\n    Status          int     `json:\"status\"`\n}\n\ntype AccountService interface {\n    NewAccount(int, NewAccountRequest) (*NewAccountResponse, error)\n    GetAccounts(int) ([]NewAccountResponse, error)\n}\n</code></pre> <pre><code>// service/account_service.go\n\n...\nfunc (s accountService) NewAccount(customerID int, request NewAccountRequest) (*AccountResponse, error) {\n    // Validation\n    if request.Amount &lt; 5000 {\n        return nil, errs.NewValidationError(\"amount at least 5,000\")\n    }\n    if string.ToLower(requset.AccountType) != \"saving\" &amp;&amp; string.ToLower(requset.AccountType) != \"checking\" {\n        return nil, errs.NewValidationError(\"account type should be saving or checking only\")\n    }\n    account := repository.Account{\n        CustomerID:     customerID,\n        OpeningDate:    time.New().Format(\"2006-1-2 15:00:00\"),\n        AccountType:    requset.AccountType,\n        Amount:         requset.Amount,\n        Status:         1,\n    }\n\n    newAcc, err := s.accRepo.Create(account)\n    if err != nil {\n        logs.Error(err)\n        return nil, errs.NewUnexpectedError()\n    }\n    response := AccountResponse{\n        AccountID:  newAcc.AccountID,\n        ...\n    }\n    return response, nil\n}\n...\n</code></pre> <pre><code>// handler/account.go\n\n...\n\nfunc (h accountHandler) NewAccount(w http.ResponseWriter, r *http.Request) {\n    customerID, _ := strconv.Atoi(mux.Vars(r)[\"customerID\"])\n    if r.Header.Get(\"content-type\") != \"application/json\" {\n        handlerError(w, errs.NewValidationError(\"request body incorrect format\"))\n        return\n    }\n\n    request := service.NewAccoutRequest{}\n    err := json.NewDecoder(r.Body).Decode(&amp;request)\n    if err != nil {\n        handlerError(w, errs.NewValidationError(\"request body incorrect format\"))\n        return\n    }\n\n    response, err := h.accSrv.NewAccount(customerID, request)\n    if err != nil {\n        handlerError(w, err)\n        return\n    }\n\n    w.WriteHeader(http.StatusCreated)\n    w.Header().Set(\"content-type\", \"application/json\")\n    json.NewEncoder(w).Encode(response)\n}\n\n...\n</code></pre>"},{"location":"tools/lang/go/go-hexagonal-architecture/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=k3JZI-sQs2k</li> </ul>"},{"location":"tools/lang/go/go-unittest/","title":"GO Unittest","text":"<p>Update: <code>2023-05-04</code> | Tag: <code>GO</code> <code>Unittest</code></p>"},{"location":"tools/lang/go/go-unittest/#test","title":"Test","text":"<pre><code>// services/grade_test.go\npackage services_test\n\nimport \"testing\"\n\nfunc TestCheckGrade(t *testing.T) {\n    // test function for CheckGrade(80)\n    grade := services.CheckGrade(80)\n    expected := \"A\"\n\n    if grade != expected {\n        t.Errorf(\"get %v expected %v\", grade, expected)\n    }\n}\n</code></pre> <pre><code>// services/grade.go\npackage services\n\nfunc CheckGrade(score int) string {\n    switch {\n        case score &gt;= 80:\n            return \"A\"\n        case score &gt;= 70:\n            return \"B\"\n        case score &gt;= 60:\n            return \"C\"\n        case score &gt;= 50:\n            return \"D\"\n        default:\n            return \"F\"\n    }\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/serices -v\n\ngo test &lt;module-name&gt;/serices -v -run=TestCheckGrade\n\ngo test ./... -v\n</code></pre> <pre><code># Get test coverage\ngo test &lt;module-name&gt;/services -cover\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#create-sub-test","title":"Create Sub-test","text":"<pre><code>// services/grade_test.go\n\nfunc TestCheckGrade(t *testing.T) {\n    t.Run(\"A\", func(t *testing.T) {\n        grade := services.CheckGrade(80)\n        expected := \"A\"\n\n        if grade != expected {\n            t.Errorf(\"get %v expected %v\", grade, expected)\n        }\n    })\n\n    t.Run(\"B\", func(t *testing.T) {\n        grade := services.CheckGrade(70)\n        expected := \"B\"\n\n        if grade != expected {\n            t.Errorf(\"get %v expected %v\", grade, expected)\n        }\n    })\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/services -run=\"TestCheckGrade/A\" -v\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#change-to-be-good-coverage","title":"Change to be Good Coverage","text":"<pre><code>// services/grade_test.go\n\nfunc TestCheckGrade(t *testing.T) {\n    type testCase struct {\n        name     string\n        score    int\n        expected string\n    }\n\n    cases := []testCase{\n        {name: \"a\", score: 80, expected: \"A\"},\n        {name: \"b\", score: 70, expected: \"B\"},\n        {name: \"c\", score: 60, expected: \"C\"},\n        {name: \"d\", score: 50, expected: \"D\"},\n        {name: \"f\", score: 0, expected: \"F\"},\n    }\n\n    for _, c := range cases {\n        t.Run(c.name, func(t *testing.T) {\n            grade := services.CheckGrade(c.score)\n            if grade != c.expected {\n                t.Errorf(\"get %v expected %v\", grade, expected)\n            }\n        })\n    }\n\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#benchmark","title":"Benchmark","text":"<pre><code>// services/grade_test.go\n\nfunc BenchmarkCheckGrade(b *testing.B) {\n    for i := 0; i &lt; b.N; i ++ {\n        services.CheckGrade(80)\n    }\n}\n</code></pre> <pre><code>go test &lt;module-name&gt;/services -bench=.\n\n# For check memory benchmark\ngo test &lt;module-name&gt;/services -bench=. -benchmem\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#example","title":"Example","text":"<pre><code>go get golang.org/x/tools/cmd/godoc\n</code></pre> <pre><code>// services/grade_test.go\n\nfunc ExampleCheckGrade() {\n    grade := services.CheckGrade(80)\n    fmt.Println(grade)\n    // Output: A\n}\n</code></pre> <pre><code>godoc -http=:8000\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#mock","title":"Mock","text":"<pre><code>go get github.com/stretchr/testify\n</code></pre> <pre><code>// main.go\npackage main\n\nfunc main() {\n    c := CustomerRepositoryMock{}\n    c.On(\"GetCustomer\", 1).Return(\"Tom\", 18, nil)\n    c.On(\"GetCustomer\", 2).Return(\"\", 0, errors.New(\"not found\"))\n\n    // Try to use\n    name, age, err := c.GetCustomer(1)\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    fmt.Println(name, age)\n}\n\ntype CustomerRepository interface {\n    GetCustomer(id int) (name string, age int, err error)\n}\n\ntype CustomerRepositoryMock struct {\n    mock.Mock\n}\n\nfunc (m *CustomerRepositoryMock) GetCustomer(id int) (name string, age int, err error) {\n    args := m.Called(id)\n    return args.String(0), args.Int(1), args.Error(2)\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#implement-mock","title":"Implement Mock","text":"<pre><code>// repositories/promotion_mock.go\npackage repositories\n\ntype promotionRepositoryMock struct {\n    mock.Mock\n}\n\nfunc NewPromotionRepositoryMock() *promotionRepositoryMock{\n    return &amp;promotionRepositoryMock{}\n}\n\nfunc (m *promotionRepositoryMock) GetPromotion() (Promotion, error) {\n    args := m.Called()\n    // Cast type of first value to Promotion\n    return args.Get(0).(Promotion), args.Error(1)\n}\n</code></pre> <pre><code>// services/promotion_test.go\npackage services_test\n\nimport (\n    \"&lt;module-name&gt;/repositories\"\n    \"&lt;module-name&gt;/services\"\n    \"testing\"\n)\n\nfunc TestPromotionCalculateDiscount(t *testing.T) {\n\n    // Arrage\n    promoRepo := repositories.NewPromotionRepositoryMock()\n    promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n        ID:              1,\n        PurchaseMin:     100,\n        DiscountPercent: 20,\n    }, nil)\n\n\n    promoService := services.NewPromotionService(promoRepo)\n\n    // Act\n    discount, _ := promoService.CalculateDiscount(100)\n    expected := 80\n\n    // Assert\n    assert.Equal(t, expected, discount)\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#change-code","title":"Change Code","text":"<pre><code>// services/promotion_test.go\npackage services_test\n\nfunc TestPromotionCalculateDiscount(t *testing.T) {\n\n    type testCase struct {\n        name            string\n        purchaseMin     int\n        discountPercent int\n        amount          int\n        expected        int\n    }\n\n    cases := []testCase{\n        {name: \"applied 100\", purchaseMin: 100, discountPercent: 20, amount: 100, expected: 80},\n        {name: \"applied 200\", purchaseMin: 100, discountPercent: 20, amount: 200, expected: 160},\n        {name: \"applied 300\", purchaseMin: 100, discountPercent: 20, amount: 300, expected: 240},\n        {name: \"not applied 50\", purchaseMin: 100, discountPercent: 20, amount: 50, expected: 50},\n    }\n\n    for _, c := range cases {\n        t.Run(c.name, func(t *testing.T) {\n            // Arrage\n            promoRepo := repositories.NewPromotionRepositoryMock()\n            promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n                ID:              1,\n                PurchaseMin:     c.purchaseMin,\n                DiscountPercent: c.discountPercent,\n            }, nil)\n\n\n            promoService := services.NewPromotionService(promoRepo)\n\n            // Act\n            discount, _ := promoService.CalculateDiscount(c.amount)\n            expected := c.expected\n\n            // Assert\n            assert.Equal(t, expected, discount)\n        })\n    }\n\n    t.Run(\"purchase amount zero\", func(t *testing.T) {\n        promoRepo := repositories.NewPromotionRepositoryMock()\n        promoReop.On(\"GetPromotion\").Return(repositories.Promotion{\n            ID:              1,\n            PurchaseMin:     100,\n            DiscountPercent: 20,\n        }, nil)\n\n        promoService := services.NewPromotionService(promoRepo)\n\n        // Act\n        _, err := promoService.CalculateDiscount(0)\n\n        // Assert\n        assert.ErrorIs(t, err, services.ErrZeroAmount)\n        promoRepo.AssertNotCalled(t, \"GetPromotion\")\n    })\n\n    t.Run(\"repository error\", func(t *testing.T) {\n        promoRepo := repositories.NewPromotionRepositoryMock()\n        promoReop.On(\"GetPromotion\").Return(repositories.Promotion{}, errors.New(\"\"))\n\n        promoService := services.NewPromotionService(promoRepo)\n\n        // Act\n        _, err := promoService.CalculateDiscount(100)\n\n        // Assert\n        assert.ErrorIs(t, err, services.ErrRepository)\n    })\n}\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#tags","title":"Tags","text":"<pre><code>// handlers/promotion_test.go\n//go:build unit\n\n...\n</code></pre> <pre><code>go test &lt;module-name&gt;/handlers -v -tags=unit\n\ngo test &lt;module-name&gt;/handlers -v -tags=integation,unit\n</code></pre>"},{"location":"tools/lang/go/go-unittest/#references","title":"References","text":"<ul> <li>https://youtu.be/Wd3O6GcA20w</li> </ul>"},{"location":"tools/lang/powershell/","title":"PowerShell","text":""},{"location":"tools/lang/powershell/win-bat-file/","title":"PowerShell: Batch File","text":"<p>As others have already said, parameters passed through the command line can be accessed in batch files with the notation <code>%1</code> to <code>%9</code>. There are also two other tokens that you can use:</p> <ul> <li><code>%0</code> is the executable (batch file) name as specified in the command line.</li> <li><code>%*</code> is all parameters specified in the command line -- this is very useful if   you want to forward the parameters to another program.</li> </ul> <p>There are also lots of important techniques to be aware of in addition to simply how to access the parameters.</p>"},{"location":"tools/lang/powershell/win-bat-file/#checking-if-a-parameter-was-passed","title":"Checking if a parameter was passed","text":"<p>This is done with constructs like <code>IF \"%~1\"==\"\"</code>, which is <code>true</code> if and only if no arguments were passed at all. Note the tilde character which causes any surrounding quotes to be removed from the value of <code>%1</code>; without a tilde you will get unexpected results if that value includes double quotes, including the possibility of syntax errors.</p>"},{"location":"tools/lang/powershell/win-bat-file/#handling-more-than-9-arguments","title":"Handling more than 9 arguments","text":"<p>If you need to access more than 9 arguments you have to use the command <code>SHIFT</code>. This command shifts the values of all arguments one place, so that <code>%0</code> takes the value of <code>%1</code>, <code>%1</code> takes the value of <code>%2</code>, etc. <code>%9</code> takes the value of the tenth argument (if one is present), which was not available through any variable before calling <code>SHIFT</code> (enter command <code>SHIFT /?</code> for more options).</p> <p><code>SHIFT</code> is also useful when you want to easily process parameters without requiring that they are presented in a specific order. For example, a script may recognize the flags <code>-a</code> and <code>-b</code> in any order. A good way to parse the command line in such cases is</p> <pre><code>:parse\n    IF \"%~1\"==\"\" GOTO endparse\n    IF \"%~1\"==\"-a\" REM do something\n    IF \"%~1\"==\"-b\" REM do something else\n    SHIFT\n    GOTO parse\n\n:endparse\n    REM ready for action!\n</code></pre>"},{"location":"tools/lang/powershell/win-bat-file/#substitution-of-batch-parameters","title":"Substitution of batch parameters","text":"<p>For parameters that represent file names the shell provides lots of functionality related to working with files that is not accessible in any other way. This functionality is accessed with constructs that begin with <code>%~</code>.</p> <p>For example, to get the size of the file passed in as an argument use</p> <pre><code>ECHO %~z1\n</code></pre> <p>To get the path of the directory where the batch file was launched from you can use</p> <pre><code>ECHO %~dp0\n</code></pre> <p>Note: \\ You can view the full range of these capabilities by typing <code>CALL /?</code> in the command prompt.</p>"},{"location":"tools/lang/powershell/win-bat-file/#full-example","title":"Full Example","text":"<pre><code>@echo off\ngoto :init\n\n:usage\n    echo USAGE:\n    echo   %__bat_filename% [flags] \"release argument\"\n    echo.\n    echo.  -h, --help           shows this help\n    echo.  -p, --port value     specifies a port number value\n    goto :eof\n\n:missing_args\n    call :usage\n    echo.\n    echo ****\n    echo MISSING RELEASE ARGUMENT !!!\n    goto :eof\n\n:port\n    echo Port does set from argument and port changes from 8000 to %__port% ...\n    goto :eof\n\n:version\n    if \"%~1\"==\"full\" call :usage &amp; goto :eof\n    echo %__version%\n    goto :eof\n\n:init\n    set \"__name=%~n0\"\n    set \"__port=8000\"\n\n    set \"__bat_filepath=%~0\"\n    set \"__bat_path=%~dp0\"\n    set \"__bat_filename=%~nx0\"\n\n    set \"__version=0.1.0\"\n    set \"__release=\"\n\n:parse\n    if \"%~1\"==\"\"                goto :validate\n\n    if /i \"%~1\"==\"-h\"           call :usage \"%~2\" &amp; goto :end\n    if /i \"%~1\"==\"--help\"       call :usage \"%~2\" &amp; goto :end\n\n    if /i \"%~1\"==\"-v\"           call :version      &amp; goto :end\n    if /i \"%~1\"==\"--version\"    call :version full &amp; goto :end\n\n    if /i \"%~1\"==\"-p\"           set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n    if /i \"%~1\"==\"--port\"       set \"__port=%~2\" &amp; shift &amp; shift &amp; call :port &amp; goto :parse\n\n    if not defined __release    set \"__release=%~1\" &amp; shift &amp; goto :parse\n\n    shift\n    goto :parse\n\n:validate\n    if not defined __release call :missing_args &amp; goto :end\n\n:main\n    echo INFO: Start running server with release \"%__release%\" ...\n    call .\\venv\\Scripts\\activate\n    call uvicorn main:app --port %__port%\n\n:end\n    echo.\n    echo End and Clean Up\n    call :cleanup\n    exit /B\n\n:cleanup\n    REM The cleanup function is only really necessary if you\n    REM are _not_ using SETLOCAL.\n\n    set \"__name=\"\n    set \"__port=\"\n\n    set \"__bat_filepath=\"\n    set \"__bat_path=\"\n    set \"__bat_filename=\"\n\n    set \"__release=\"\n    set \"__version=\"\n\n    goto :eof\n</code></pre> <p>Note: \\ <code>setlocal</code> allows you to set environmental variables (that are usually global) only seen inside your batch file and automatically cleans them up when the <code>endlocal</code> call is executed or the batch file ends.</p> <p>The most frequent use of SETLOCAL is to turn on command extensions and allow delayed expansion of variables: <code>setlocal enableextensions enabledelayedexpansion</code></p>"},{"location":"tools/lang/powershell/win-bat-file/#references","title":"References","text":"<ul> <li>https://stackoverflow.com/questions/14286457/using-parameters-in-batch-files-at-windows-command-line</li> <li>https://stackoverflow.com/questions/3973824/windows-bat-file-optional-argument-parsing</li> </ul>"},{"location":"tools/lang/powershell/win-bat-restapi/","title":"PowerShell: RestAPI","text":""},{"location":"tools/lang/powershell/win-bat-restapi/#webrequest","title":"WebRequest","text":"<p>Create Function to get the response code:</p> <pre><code>function Get-UrlStatusCode([string] $Url)\n{ try\n    { (Invoke-WebRequest -Uri $Url -UseBasicParsing -DisableKeepAlive).StatusCode }\n  catch [Net.WebException]\n    { [int]$_.Exception.Response.StatusCode }\n}\n</code></pre> <pre><code>$r = Get-UrlStatusCode \"http://localhost:8000/health\"\nif ($r -eq 200) {\n    echo \"The application can run normally.\"\n} else {\n    echo \"Failed to request to health check endpoint.\"\n}\n</code></pre>"},{"location":"tools/lang/powershell/win-bat-restapi/#restmethod","title":"RestMethod","text":"<pre><code>$Body = @{\n    Cook = \"Barbara\"\n    Meal = \"Pizza\"\n}\n\n$Header = @{\n    \"authorization\" = \"Bearer $token\"\n}\n\n$Parameters = @{\n    Method      = \"POST\"\n    Uri         =  \"https://4besday4.azurewebsites.net/api/AddMeal\"\n    Body        = ($Body | ConvertTo-Json)\n    ContentType = \"application/json\"\n    Headers     = $Header\n}\nInvoke-RestMethod @Parameters\n</code></pre> <p>Note</p> <pre><code>$User = \"GitHubUserName\"\n$Token = \"tokenhere\"\n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\"$($User):$($Token)\"))\n$Header = @{\n    Authorization = \"Basic $base64AuthInfo\"\n}\n</code></pre>"},{"location":"tools/lang/python/","title":"Python","text":""},{"location":"tools/lang/python/#best-practice","title":"Best Practice","text":"<p>https://medium.com/@Sabrina-Carpenter/10-advanced-code-performance-tips-for-senior-python-developers-47e752e95333</p>"},{"location":"tools/lang/python/#create-package","title":"Create Package","text":"<ul> <li>Advanced Project Structuring \u2014 Python</li> <li>https://medium.com/towards-data-engineering/pip-install-your-de-packages-from-github-fbfeed553555</li> </ul>"},{"location":"tools/lang/python/py-data-structure-for-data-engineer/","title":"Python: Data Structure for Data Engineer","text":"<p>https://medium.com/@SaiParvathaneni/data-structures-for-data-engineers-linked-lists-singly-linked-list-6ad66fd0bd62</p>"},{"location":"tools/lang/python/py-dependency-wheel/","title":"Dependencies Wheel","text":""},{"location":"tools/lang/python/py-dependency-wheel/#use-case-deploy-to-on-premise","title":"Use-case: Deploy to On-premise","text":"<p>This will use when you want to compress all dependencies from internet-able machine to non-internet-able machine</p> <ul> <li>Pack the dependencies to <code>.whl</code> files to wheels folder</li> </ul> <pre><code>pip wheel -w wheels -r requirements.txt\n</code></pre> <ul> <li>Install <code>.whl</code> files in wheel folder</li> </ul> <pre><code>pip install --no-index --find-links=wheels/ -r requirements.txt\n</code></pre>"},{"location":"tools/lang/python/py-dependency-wheel/#what-is-a-python-wheel","title":"What Is a Python Wheel?","text":"<p>A Python <code>.whl</code> file is essentially a ZIP (<code>.zip</code>) archive with a specially crafted filename that tells installers what Python versions and platforms the wheel will support.</p> <p>A wheel is a type of built distribution. In this case, built means that the wheel comes in a ready-to-install format and allows you to skip the build stage required with source distributions.</p> <p>Note</p> <p>It\u2019s worth mentioning that despite the use of the term built, a wheel doesn't contain <code>.pyc</code> files, or compiled Python bytecode.</p> <p>A wheel filename is broken down into parts separated by hyphens:</p> <pre><code>{dist}-{version}(-{build})?-{python}-{abi}-{platform}.whl\n</code></pre>"},{"location":"tools/lang/python/py-dependency-wheel/#references","title":"References","text":"<ul> <li>https://realpython.com/python-wheels/</li> </ul>"},{"location":"tools/lang/python/py-rust-migrate/","title":"Migrate Rust in Python","text":""},{"location":"tools/lang/python/py-rust-migrate/#references","title":"References","text":"<ul> <li>This Is How You Make Your Python Functions 5000% Faster With Rust (Yes, You Read It Right!)</li> </ul>"},{"location":"tools/lang/python/py-sync-multi-processes/","title":"Synchronizing Multi-Processes","text":"<pre><code>import multiprocessing\nimport time\n\n\ndef increase_counter(counter, lock):\n    for _ in range(20):\n        lock.acquire()\n        counter.value() += 10\n        lock.release()\n        time.sleep(0.1)\n\n\ndef decrease_counter(counter, lock):\n    for _ in range(20):\n        with lock:\n            counter.value() -= 10\n        time.sleep(0.1)\n\ncounter = multiprocessing.Value('i', 0)\nlock = multiprocessing.Lock()\n\nincrease = []\ndecrease = []\n\nfor _ in range(5):\n    p = multiprocessing.Process(target=increase_counter, args=(counter, lock, ))\n    p.start()\n    increase.append(p)\n\nfor _ in range(5):\n    p = multiprocessing.Process(target=decrease_counter, args=(counter, lock, ))\n    p.start()\n    decrease.append(p)\n\nfor p in increase:\n    p.join()\n\nfor p in decrease:\n    p.join()\n\nprint(counter)\n</code></pre>"},{"location":"tools/lang/python/py-threading/","title":"Threading","text":""},{"location":"tools/lang/python/py-threading/#getting-started","title":"Getting Started","text":"<p>Quote</p> <p>A thread is the basic unit of execution within a process. It is an independent flow of execution that shares the same address space as other independent flows of execution within the same process. A process can have one or more threads, one of them is the main thread, which is the default thread of a Python process.</p>"},{"location":"tools/lang/python/py-threading/#read-mores","title":"Read Mores","text":"<ul> <li>Threading in Python</li> </ul>"},{"location":"tools/lang/python/py-with-rust/","title":"Python with Rust","text":"<p>Speed Up Your Python Programs with Rust</p>"},{"location":"tools/lang/python/functional/py-func-monad/","title":"Python Functional Programing: PyMonad","text":"<pre><code>pip install pymonad\n</code></pre>"},{"location":"tools/lang/python/functional/py-func-toolz/","title":"Python Functional Programing: Toolz","text":"<pre><code>pip install toolz\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/","title":"Python: Joblib Package","text":""},{"location":"tools/lang/python/libs/py-joblib/#installation","title":"Installation","text":"<pre><code>pip install joblib\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#parallel","title":"Parallel","text":""},{"location":"tools/lang/python/libs/py-joblib/#functions","title":"Functions","text":"<pre><code>import os\nimport uuid\nimport requests\nimport pandas as pd\nfrom colorthief import ColorThief\nfrom joblib import Parallel, delayed\n\ndata = pd.read_csv(\"dress.csv\")\n\n\ndef extract_img_colors(url: str):\n    unique_id = uuid.uuid4()\n    with open(f\"{unique_id}.jpg\", \"wb\") as f:\n        f.write(requests.get(url).content)\n    color_thief = ColorThief(f\"{unique_id}.jpg\")\n    palette = color_thief.get_palette(color_count=2)\n    os.remove(f\"{unique_id}.jpg\")\n    return palette[0], palette[1]\n\ncolors = Parallel(n_jobs=-1)(\n    delayed(extract_img_colors)(url)\n    for url in data['image_url'].values[:100]\n)\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#classes","title":"Classes","text":"<pre><code>from joblib import Parallel, delayed\n\n\nclass A:\n\n    def __init__(self, x):\n        self.x = x\n\n    def square(self):\n        return self.x ** 2\n\n\nruns = [A(x) for x in range(20)]\nwith Parallel(n_jobs=6, verbose=5) as parallel:\n    delayed_func = [delayed(lambda x: x.square())(run) for run in runs]\n    output = parallel(delayed_func)\n</code></pre>"},{"location":"tools/lang/python/libs/py-joblib/#references","title":"References","text":"<ul> <li>Python Package: <code>joblib</code></li> </ul>"},{"location":"tools/lang/python/libs/py-pre-commit/","title":"Pre-Commit","text":"<pre><code>pip install pre-commit\n</code></pre>"},{"location":"tools/lang/python/libs/py-pre-commit/#create-custom-pre-commit","title":"Create Custom Pre-Commit","text":"<p>Custom pre-commit hooks for safer code changes</p>"},{"location":"tools/lang/python/libs/py-pydantic/","title":"Python: Pydantic","text":"<p>https://medium.com/towards-data-engineering/pydantic-for-data-engineers-5064ee2e4489</p>"},{"location":"tools/lang/python/libs/py-pytest/","title":"Pytest","text":"<ul> <li>https://towardsdatascience.com/a-guide-to-data-pipeline-testing-with-python-a85e3d37d361</li> </ul>"},{"location":"tools/lang/python/libs/py-sqlalchemy/","title":"SQLAlchemy","text":"<ul> <li>4 SQLAlchemy Features For Data Engineering</li> </ul>"},{"location":"tools/lang/python/updates/py-py312/","title":"Python 3.12","text":"<ul> <li>5 Handy Python 3.12 New Features That Improve Your Coding Experience</li> </ul>"},{"location":"tools/lang/rust/","title":"Rust","text":"<p>Rust was created by Graydon Hoare in 2006, who was working at Mozilla by the time. It began as a personal project, driven by the desire to create a more secure and efficient language for system-level programming.</p> <p>In 2009, Mozilla began sponsoring the project. The goal was to create a language that could power the next generation of web applications and services, particularly in performance-critical components of Firefox and its layout engine, Servo.</p> <p>Rust drew inspiration from several existing languages. Its syntax has similarities to C++ but also incorporates features from languages like Erlang and Haskell, especially in terms of its approach to concurrency and memory safety.</p> <p>In general terms</p> <p>Rust brings to the table a unique blend of features, primarily focusing on performance, in a similar way to C++, but with modern language features that make development safer.</p>"},{"location":"tools/lang/rust/#safety","title":"Safety","text":"<p>The key features that define safety in rust are:</p> <ul> <li>Ownership &amp; Memory management</li> <li>Borrowing and references</li> <li>Lifetimes</li> <li>Error handling</li> </ul>"},{"location":"tools/lang/rust/#ownership-memory-management","title":"Ownership &amp; Memory management","text":"<p>Ownership is a set of rules that govern how a program manages memory. It refers to how different parts of the system\u2019s memory are allocated and controlled.</p> <p>Imagine having an assistant who periodically checks your desk, removing items you\u2019re done with so your workspace remains uncluttered and efficient.</p> <p>All programs have to manage the way they use a computer\u2019s memory while running. Some languages have garbage collection that regularly looks for no longer-used memory as the program runs; in other languages, the programmer must explicitly allocate and free the memory.</p> <p>Ownership in Rust:</p> <p>Rust manages memory through a system of ownership with a set of rules that the compiler checks. If any of the rules are violated, the program won\u2019t compile.</p> <p>Ownership Rules:</p> <ul> <li>Each value in Rust has an owner.</li> <li>There can only be one owner at a time.</li> <li>When the owner goes out of scope, the value will be dropped.</li> </ul> <p>Read More:  Rust Ownership - Explained for Beginners</p> <p>None of the features of ownership will slow down a Rust program while it\u2019s running.</p> <p>This ensures that Rust programs not only benefit from enhanced memory safety and efficiency but also maintain optimal runtime performance, as the ownership model imposes no runtime overhead.</p>"},{"location":"tools/lang/rust/#borrowing-and-references","title":"Borrowing and References","text":"<p>Borrowing and references are fundamental concepts in Rust that work hand-in-hand with the ownership system to ensure memory safety and data race protection without the overhead of garbage collection.</p>"},{"location":"tools/lang/rust/#lifetimes","title":"Lifetimes","text":""},{"location":"tools/lang/rust/#error-handling","title":"Error handling","text":""},{"location":"tools/lang/rust/#speed","title":"Speed","text":""},{"location":"tools/lang/rust/#concurrency","title":"Concurrency","text":""},{"location":"tools/lang/rust/#references","title":"References","text":"<ul> <li> Rust -- A New Titan in Data Science</li> </ul>"},{"location":"tools/lang/rust/rust-cli-app/","title":"CLI Application","text":""},{"location":"tools/lang/rust/rust-cli-app/#getting-started","title":"Getting Started","text":"<p>If you haven\u2019t already, install Rust on your computer. After that, open a terminal and navigate to the directory you want to put your application code into.</p> <p>Start by running <code>cargo new grrs</code> in the directory you store your programming projects in. If you look at the newly created <code>grrs</code> directory, you\u2019ll find a typical setup for a Rust project:</p> <ul> <li>A <code>Cargo.toml</code> file that contains metadata for our project, incl.   a list of dependencies/external libraries we use.</li> <li>A <code>src/main.rs</code> file that is the entry point for our (main) binary.</li> </ul> <p>If you can execute <code>cargo run</code> in the <code>grrs</code> directory and get a <code>\"Hello World\"</code>, you\u2019re all set up.</p> <pre><code>$ cargo new grrs\n     Created binary (application) `grrs` package\n$ cd grrs/\n$ cargo run\n   Compiling grrs v0.1.0 (/Users/pascal/code/grrs)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.70s\n     Running `target/debug/grrs`\nHello, world!\n</code></pre>"},{"location":"tools/lang/rust/rust-cli-app/#references","title":"References","text":"<ul> <li>Command line apps in Rust</li> <li>Rust: Command-line apps</li> </ul>"},{"location":"tools/lang/rust/rust-from-python/","title":"Rust: Python to Rust","text":"<p>How Rust and Python Manage Memory</p> <p>https://towardsdatascience.com/python-to-rust-everything-you-must-know-about-virtual-environments-c1cd0e529835</p>"},{"location":"tools/lang/rust/rust-learning/","title":"Start Learning","text":"<ul> <li>Day 1 of learning Rust : The basics</li> </ul>"},{"location":"tools/lang/scala/","title":"Scala","text":"<p>https://medium.com/odds-team</p>"},{"location":"tools/lang/scala/#installation","title":"Installation","text":"<p>We should to install 2 components for running scalar on local:</p> <ol> <li>Java JDK</li> <li>Scalar</li> </ol> <pre><code>project-directory\n|\n|---&gt; build.sbt\n|---&gt; MyMain.scalar\n</code></pre> <pre><code>sbt run\n</code></pre>"},{"location":"tools/lang/scala/#main-object","title":"Main Object","text":"<pre><code>object MainObject {\n    def main(args: Array[String]) = {\n        ...\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-advance-feature/","title":"Scala Advance Feature","text":""},{"location":"tools/lang/scala/scala-advance-feature/#lazy-evaluation","title":"Lazy Evaluation","text":"<pre><code>class LazyEval {\n    lazy val l = {\n        println(\"lazy\")\n        9\n    }\n}\n\nval x = new LazyEval;  // Does not print anything\nprintln(y.l);\n</code></pre> <pre><code>lazy\n9\n</code></pre> <ul> <li>Method Lazy</li> </ul> <pre><code>def methodNormal(n: Int) {\n    println(\"Normal\");\n    println(n);\n}\n\ndef methodLazy(n: =&gt; Int) {\n    println(\"Lazy\");\n    println(n);\n}\n\nval add = (a: Int, b: Int) =&gt; {\n    print(\"Add\");\n    a + b;\n}\n\nmethodNormal(add(5, 6));\nmethodLazy(add(5, 6));\n</code></pre> <pre><code>Add\nNormal\n11\nLazy\nAdd\n11\n</code></pre>"},{"location":"tools/lang/scala/scala-advance-feature/#multi-threading","title":"Multi-threading","text":"<pre><code>class ThreadExample extends Thread {\n    override def run() {\n        for (i &lt;- 0 to 3) {\n            println(this.getName() + \":\" + this.getPriority() + \" - \" + i);\n            Thread.sleep(500);\n        }\n    }\n}\n\nvar t1 = new ThreadExample();\nvar t2 = new ThreadExample();\nvar t3 = new ThreadExample();\nt1.start();\nt1.join();\nt1.setName(\"First Thread:1\");\nt2.setName(\"Second Thread:10\");\nt1.setPriority(Thread.MIN_PRIORITY);\nt2.setPriority(Thread.MAX_PRIORITY);\nt1.start();\nt2.start();\n</code></pre> <pre><code>0\n1\n2\n3\nFirst Thread:1 - 0\nSecond Thread:10 - 0\nSecond Thread:10 - 1\nFirst Thread:1 - 1\nSecond Thread:10 - 2\nFirst Thread:1 - 2\nSecond Thread:10 - 3\nFirst Thread:1 - 3\n</code></pre> <ul> <li>Thread Multitasking</li> </ul> <pre><code>class ThreadExample() extends Thread {\n    override def run() {\n        for (i &lt;- 0 to 5) {\n            println(i);\n            Thread.sleep(500);\n        }\n    }\n    def task() {\n        for (i &lt;- 0 to 5) {\n            println(i);\n            Thread.sleep(200);\n        }\n    }\n}\n\nvar t1 = new ThreadExample();\nt1.start();\nt1.task();\n</code></pre> <pre><code>0\n0\n1\n2\n1\n3\n4\n2\n5\n3\n4\n5\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/","title":"Scalar Basic Command Line","text":"<p>Table of Contents:</p> <ul> <li>While Loop</li> <li>For Loop</li> <li>Pattern Matching</li> <li>Break</li> <li>Function</li> <li>Higher-Order Function</li> <li>String</li> <li>Option</li> <li>Exception</li> </ul>"},{"location":"tools/lang/scala/scala-basic-command/#while-loop","title":"While Loop","text":"<ul> <li>Method 01: Common while loop</li> </ul> <pre><code>var x = 0;\nwhile (x &lt; 10) {\n    println(\"x = \" + x );\n    x += 1;\n}\n</code></pre> <ul> <li>Method 02: Switch while loop to the end of doing (do-while)</li> </ul> <pre><code>var x = 0;\ndo {\n    println(\"x = \" + x );\n    x += 1;\n} while (x &lt; 10);\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#for-loop","title":"For Loop","text":"<pre><code>for (i &lt;- 1 to 5) {\n    println(\"i using to \" + i);\n}\n</code></pre> <p>Note: \\ We can use <code>1.to(5)</code>, <code>1 until 6</code>, or <code>1.until(6)</code> for represent range of number from 1 to 5.\\ Optional, if you want to step this range, you can use <code>by</code> like <code>for(i&lt;-1 to 10 by 2)</code>.</p> <ul> <li>Loop</li> </ul> <pre><code>for (i &lt;- 1 to 9; j &lt;- 1 to 3) {\n    println(\"i using until \" + i + \" \" + j);\n}\n</code></pre> <pre><code>i using until 1 1\ni using until 1 2\ni using until 1 3\ni using until 2 1\n...\n</code></pre> <ul> <li>Using <code>List</code></li> </ul> <pre><code>val lst = List(1, 2, 3, 78, 9);';\nfor (i &lt;- lst; if i &lt; 6) {\n    println(\"i using Filters \" + i);\n}\n</code></pre> <pre><code>lst.foreach(print)\n// 123789\n</code></pre> <pre><code>lst.foreach((element:Int) =&gt; print(element + \" \"))\n// 1 2 3 78 9\n</code></pre> <ul> <li>Pass for loop into val</li> </ul> <pre><code>val result = for {i &lt;- lst; if i &lt; 6} yield {\n    i * i\n}\nprintln(\"result = \" + result);\n</code></pre> <pre><code>result = List(1, 4, 9)\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#break","title":"Break","text":"<pre><code>import scala.util.control.Breaks._\n\n// Breakable method to avoid exception\nbreakable {\n    for (i &lt;- 1 to 10 by 2) {\n        if (i == 7)\n            break\n        else\n            println(\"i using to \" + i)\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#pattern-matching","title":"Pattern Matching","text":"<pre><code>def search(a: Any): Any = a match {\n    case 1  =&gt; println(\"One\");\n    case \"Two\" =&gt; println(\"Two\");\n    case \"Hello\" =&gt; println(\"Hello\");\n    case _ =&gt; println(\"No\");\n}\n</code></pre> <pre><code>val i = 7;\ni match {\n    case 1 | 3 | 5 | 7 | 9 =&gt; println(\"odd\");\n    case 2 | 4 | 6 | 8 =&gt; println(\"even\");\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#function","title":"Function","text":"<pre><code>def add(x: Int, y: Int): Int = {\n    return x + y;\n}\n</code></pre> <pre><code>def subtract(x: Int, y: Int): Int = {\n    x - y;\n}\n</code></pre> <pre><code>def multiply(x: Int, y: Int): Int = x * y;\n</code></pre> <pre><code>def devide(x: Int, y: Int) = x / y;\n</code></pre> <p>Note: \\ You can ignore to write <code>return</code> in last line of function body that will auto use the last line to return.</p> <pre><code>object Math {\n    def square(x: Int) = x * x;\n}\n\n// Use method function of object with space seperator\nprintln(Math square 3);\n</code></pre> <ul> <li>Recursion Function</li> </ul> <pre><code>// We can set default value to `y` parameter\ndef plusRecursion(x: Int, y: Int = 40): Int = {\n    if (y == 0)\n        0\n    else\n        x + plusRecursion(x, y - 1)\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#higher-order-function","title":"Higher-Order Function","text":"<ul> <li>Passing a Function as Parameter</li> </ul> <pre><code>def math(x: Double, y: Double, f: (Double, Double) =&gt; Double): Double = f(x, y);\n</code></pre> <pre><code>val result = math(50, 20, (x, y) =&gt; x max y);\n</code></pre> <pre><code>def math(x: Double, y: Double, z: Double, f: (Double, Double) =&gt; Double): Double = f(f(x, y), z);\n</code></pre> <pre><code>val result = math(50, 20, 10, (x, y) =&gt; x + y);\n</code></pre> <p>Note: \\ You can use wild-card like <code>math(50, 20, 10, _ + _);</code></p> <ul> <li>Function Composition</li> </ul> <pre><code>def addTwo(x: Int): Int = x + 2;\ndef multiplyTwo(x: Int): Int = x * 2;\n</code></pre> <pre><code>val result = multiplyTwo(addTwo(10));\n</code></pre> <ul> <li>Anonymous (lambda) Function</li> </ul> <pre><code>// Anonymous function by using =&gt; (rocket)\nval result = (x: Int, y: Int) =&gt; x + y;\n</code></pre> <pre><code>// Anonymous function by using _ (underscore) wild-card\nval result = (_: Int) + (_: Int)\n</code></pre> <ul> <li>Multiline Expression</li> </ul> <pre><code>def add(x: Int, y: Int) = {\n    x +\n    y\n}\n</code></pre> <pre><code>def add(x: Int, y: Int) = {\n    (x\n    + y)\n}\n</code></pre> <ul> <li>Partially Applied Function</li> </ul> <pre><code>val sum(x: Int, y: Int, z: Int) =&gt; x + y + z;\n</code></pre> <pre><code>val f = sum(10, 20, _: Int);\nval result = f(200);\n</code></pre> <ul> <li>Closures</li> </ul> <pre><code>var number = 10;\nval add = (x: Int) =&gt; x + number;\n\ndef main(args: Array[String]) {\n    number = 100;\n    println(add(20)); // stdout will show 120\n}\n</code></pre> <pre><code>val add = (x: Int) =&gt; {\n    number = x + number;\n    number;\n}\n\ndef main(args: Array[String]) {\n    number = 100;\n    println(add(20)); // stdout will show 120\n    println(number); // stdout will show 120\n}\n</code></pre> <ul> <li>Function Currying</li> </ul> <pre><code>def add(x: Int)(y: Int) = {\n    x + y;\n}\n</code></pre> <pre><code>val result = add(10)(10);\n</code></pre> <pre><code>val add40 = add(40)_;\nval result = add40(100);\n</code></pre> <p>Note: \\ You can write with oneline like <code>def add (x: Int) = (y: Int) =&gt; x + y;</code>, and Change to use with <code>val add20 = add(20)</code></p> <ul> <li>Nested Function</li> </ul> <pre><code>def add(x: Int, y: Int, z: Int) = {\n    def addTwo(a: Int, b: Int) = {\n        a + b;\n    }\n    addTwo(x, addTwo(y, z));\n}\n</code></pre> <ul> <li>Function with Variable Length Parameters</li> </ul> <pre><code>def addAll(args: Int*) = {\n    var sum = 0;\n    for (a &lt;- args) sum += a;\n    sum;\n</code></pre> <pre><code>val result = addAll(1, 2, 3, 4, 5);\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#string","title":"String","text":"<pre><code>val str1: String = \"Hello World\";\nval str2: String = \" Max\";\n</code></pre> <pre><code>str1.length();  // 11\nstr1.concat(str2);  // Hello World Max\nstr1 + str2;  // Hello World Max\nstr1.equals(str2);  // false\nstr1.compareTo(str2);  // -7\nstr1.substring(0, 5);  // Hello\n</code></pre> <ul> <li>String Interpolation</li> </ul> <pre><code>var s1 = \"Scala string example\";\nvar version = 2.12;\n\nprintln(f\"This is $s1%s, scala version is $version%2.2f\");  // This is Scala string example, scala version is 2.12\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#option","title":"Option","text":"<pre><code>val lst = List(1, 2, 3);\nval map = Map(1 -&gt; \"Tom\", 2 -&gt; \"Sara\");\nval opt: Option[Int] = None;\n</code></pre> <pre><code>lst.find(_ &gt; 3);  // None\nlst.find(_ &gt; 2);  // Some(3)\nmap.get(1);  // Some(Tom)\nmap.get(3);  // None\n</code></pre> <pre><code>lst.find(_ &gt; 2).get;  // 3\nmap.get(1).get;  // Tom\nmap.get(3).getOrElse(\"No name found\");  // No name found\n</code></pre>"},{"location":"tools/lang/scala/scala-basic-command/#exception","title":"Exception","text":"<ul> <li>Try Catch</li> </ul> <pre><code>class ExceptionExample {\n    def divide(a: Int, b: Int) = {\n        try {\n            a / b;\n            var arr = Array(1,2);\n            arr(10);\n        } catch {\n            case e: ArithmeticException =&gt; println(e)\n            case ex: Throwable =&gt; println(\"found a unknown exception: \" + ex)\n        } finally {\n            println(\"Finally block always executes\")\n        }\n        println(\"Rest of the code is executing...\");\n    }\n}\n\nvar e = new ExceptionExample();\ne.divide(100, 0);\ne.divide(100,10)\n</code></pre> <pre><code>java.lang.ArithmeticException: / by zero\nFinally block always executes\nRest of the code is executing...\nfound a unknown exception: java.lang.ArrayIndexOutOfBoundsException: 10\nFinally block always executes\nRest of the code is executing...\n</code></pre> <ul> <li>Throw keyword</li> </ul> <pre><code>class ExceptionExample {\n    def validate(age: Int) = {\n        if (age &lt; 18)\n            throw new ArithmeticException(\"You are not eligible\")\n        else println(\"You are eligible\")\n    }\n}\n\nvar e = new ExceptionExample();\ne.validate(10);\n</code></pre> <pre><code>java.lang.ArithmeticException: You are not eligible\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/","title":"Scala Collection","text":"<p>Scala provides rich set of collection library. It contains classes and traits to collect data. These collections can be mutable or immutable. You can use them according to your requirement. <code>Scala.collection.mutable</code> package contains all the mutable collections. You can add, remove and update data while using this package.</p> <p></p> <p><code>Scala.collection.immutable</code> contains all the immutable collections. It does not allow you to modify data. Scala imports this package by default. If you want mutable collection, you must import <code>Scala.collection.mutable</code> package in your code.</p> <p>Note: \\ Traversable is a trait and used to traverse collection elements. It is a base trait for all scala collections. It implements the methods which are common to all collections.</p> Method Description def head: A It returns the first element of collection. def init: Traversable[A] It returns all elements except last one. def isEmpty: Boolean It checks whether the collection is empty or not. It returns either true or false. def last: A It returns the last element of this collection. def max: A It returns the largest element of this collection. def min: A It returns smallest element of this collection def size: Int It is used to get size of this traversable and returns a number of elements present in this traversable. def sum: A It returns sum of all elements of this collection. def tail: Traversable[A] It returns all elements except first. def toArray: Array[A] It converts this collection to an array. def toList: List[A] It converts this collection to a list. def toSeq: Seq[A] It converts this collection to a sequence. def toSet[B &gt;: A]: immutable.Set[B] It converts this collection to a set. <p>Note: \\ Iterable is a next trait from the top of the hierarchy and a base trait for iterable collections. It extends traversable trait and provides important methods to concrete classes.</p> <p>Table of Contents:</p> <ul> <li>Array</li> <li>Seq</li> <li>List</li> <li>Vector</li> <li>Queue</li> <li>Stream</li> <li>Set</li> <li>HashSet</li> <li>BitSet</li> <li>ListSet</li> <li>Map</li> <li>HashMap</li> <li>ListMap</li> <li>Tuple</li> <li>Method of Collections</li> </ul>"},{"location":"tools/lang/scala/scala-collection/#array","title":"Array","text":"<p>Array is a collection of mutable values. It is an index based data structure which starts from 0 index to n-1 where n is length of array.</p> <p>Scala arrays can be generic. It means, you can have an <code>Array[T]</code>, where <code>T</code> is a type parameter or abstract type. Scala arrays are compatible with Scala sequences - you can pass an <code>Array[T]</code> where a <code>Seq[T]</code> is required. It also supports all the sequence operations.</p>"},{"location":"tools/lang/scala/scala-collection/#single-dimensional-array","title":"Single Dimensional Array","text":"<p>Syntax for Single Dimensional Array:</p> <pre><code>var arrayName : Array[arrayType] = new Array[arrayType](arraySize);\nvar arrayName = new Array[arrayType](arraySize);\nvar arrayName : Array[arrayType] = new Array(arraySize);\nvar arrayName = Array(element1, element2 ... elementN);\n</code></pre> <pre><code>val arr = Array(1, 2, 3, 4, 5);\nfor (x &lt;- arr) {\n    println(x);\n}\n</code></pre> <pre><code>1\n2\n3\n4\n5\n</code></pre> <p>Note: You can iterate an array by using foreach Loop like, <code>arr.foreach((x: Int) =&gt; println(x))</code></p> <ul> <li>Array Function</li> </ul> <pre><code>import Array._\n\nval arr1 = Array(1, 2, 3, 4, 5);\nval arr2 = Array(6, 7, 8);\nval result = concat(arr1, arr2);  // Array(1, 2, 3, 4, 5, 6, 7, 8)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#multidimensional-array","title":"Multidimensional Array","text":"<p>Multidimensional Array Syntax:</p> <pre><code>var arrayName = Array.ofDim[ArrayType](NoOfRows,NoOfColumns);\nvar arrayName = Array(Array(element...), Array(element...), ...);\n</code></pre> <pre><code>var arr = Array.ofDim[Int](2,2);\narr(1)(0) = 15;\nfor( i &lt;- 0 to 1){\n    for( j &lt;- 0 to 1){\n        print(\" \" + arr(i)(j))\n    }\n    println();\n}\n</code></pre> <pre><code>0 0\n15 0\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#seq","title":"Seq","text":"<p>Seq is a trait which represents indexed sequences that are guaranteed immutable. You can access elements by using their indexes. It maintains insertion order of elements.</p> <p>Sequences support a number of methods to find occurrences of elements or subsequences. It returns a list.</p> <p>Commonly used Methods of Seq:</p> Method Description def containsA1 &gt;: A: Boolean Check whether the given element present in this sequence. def copyToArray(xs: Array[A], start: Int, len: Int): Unit It copies the seq elements to an array. def endsWithB: Boolean It tests whether this sequence ends with the given sequence or not. def head: A It selects the first element of this seq collection. def indexOf(elem: A): Int It finds index of first occurrence of a value in this immutable sequence. def isEmpty: Boolean It tests whether this sequence is empty or not. def lastIndexOf(elem: A): Int It finds index of last occurrence of a value in this immutable sequence. def reverse: Seq[A] It returns new sequence with elements in reversed order. <pre><code>import scala.collection.immutable.Seq\n\nvar seq: Seq[Int] = Seq(52, 85, 1, 8, 3, 2, 7);\nseq.reverse;  // List(7, 2, 3, 8, 1, 85, 52)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#list","title":"List","text":"<p>List is used to store ordered elements. It extends LinearSeq trait. It is a class for immutable linked lists. This class is good for last-in-first-out (LIFO), stack-like access patterns.</p> <p>is a next trait from the top of the hierarchy and a base trait for iterable collections. It extends traversable trait and provides important methods to concrete classes.</p> <pre><code>val lst: List[Int] = List(1, 8, 5, 6, 9, 58, 23, 15, 4);\nval lst2: List[Int] = List(88, 100);\nval lstNil: List[Int] = 1 :: 5 :: 9 :: Nil;  // List(1, 5, 9)\nval lstTwo: List[Int] = List.fill(5)(2);  // List(2, 2, 2, 2, 2)\n</code></pre> <pre><code>val lst3 = lst ++ lst2;  // List(1, 8, 5, 6, 9, 58, 23, 15, 4, 88, 100)\nval lst4 = lst3.sorted;  // List(1, 4, 5, 6, 8, 9, 15, 23, 58, 88, 100)\nval lst5 = lst3.reverse;  // List(100, 88, 4, 15, 23, 58, 9, 6, 5, 8, 1)\n</code></pre> <pre><code>var result = 0 :: lst;  // List(0, 1, 8, 5, 6, 9, 58, 23, 15, 4);\nvar result = lst.max;  // 58\n</code></pre> <pre><code>var sum: Int = 0;\nval result = lst2.foreach(sum += _);  // 188\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#vector","title":"Vector","text":"<p>Vector is a general-purpose, immutable data structure. It provides random access of elements. It is good for large collection of elements.</p> <p>It extends an abstract class AbstractSeq and IndexedSeq trait.</p> <pre><code>import scala.collection.immutable.Vector\n\nvar vector: Vector[Int] = Vector(5, 8, 3, 6, 9, 4);\nvar vector2 = Vector.empty;\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#queue","title":"Queue","text":"<p>Queue implements a data structure that allows inserting and retrieving elements in a first-in-first-out (FIFO) manner.</p> <p>In scala, Queue is implemented as a pair of lists. One is used to insert the elements and second to contain deleted elements. Elements are added to the first list and removed from the second list.</p> <pre><code>import scala.collection.immutable._\n\nvar queue: Queue[Int] = Queue(1, 5, 6, 2, 3, 9, 5, 2, 5);\n</code></pre> <pre><code>queue.front;  // 1\nqueue.enqueue(100);  // Queue(1, 5, 6, 2, 3, 9, 5, 2, 5, 100)\nqueue.dequeue;  // (1,Queue(5, 6, 2, 3, 9, 5, 2, 5))\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#stream","title":"Stream","text":"<p>Stream is a lazy list. It evaluates elements only when they are required. This is a feature of scala. Scala supports lazy computation. It increases performance of your program.</p> <pre><code>val stream = 100 #:: 200 #:: 85 #:: Stream.empty;  // Stream(100, ?)\nval stream2 = (1 to 10).toStream;  // Stream(1, ?)\n</code></pre> <pre><code>stream2.head;  // 1\nstream2.take(10);  // Stream(1, ?)\nstream.map{_*2};  // Stream(200, ?)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#set","title":"Set","text":"<p>It is used to store unique elements in the set. It does not maintain any order for storing elements. You can apply various operations on them. It is defined in the <code>Scala.collection.immutable</code> package.</p> <p>Scala Set Syntax:</p> <pre><code>val variableName: Set[Type] = Set(element1, element2,... elementN);\nval variableName = Set(element1, element2,... elementN);\n</code></pre> <pre><code>import scala.collection.immutable._\n\nval alphabet = Set(\"C\",\"F\",\"H\",\"G\");\nval alphabet2 = Set(\"A\",\"B\", \"D\",\"E\");\n</code></pre> <pre><code>var result = alphabet ++ alphabet2;  // Set(E, F, G, H, A, B, C, D);\nvar result = alphabet.size;  // 4\nvar result = alphabet.contains(\"G\");  // true\n</code></pre> <pre><code>alphabet += \"R\";  // Set(E, F, G, H, A, B, C, D, R);\nalphabet += \"A\";  // Set(E, F, G, H, A, B, C, D, R);\nalphabet -= \"C\";  // Set(E, F, G, H, A, B, D, R);\n</code></pre> <p>Note: \\ We can use <code>variable(something)</code> instead <code>variable.contains(something)</code>.</p> <p>Note: \\ If you want to create set that able to sort, you can use <code>scala.collection.immutable.SortedSet</code>.</p>"},{"location":"tools/lang/scala/scala-collection/#hashset","title":"HashSet","text":"<p>HashSet is a sealed class. It extends AbstractSet and immutable Set trait. It uses hash code to store elements. It neither maintains insertion order nor sorts the elements.</p> <pre><code>import scala.collection.immutable.HashSet\n\nvar hashset = HashSet(4, 2, 8, 0, 6, 3, 45);\nhashset.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>0\n6\n2\n45\n3\n8\n4\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#bitset","title":"BitSet","text":"<p>Bitsets are sets of non-negative integers which are represented as variable-size arrays of bits packed into 64-bit words. The memory footprint of a bitset is determined by the largest number stored in it. It extends Set trait.</p> <pre><code>import scala.collection.immutable.BitSet\n\nvar numbers = BitSet(1, 5, 8, 6, 9, 0);\nnumbers.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>0\n1\n5\n6\n8\n9\n</code></pre> <pre><code>numbers += 20;  // BitSet(0 1 5 6 8 9 20)\nnumbers -= 0;  // BitSet(1 5 6 8 9 20)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#listset","title":"ListSet","text":"<p>In scala, ListSet class implements immutable sets using a list-based data structure. Elements are stored internally in reversed insertion order, which means the newest element is at the head of the list. It maintains insertion order.</p> <p>This collection is suitable only for a small number of elements. You can create empty <code>ListSet</code> either by calling the constructor or by applying the function <code>ListSet.empty</code>. Its iterate and traversal methods visit elements in the same order in which they were first inserted.</p> <pre><code>import scala.collection.immutable.ListSet\n\nvar listset = ListSet(4, 2, 8, 0, 6, 3, 45);\nlistset.foreach((element: Int) =&gt; println(element));\n</code></pre> <pre><code>4\n2\n8\n0\n6\n3\n45\n</code></pre> <pre><code>var listset: ListSet[String] = new ListSet();\nvar listset2: ListSet[String] = ListSet.empty;\n\nlistset += \"India\";  // ListSet(India)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#map","title":"Map","text":"<p>Map is used to store elements. It stores elements in pairs of key and values. In scala, you can create map by using two ways either by using comma separated pairs or by using rocket operator.</p> <pre><code>val map: Map[String, String] = Map((\"A\", \"Apple\"), (\"B\", \"Ball\"));\nval map2 = Map(\"D\" -&gt; \"Dog\");\n</code></pre> <pre><code>var result = map + (\"C\" -&gt; \"Cat\");  // Map(A -&gt; Apple, B -&gt; Ball, C -&gt; Cat)\nvar result = map - (\"B\");  // Map(A -&gt; Apple)\nvar result = map ++ map2;  // Map(A -&gt; Apple, B -&gt; Ball, D -&gt; Dog)\n</code></pre> <pre><code>map.keys;  // Set(A, B, C)\nmap.values;  // MapLike.DefaultValuesIterable(Apple, Ball, Cat)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#hashmap","title":"HashMap","text":"<p>HashMap is used to store element. It use hash code to store elements and return a map.</p> <pre><code>import scala.collection.immutable.HashMap\n\nvar hashMap = new HashMap();\nvar hashMap2 = HashMap(\"A\" -&gt; \"Apple\", \"B\" -&gt; \"Ball\", \"C\" -&gt; \"Cat\");\n\nhashMap2.foreach {\n    case (key, value) =&gt; println(key + \" -&gt; \" + value)\n}\n</code></pre> <pre><code>A -&gt; Apple\nB -&gt; Ball\nC -&gt; Cat\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#tuple","title":"Tuple","text":"<p>A tuple is a collection of elements in ordered form. If there is no element present, it is called empty tuple. You can use tuple to store any type of data. You can store similar type or mix type data also. You can use it to return multiple values from a function.</p> <pre><code>val tp = (1, 2.5, \"Apple\");\nval tp2 = 1 -&gt; \"Tom\";  // (1, Tom)\n</code></pre> <pre><code>tp.productIterator.foreach(println)\n</code></pre> <pre><code>1\n2.5\nApple\n</code></pre> <pre><code>tp._1;  // 1\ntp._2;  // 2.5\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#listmap","title":"ListMap","text":"<p>This class implements immutable maps by using a list-based data structure. It maintains insertion order and returns <code>ListMap</code>. This collection is suitable for small elements.</p> <pre><code>import scala.collection.immutable._\n\n\nvar listMap = ListMap(\"Rice\" -&gt; \"100\", \"Wheat\" -&gt; \"50\", \"Gram\" -&gt; \"500\");\nvar emptyListMap = new ListMap();\nvar emptyListMap2 = ListMap.empty;\n</code></pre> <pre><code>var newListMap = listMap + (\"Pulses\" -&gt; \"550\");\nnewListMap.foreach {\n    case (key, value) =&gt; println(key + \" -&gt; \" + value)\n}\n</code></pre> <pre><code>Rice -&gt; 100\nWheat -&gt; 50\nGram -&gt; 500\nPulses -&gt; 550\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#method-of-collections","title":"Method of Collections","text":""},{"location":"tools/lang/scala/scala-collection/#map-filter","title":"Map &amp; Filter","text":"<pre><code>val lst = List(1, 2, 3, 4, 5);\nvar result = lst.map(x =&gt; x * 2);  // List(2, 4, 6, 8, 10)\nvar result = lst.map(x =&gt; List(x, x + 1));  // List(List(1, 2), List(2, 3), List(3, 4), List(4, 5), List(5, 6))\nvar result = lst.flatMap(x =&gt; List(x, x + 1));  // List(1, 2, 2, 3, 3, 4, 4, 5, 5, 6)\nvar result = lst.fillter(x =&gt; x % 2 == 0);  // List(2, 4)\n</code></pre> <pre><code>val map = Map(1 -&gt; \"Apple\", 2 -&gt; \"Ball\");\nvar result = map.map(x =&gt; \"hi\" + x);  // List(hi(1,Apple), hi(2,Ball))\nvar result = map.mapValues(x =&gt; \"hi\" + x);  // Map(1 -&gt; hi Apple, 2 -&gt; hi Ball)\n</code></pre> <pre><code>\"Hello\".map(_.toUpper);  // HELLO\n</code></pre> <pre><code>List(List(1, 2, 3), List(3, 4)).flatten;  // List(1, 2, 3, 3, 4)\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#reduce-fold-or-scan","title":"Reduce &amp; Fold or Scan","text":"<pre><code>val lst = List(1, 2, 3, 10, 13);\nvar result = lst.reduceLeft(_ + _);  // 29\nvar result = lst.foldLeft(0)(_ + _);  // 29\nvar result = lst.foldLeft(100)(_ + _);  // 129\nvar result = lst.scanLeft(100)(_ + _);  // List(100, 101, 103, 106, 116, 129)\n\nval lst2 = List(\"A\", \"B\", \"C\");\nvar result = lst2.reduceLeft(_ + _);  // ABC\nvar result = lst2.foldLeft(\"z\")(_ + _);  // zABC\nvar result = lst2.scanLeft(\"z\")(_ + _);  // List(z, zA, zAB, zABC)\n</code></pre> <pre><code>var result = lst.reduceLeft((x, y) =&gt; {println(x + \" , \" + y); x + y;});\n</code></pre> <pre><code>1, 2\n3, 3\n6, 10\n16, 13\n29\n</code></pre>"},{"location":"tools/lang/scala/scala-collection/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=OfngvXKNkpM</li> <li>https://www.javatpoint.com/</li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/","title":"Scala OOPs Concepts","text":"<p>Table of Contents:</p> <ul> <li>Object and Class</li> <li>[Singleton Object]</li> <li>[Companion Object]</li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/#object-and-class","title":"Object and Class","text":"<p>Unlike java, scala is a pure object-oriented programming language. It allows us to create object and class so that you can develop object-oriented applications.</p> <ul> <li>Object</li> </ul> <p>Object is a real world entity. It contains state and behavior. Laptop, car,   cell phone are the real world objects.</p> <p>Object typically has two characteristics:</p> <ul> <li>State: data values of an object are known as its state.</li> <li>Behavior: functionality that an object performs is known as its behavior.</li> </ul> <p>Note: \\ Object in scala is an instance of class. It is also known as runtime entity.</p> <ul> <li>Class</li> </ul> <p>Class is a template or a blueprint. It is also known as collection of objects   of similar type.</p> <p>In scala, a class can contain:</p> <ul> <li>Data member</li> <li>Member method</li> <li>Constructor</li> <li>Block</li> <li>Nested class</li> <li>Super class information etc.</li> </ul> <p>You must initialize all instance variables in the class. There is no default   scope. If you don't specify access scope, it is public. There must be an object   in which main method is defined. It provides starting point for your program.   Here, we have created an example of class.</p> <pre><code>class Student {\n    var id: Int = 0;\n    var name: String = null;\n}\n\nvar s = new Student();\ns.id = 10;\nprintln(s.id + \" \" + s.name);  // 10 null\n</code></pre> <pre><code>// Primary constructor with `val` by default\nclass Student(id: Int, name: String) {\n    def this() {\n        this(99, \"Default\");\n    }\n\n    def this(id: Int) {\n        this(id, \"Default);\n    }\n\n    def show() {\n        println(id + \" \" + name)\n    }\n}\n\nvar s = new Student(100, \"Martin\");\ns.show();  // 100 Martin\n\nvar s = new Student();  // Student(99, Default)\nvar s = new Student(200);  // Student(200, Default)\n</code></pre> <pre><code>class User(private var name: String, val age: Int) {\n    def printName{ println(name) }\n}\n\nvar u = new User(\"Max\", 28);\nu.printName;  // Max\n</code></pre> <pre><code>// Anonymous object\nclass Arithmetic {\n    def add(a: Int, b: Int) {\n        var add = a + b;\n        println(\"sum = \" + add);\n    }\n}\n\nnew Arithmetic().add(10, 10);  // Sum = 20\n</code></pre> <p>Note: \\ In scala, <code>this</code> is a keyword and used to refer current object. You can call instance variables, methods, constructors by using <code>this</code> keyword.</p> <p>Note: \\ Access modifier is used to define accessibility of data and our code to the outside world. Scala provides only three types of access modifiers, which are given below:</p> <ul> <li>No modifier</li> <li>Protected: <code>protected var ...</code></li> <li>Private: <code>private var ...</code></li> </ul>"},{"location":"tools/lang/scala/scala-oop-concepts/#singleton-object","title":"Singleton Object","text":"<p>Singleton object is an object which is declared by using object keyword instead by class. No object is required to call methods declared inside singleton object.</p> <p>In scala, there is no static concept. So scala creates a singleton object to provide entry point for your program execution.</p> <p>If you don't create singleton object, your code will compile successfully but will not produce any output. Methods declared inside Singleton Object are accessible globally. A singleton object can extend classes and traits.</p> <pre><code>object SingletonObject{\n    def hello(){\n        println(\"Hello, This is Singleton Object\")\n    }\n}\n\nSingletonObject.hello();  // Hello, This is Singleton Object\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#companion-object","title":"Companion Object","text":"<p>In scala, when you have a class with same name as singleton object, it is called companion class and the singleton object is called companion object.</p> <p>The companion class and its companion object both must be defined in the same source file.</p> <pre><code>class ComapanionClass{\n    def hello(){\n        println(\"Hello, this is Companion Class.\")\n    }\n}\n\nnew ComapanionClass().hello();  // Hello, this is Companion Class.\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#inheritance","title":"Inheritance","text":"<p>Inheritance is an object-oriented concept which is used to re-usability of code. You can achieve inheritance by using extends keyword. To achieve inheritance a class must extend to other class. A class which is extended called super or parent class. a class which extends class is called derived or base class.</p> <p>Inheritance Syntax:</p> <pre><code>class SubClassName extends SuperClassName() {\n  /* Write your code\n  *  methods and fields etc.\n  */\n}\n</code></pre> <p>Types of Inheritance in Scala:</p> <p>Scala supports various types of inheritance including single, multilevel, multiple, and hybrid. You can use single, multilevel and hierarchical in your class. Multiple and hybrid can only be achieved by using traits. Here, we are representing all types of inheritance by using pictorial form.</p> <p></p> <pre><code>class Employee {\n    val salary: Float = 10000;\n    def title : String = \"Salary: \" + salary;\n}\n\nclass Programmer(name: String) extends Employee {\n    override val salary: Float = 15000;\n    var bonus: Int = 5000;  // this cannot override a mutable variable\n    override def title: String = name + \" Salary: \" + salary + \", Bonus: \" + bonus;\n}\n\nvar p = new Programmer(\"Jax\");\np.title;  // Jax Salary: 15000, Bonus: 5000\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#final","title":"Final","text":"<p>Final is a keyword, which is used to prevent inheritance of super class members into derived class. You can declare final variables, methods and classes also.</p> <pre><code>class Vehicle{\n    final val speed: Int = 60;\n}\n\nclass Bike extends Vehicle{\n    override val speed: Int = 100;\n}\n</code></pre> <pre><code>Error - value speed cannot override final member\n</code></pre> <p>Note: \\ The <code>final</code> can use to method and class like <code>final def ...</code> and <code>final class ...</code>.</p>"},{"location":"tools/lang/scala/scala-oop-concepts/#case-classes","title":"Case Classes","text":"<p>Scala case classes are just regular classes which are immutable by default and decomposable through pattern matching.</p> <p>It uses equal method to compare instance structurally. It does not use new keyword to instantiate object.</p> <p>Warning: \\ All the parameters listed in the case class are public and immutable by default.</p> <pre><code>case class Task(id: Int, title: String, var state: Int = 1) {\n    def next() { if (state == 3) 3 else state += 1 }\n}\n\nval buyBanana = Task(id = 0, title = \"Buy Banana\");  // Task(0, Buy Banana, 3)\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#abstract-class","title":"Abstract Class","text":"<p>A class which is declared with abstract keyword is known as abstract class. An abstract class can have abstract methods and non-abstract methods as well. Abstract class is used to achieve abstraction. Abstraction is a process in which we hide complex implementation details and show only functionality to the user.</p> <p>In scala, we can achieve abstraction by using abstract class and trait. We have discussed these in detail here.</p> <pre><code>abstract class Bike(name: String) {\n    var b: Int = 20;\n\n    // Abstract method\n    def run()\n\n    // Non-abstract method\n    def performance() {\n        println(\"Performance awesome\")\n    }\n}\n\nclass Hero extends Bike {\n    def run() {\n        println(\"running fine...\")\n    }\n}\n</code></pre>"},{"location":"tools/lang/scala/scala-oop-concepts/#trait","title":"Trait","text":"<p>A trait is like an interface with a partial implementation. In scala, trait is a collection of abstract and non-abstract methods. You can create trait that can have all abstract methods or some abstract and some non-abstract methods.</p> <p>A variable that is declared either by using <code>val</code> or <code>var</code> keyword in a trait get internally implemented in the class that implements the <code>trait</code>. Any variable which is declared by using <code>val</code> or <code>var</code> but not initialized is considered abstract.</p> <p>Traits are compiled into Java interfaces with corresponding implementation classes that hold any methods implemented in the traits.</p> <pre><code>trait Printable {\n    def print()\n}\n\nclass A4 extends Printable{\n  def print() {\n        println(\"Hello I am A4\")\n    }\n}\n</code></pre> <ul> <li>Implementing Multiple Traits in a Class</li> </ul> <pre><code>trait Printable {\n    def print()\n\n    // Non-abstract method\n    def title() {\n        println(\"This is show method\")\n    }\n}\n\ntrait Showable {\n    def show()\n}\n\nclass A6 extends Printable with Showable {\n    def print() {\n        println(\"This is printable\")\n    }\n    def show() {\n        println(\"This is showable\");\n    }\n}\n</code></pre> <p>Note: \\ On above example, <code>Printable</code> can be abstract class but <code>Showable</code> can not. That mean <code>A6</code> need to inherited only one abstract class and <code>Showable</code> should be trait. \\ <code>class ClassName extends AbstractClass with FirstTrait with SecondTrait { ... }</code></p>"},{"location":"tools/lang/scala/scala-oop-concepts/#trait-mixins","title":"Trait Mixins","text":"<p>In scala, trait mixins means you can extend any number of traits with a class or abstract class. You can extend only traits or combination of traits and class or traits and abstract class.</p> <p>It is necessary to maintain order of mixins otherwise compiler throws an error.</p> <pre><code>trait Print {\n    def print()\n}\n\nabstract class PrintA4 {\n    def printA4()\n}\n\nclass A6 extends PrintA4 with Print {\n    // Trait print\n    def print() {\n        println(\"print sheet\");\n    }\n    // Abstract class printA4\n    def printA4() {\n        println(\"Print A4 Sheet\");\n    }\n}\n\nclass A6After extends PrintA4 {\n    // Trait print\n    def print() {\n        println(\"print sheet\");\n    }\n    // Abstract class printA4\n    def printA4() {\n        println(\"Print A4 Sheet\");\n    }\n}\n\nvar a = new A6();\nvar a = new A6After() with Print;\n</code></pre>"},{"location":"tools/lang/shell/","title":"Shell","text":"<p>Is the basic programming language that use for communicate with OS of any machine, like Linux, Windows, or Mac.</p>"},{"location":"tools/lang/shell/ubt-sh-common-file/","title":"App","text":"<ul> <li>https://stackoverflow.com/questions/3534280/how-can-i-pass-a-file-argument-to-my-bash-script-using-a-terminal-command-in-lin</li> </ul>"},{"location":"tools/lang/sql/","title":"SQL","text":"<p>Mastering SQL for Data Engineers: A Comprehensive Learning Roadmap</p>"},{"location":"tools/lang/sql/sql-optimizing-sql-queries/","title":"SQL: Optimizing SQL Queries","text":"<p>https://medium.com/@vishalbarvaliya/optimizing-sql-queries-25-techniques-for-efficient-database-queries-b31ce7559079</p>"},{"location":"blogs/archive/2024/03/","title":"March 2024","text":""},{"location":"blogs/category/knowledge/","title":"Knowledge","text":""}]}